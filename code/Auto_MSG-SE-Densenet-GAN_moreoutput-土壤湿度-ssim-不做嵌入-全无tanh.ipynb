{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fad81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T06:06:08.240083Z",
     "start_time": "2024-04-12T06:06:08.072547Z"
    }
   },
   "outputs": [],
   "source": [
    "def Auto_MSG_SE_Densenet_GAN(vy,vx,upscale,test_size=0.25,if_best_mode='no',modelpath=None,conv_core_num=512,model_deep=5,Vgg_deep=5,se_radio=0.5,if_weight_initialize='no',weight_initialize_method='TruncatedNormal',weight_initialize_parameter1=0.00,weight_initialize_parameter2=0.05,loss_function='default',if_print_model='yes',optimizer='SGD',g_learning_rate=0.001,d_learning_rate=0.01,epochs=2000,g_train_time=2,ifrandom_split='yes',ifmute='no',ifsave='no',savepath=None,device='cpu'):\n",
    "    import tensorflow as tf\n",
    "    from keras.models import Sequential,Model\n",
    "    import math\n",
    "    from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "    from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.optimizers import SGD,Adam\n",
    "    from scipy.stats import pearsonr\n",
    "    from keras.models import load_model\n",
    "    import os\n",
    "    from sklearn.metrics import accuracy_score,log_loss\n",
    "    import keras.backend as K\n",
    "        \n",
    "    if vx.ndim==3:\n",
    "        vx=np.array(vx).reshape(vx.shape[0],vx.shape[1],vx.shape[2],1)\n",
    "    if vy.ndim==3:\n",
    "        vy=np.array(vy).reshape(vy.shape[0],vy.shape[1],vy.shape[2],1)\n",
    "    if ifrandom_split=='yes':\n",
    "        trainx,testx,trainy,testy = train_test_split(vx,vy,test_size=test_size,random_state=25)\n",
    "    else:\n",
    "        index=int((1-test_size)*vy.shape[0])\n",
    "        trainy=vy[:index,:,:,:]\n",
    "        testy=vy[index:,:,:,:]\n",
    "        trainx=vx[:index,:,:,:]\n",
    "        testx=vx[index:,:,:,:]\n",
    "    if device=='gpu':\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  \n",
    "        tf.device('/gpu:0')\n",
    "        if optimizer == 'SGD':\n",
    "            g_opt = SGD(lr = g_learning_rate)\n",
    "            d_opt = SGD(lr = d_learning_rate)\n",
    "        elif optimizer == 'Adam':\n",
    "            g_opt = Adam(lr = g_learning_rate)\n",
    "            d_opt = Adam(lr = d_learning_rate)\n",
    "        def build_generator(trainy,generator_input,model_deep,conv_core_num,upscale,se_radio,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2):\n",
    "            import tensorflow as tf\n",
    "            from keras.models import Sequential,Model\n",
    "            import math\n",
    "            from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "            from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            import numpy as np\n",
    "            from tensorflow.keras.optimizers import SGD,Adam\n",
    "            from scipy.stats import pearsonr\n",
    "            from keras.models import load_model\n",
    "            import os\n",
    "            generator_inputs=Input(shape=(generator_input.shape[1],generator_input.shape[2],vx.shape[3]))\n",
    "            if if_weight_initialize=='no':\n",
    "                exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\")(generator_inputs)')\n",
    "            else:\n",
    "                if weight_initialize_method=='RandomNormal':\n",
    "                    exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_inputs)')\n",
    "                elif weight_initialize_method=='RandomUniform':\n",
    "                    exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_inputs)')\n",
    "                elif weight_initialize_method=='TruncatedNormal':\n",
    "                    exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_inputs)')\n",
    "            exec('generator_act_start_1=Activation(\"leaky_relu\")(generator_conv_start_1)')\n",
    "            if if_weight_initialize=='no':\n",
    "                exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\")(generator_act_start_1)')\n",
    "            else:\n",
    "                if weight_initialize_method=='RandomNormal':\n",
    "                    exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act_start_1)')\n",
    "                elif weight_initialize_method=='RandomUniform':\n",
    "                    exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_act_start_1)')\n",
    "                elif weight_initialize_method=='TruncatedNormal':\n",
    "                    exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act_start_1)')\n",
    "            exec('generator_act_start_2=Activation(\"leaky_relu\")(generator_conv_start_2)')\n",
    "            exec('segap_start=GlobalAveragePooling2D()(generator_act_start_2)')\n",
    "            exec('sefc_start_1=Dense(int(conv_core_num*se_radio))(segap_start)')\n",
    "            exec('seact_start_1=Activation(\"leaky_relu\")(sefc_start_1)')\n",
    "            exec('sefc_start_2=Dense(conv_core_num)(seact_start_1)')\n",
    "            exec('seact_start_2=Activation(\"leaky_relu\")(sefc_start_2)')\n",
    "            exec('semulti_start=Multiply()([generator_act_start_2,seact_start_2])')\n",
    "            exec('seadd_start=Add()([semulti_start,generator_act_start_2])')\n",
    "            for i in range(model_deep):\n",
    "                if i==0:\n",
    "                    exec('generator_upsample_'+str(i+1)+'=UpSampling2D(size=(upscale,upscale))(seadd_start)')\n",
    "                else:\n",
    "                    exec('generator_upsample_'+str(i+1)+'=UpSampling2D(size=(upscale,upscale))(generator_act'+str(i)+'_2)')             \n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\")(generator_upsample_'+str(i+1)+')')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_upsample_'+str(i+1)+')')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_upsample_'+str(i+1)+')')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_upsample_'+str(i+1)+')')\n",
    "                exec('generator_act'+str(i+1)+'_1=Activation(\"leaky_relu\")(generator_conv'+str(i+1)+'_1)')\n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\")(generator_act'+str(i+1)+'_1)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_1)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_act'+str(i+1)+'_1)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_1)')\n",
    "                exec('generator_act'+str(i+1)+'_2=Activation(\"leaky_relu\")(generator_conv'+str(i+1)+'_2)')\n",
    "            if if_weight_initialize=='no':\n",
    "                generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\")(generator_act'+str(i+1)+'_2)')\n",
    "            else:\n",
    "                if weight_initialize_method=='RandomNormal':\n",
    "                    generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_2)')\n",
    "                elif weight_initialize_method=='RandomUniform':\n",
    "                    generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_act'+str(i+1)+'_2)')\n",
    "                elif weight_initialize_method=='TruncatedNormal':\n",
    "                    generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_2)')\n",
    "            return Model(inputs=generator_inputs, outputs=generator_output)\n",
    "        def build_discriminator(trainy,discriminator_input,model_deep,upscale,conv_core_num,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2):\n",
    "            import tensorflow as tf\n",
    "            from keras.models import Sequential,Model\n",
    "            import math\n",
    "            from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "            from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            import numpy as np\n",
    "            from tensorflow.keras.optimizers import SGD,Adam\n",
    "            from scipy.stats import pearsonr\n",
    "            from keras.models import load_model\n",
    "            import os\n",
    "            from keras.layers import Layer, InputSpec\n",
    "            from keras import initializers\n",
    "            from keras import regularizers\n",
    "            from keras import constraints\n",
    "            from keras import backend as K\n",
    "\n",
    "            from keras.utils.generic_utils import get_custom_objects\n",
    "            class GroupNormalization(Layer):\n",
    "                \"\"\"Group normalization layer\n",
    "\n",
    "                Group Normalization divides the channels into groups and computes within each group\n",
    "                the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
    "                and its accuracy is stable in a wide range of batch sizes\n",
    "\n",
    "                # Arguments\n",
    "                    groups: Integer, the number of groups for Group Normalization.\n",
    "                    axis: Integer, the axis that should be normalized\n",
    "                        (typically the features axis).\n",
    "                        For instance, after a `Conv2D` layer with\n",
    "                        `data_format=\"channels_first\"`,\n",
    "                        set `axis=1` in `BatchNormalization`.\n",
    "                    epsilon: Small float added to variance to avoid dividing by zero.\n",
    "                    center: If True, add offset of `beta` to normalized tensor.\n",
    "                        If False, `beta` is ignored.\n",
    "                    scale: If True, multiply by `gamma`.\n",
    "                        If False, `gamma` is not used.\n",
    "                        When the next layer is linear (also e.g. `nn.relu`),\n",
    "                        this can be disabled since the scaling\n",
    "                        will be done by the next layer.\n",
    "                    beta_initializer: Initializer for the beta weight.\n",
    "                    gamma_initializer: Initializer for the gamma weight.\n",
    "                    beta_regularizer: Optional regularizer for the beta weight.\n",
    "                    gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "                    beta_constraint: Optional constraint for the beta weight.\n",
    "                    gamma_constraint: Optional constraint for the gamma weight.\n",
    "\n",
    "                # Input shape\n",
    "                    Arbitrary. Use the keyword argument `input_shape`\n",
    "                    (tuple of integers, does not include the samples axis)\n",
    "                    when using this layer as the first layer in a model.\n",
    "\n",
    "                # Output shape\n",
    "                    Same shape as input.\n",
    "\n",
    "                # References\n",
    "                    - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "                \"\"\"\n",
    "\n",
    "                def __init__(self,\n",
    "                             groups=2,\n",
    "                             axis=-1,\n",
    "                             epsilon=1e-5,\n",
    "                             center=True,\n",
    "                             scale=True,\n",
    "                             beta_initializer='zeros',\n",
    "                             gamma_initializer='ones',\n",
    "                             beta_regularizer=None,\n",
    "                             gamma_regularizer=None,\n",
    "                             beta_constraint=None,\n",
    "                             gamma_constraint=None,\n",
    "                             **kwargs):\n",
    "                    super(GroupNormalization, self).__init__(**kwargs)\n",
    "                    self.supports_masking = True\n",
    "                    self.groups = groups\n",
    "                    self.axis = axis\n",
    "                    self.epsilon = epsilon\n",
    "                    self.center = center\n",
    "                    self.scale = scale\n",
    "                    self.beta_initializer = initializers.get(beta_initializer)\n",
    "                    self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "                    self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "                    self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "                    self.beta_constraint = constraints.get(beta_constraint)\n",
    "                    self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "                def build(self, input_shape):\n",
    "                    dim = input_shape[self.axis]\n",
    "\n",
    "                    if dim is None:\n",
    "                        raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                                         'input tensor should have a defined dimension '\n",
    "                                         'but the layer received an input with shape ' +\n",
    "                                         str(input_shape) + '.')\n",
    "\n",
    "                    if dim < self.groups:\n",
    "                        raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                                         'more than the number of channels (' +\n",
    "                                         str(dim) + ').')\n",
    "\n",
    "                    if dim % self.groups != 0:\n",
    "                        raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
    "                                         'multiple of the number of channels (' +\n",
    "                                         str(dim) + ').')\n",
    "\n",
    "                    self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                                axes={self.axis: dim})\n",
    "                    shape = (dim,)\n",
    "\n",
    "                    if self.scale:\n",
    "                        self.gamma = self.add_weight(shape=shape,\n",
    "                                                     name='gamma',\n",
    "                                                     initializer=self.gamma_initializer,\n",
    "                                                     regularizer=self.gamma_regularizer,\n",
    "                                                     constraint=self.gamma_constraint)\n",
    "                    else:\n",
    "                        self.gamma = None\n",
    "                    if self.center:\n",
    "                        self.beta = self.add_weight(shape=shape,\n",
    "                                                    name='beta',\n",
    "                                                    initializer=self.beta_initializer,\n",
    "                                                    regularizer=self.beta_regularizer,\n",
    "                                                    constraint=self.beta_constraint)\n",
    "                    else:\n",
    "                        self.beta = None\n",
    "                    self.built = True\n",
    "\n",
    "                def call(self, inputs, **kwargs):\n",
    "                    input_shape = K.int_shape(inputs)\n",
    "                    tensor_input_shape = K.shape(inputs)\n",
    "\n",
    "                    # Prepare broadcasting shape.\n",
    "                    reduction_axes = list(range(len(input_shape)))\n",
    "                    del reduction_axes[self.axis]\n",
    "                    broadcast_shape = [1] * len(input_shape)\n",
    "                    broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "                    broadcast_shape.insert(1, self.groups)\n",
    "\n",
    "                    reshape_group_shape = K.shape(inputs)\n",
    "                    group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "                    group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "                    group_axes.insert(1, self.groups)\n",
    "\n",
    "                    # reshape inputs to new group shape\n",
    "                    group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "                    group_shape = K.stack(group_shape)\n",
    "                    inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "                    group_reduction_axes = list(range(len(group_axes)))\n",
    "                    group_reduction_axes = group_reduction_axes[2:]\n",
    "\n",
    "                    mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "                    variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\n",
    "                    inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "                    # prepare broadcast shape\n",
    "                    inputs = K.reshape(inputs, group_shape)\n",
    "                    outputs = inputs\n",
    "\n",
    "                    # In this case we must explicitly broadcast all parameters.\n",
    "                    if self.scale:\n",
    "                        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                        outputs = outputs * broadcast_gamma\n",
    "\n",
    "                    if self.center:\n",
    "                        broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                        outputs = outputs + broadcast_beta\n",
    "\n",
    "                    outputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "                def get_config(self):\n",
    "                    config = {\n",
    "                        'groups': self.groups,\n",
    "                        'axis': self.axis,\n",
    "                        'epsilon': self.epsilon,\n",
    "                        'center': self.center,\n",
    "                        'scale': self.scale,\n",
    "                        'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "                        'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "                        'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "                        'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "                        'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "                        'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "                    }\n",
    "                    base_config = super(GroupNormalization, self).get_config()\n",
    "                    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "                def compute_output_shape(self, input_shape):\n",
    "                    return input_shape\n",
    "            \n",
    "            discriminator_inputs=Input(shape=(discriminator_input.shape[1],discriminator_input.shape[2],discriminator_input.shape[3]))\n",
    "            if if_weight_initialize=='no':\n",
    "                exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\")(discriminator_inputs)')\n",
    "            else:\n",
    "                if weight_initialize_method=='RandomNormal':\n",
    "                    exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_inputs)')\n",
    "                elif weight_initialize_method=='RandomUniform':\n",
    "                    exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_inputs)')\n",
    "                elif weight_initialize_method=='TruncatedNormal':\n",
    "                    exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_inputs)')\n",
    "            exec('discriminator_act_start_1=Activation(\"leaky_relu\")(discriminator_conv_start_1)')\n",
    "            if if_weight_initialize=='no':\n",
    "                exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\")(discriminator_act_start_1)')\n",
    "            else:\n",
    "                if weight_initialize_method=='RandomNormal':\n",
    "                    exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_1)')\n",
    "                elif weight_initialize_method=='RandomUniform':\n",
    "                    exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_act_start_1)')\n",
    "                elif weight_initialize_method=='TruncatedNormal':\n",
    "                    exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_1)')\n",
    "            exec('discriminator_act_start_2=Activation(\"leaky_relu\")(discriminator_conv_start_2)')\n",
    "            exec('discriminator_norm_start=GroupNormalization(groups=int(conv_core_num/(2**(model_deep))),axis=-1, epsilon=0.1)(discriminator_act_start_2)')\n",
    "            if if_weight_initialize=='no':\n",
    "                exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\")(discriminator_norm_start)')\n",
    "            else:\n",
    "                if weight_initialize_method=='RandomNormal':\n",
    "                    exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_start)')\n",
    "                elif weight_initialize_method=='RandomUniform':\n",
    "                    exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm_start)')\n",
    "                elif weight_initialize_method=='TruncatedNormal':\n",
    "                    exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_start)')\n",
    "            exec('discriminator_act_start_3=Activation(\"leaky_relu\")(discriminator_conv_start_3)')\n",
    "            exec('discriminator_pool_start_3=AveragePooling2D(pool_size=(upscale, upscale), strides=upscale, padding=\"valid\")(discriminator_act_start_3)')\n",
    "            exec('discriminator_act_start_4=Activation(\"leaky_relu\")(discriminator_pool_start_3)')\n",
    "            exec('discriminator_conc=Flatten()(discriminator_act_start_4)')\n",
    "            for i in range(model_deep):\n",
    "                if i!= model_deep-1:  \n",
    "                    if i==0:\n",
    "                        if if_weight_initialize=='no':\n",
    "                            exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\")(discriminator_act_start_4)')\n",
    "                        else:\n",
    "                            if weight_initialize_method=='RandomNormal':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_4)')\n",
    "                            elif weight_initialize_method=='RandomUniform':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_act_start_4)')\n",
    "                            elif weight_initialize_method=='TruncatedNormal':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_4)')\n",
    "                    else:\n",
    "                        if if_weight_initialize=='no':\n",
    "                            exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\")(discriminator_act'+str(i)+'_3)')\n",
    "                        else:\n",
    "                            if weight_initialize_method=='RandomNormal':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act'+str(i)+'_3)')\n",
    "                            elif weight_initialize_method=='RandomUniform':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_act'+str(i)+'_3)')\n",
    "                            elif weight_initialize_method=='TruncatedNormal':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act'+str(i)+'_3)')\n",
    "                    exec('discriminator_act'+str(i+1)+'_1=Activation(\"leaky_relu\")(discriminator_conv'+str(i+1)+'_1)')\n",
    "                    exec('discriminator_norm'+str(i+1)+'=GroupNormalization(groups=int(conv_core_num/(2**(model_deep-i-2))),axis=-1, epsilon=0.1)(discriminator_act'+str(i+1)+'_1)')\n",
    "                    if if_weight_initialize=='no':\n",
    "                        exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\")(discriminator_norm'+str(i+1)+')')\n",
    "                    else:\n",
    "                        if weight_initialize_method=='RandomNormal':\n",
    "                            exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm'+str(i+1)+')')\n",
    "                        elif weight_initialize_method=='RandomUniform':\n",
    "                            exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm'+str(i+1)+')')\n",
    "                        elif weight_initialize_method=='TruncatedNormal':\n",
    "                            exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm'+str(i+1)+')')\n",
    "                    exec('discriminator_act'+str(i+1)+'_2=Activation(\"leaky_relu\")(discriminator_conv'+str(i+1)+'_2)')\n",
    "                    exec('discriminator_pool'+str(i+1)+'=AveragePooling2D(pool_size=(upscale, upscale), strides=upscale, padding=\"valid\")(discriminator_act'+str(i+1)+'_2)')\n",
    "                    exec('discriminator_act'+str(i+1)+'_3=Activation(\"leaky_relu\")(discriminator_pool'+str(i+1)+')')\n",
    "                    exec('discriminator_conc=Concatenate()([discriminator_conc,Flatten()(discriminator_act'+str(i+1)+'_3)])')\n",
    "                else:\n",
    "                    if i==0:\n",
    "                        exec('discriminator_norm_last_1=BatchNormalization()(discriminator_act_start_4)')\n",
    "                    else:\n",
    "                        exec('discriminator_norm_last_1=BatchNormalization()(discriminator_act'+str(i)+'_3)')\n",
    "                    if if_weight_initialize=='no':\n",
    "                        exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\")(discriminator_norm_last_1)')\n",
    "                    else:\n",
    "                        if weight_initialize_method=='RandomNormal':\n",
    "                            exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_1)')\n",
    "                        elif weight_initialize_method=='RandomUniform':\n",
    "                            exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm_last_1)')\n",
    "                        elif weight_initialize_method=='TruncatedNormal':\n",
    "                            exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_1)')\n",
    "                    exec('discriminator_act_last_1=Activation(\"leaky_relu\")(discriminator_conv_last_1)')\n",
    "                    exec('discriminator_norm_last_2=GroupNormalization(groups=int(conv_core_num/(2**(model_deep-i-1))),axis=-1, epsilon=0.1)(discriminator_act_last_1)')\n",
    "                    if if_weight_initialize=='no':\n",
    "                        exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\")(discriminator_norm_last_2)')\n",
    "                    else:\n",
    "                        if weight_initialize_method=='RandomNormal':\n",
    "                            exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_2)')\n",
    "                        elif weight_initialize_method=='RandomUniform':\n",
    "                            exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm_last_2)')\n",
    "                        elif weight_initialize_method=='TruncatedNormal':\n",
    "                            exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_2)')\n",
    "                    exec('discriminator_act_last_2=Activation(\"leaky_relu\")(discriminator_conv_last_2)')\n",
    "                    exec('discriminator_conc=Concatenate()([discriminator_conc,Flatten()(discriminator_act_last_2)])')\n",
    "                    exec('discriminator_fc_1=Dense(int(conv_core_num/(2**(model_deep-i-1))))(discriminator_conc)')\n",
    "                    exec('discriminator_act_last_3=Activation(\"leaky_relu\")(discriminator_fc_1)')\n",
    "                    discriminator_output=eval('Dense(trainy.shape[3])(discriminator_act_last_3)')\n",
    "                    \n",
    "            return Model(inputs=discriminator_inputs, outputs=discriminator_output)\n",
    "        def build_Vgg_19(vgg_input,Vgg_deep):\n",
    "            import tensorflow as tf\n",
    "            from keras.models import Sequential,Model\n",
    "            import math\n",
    "            from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "            from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            import numpy as np\n",
    "            from tensorflow.keras.optimizers import SGD,Adam\n",
    "            from scipy.stats import pearsonr\n",
    "            from keras.models import load_model\n",
    "            import os\n",
    "            \n",
    "            vgg_inputs=Input(shape=(vgg_input.shape[1],vgg_input.shape[2],vgg_input.shape[3]))\n",
    "            hight=trainx.shape[1]\n",
    "            weight=trainx.shape[2]\n",
    "            if Vgg_deep>=5:\n",
    "                Vgg_deeps=5\n",
    "            else:\n",
    "                Vgg_deeps=Vgg_deep\n",
    "            for i in range(Vgg_deeps):\n",
    "                conv_core_nums=[64,128,256,512,512]\n",
    "                if i!=0 or i!=1:\n",
    "                    conv_block_len=4\n",
    "                else:\n",
    "                    conv_block_len=2\n",
    "                for j in range(conv_block_len):\n",
    "                    if i ==0:\n",
    "                        if j==0:\n",
    "                            exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_inputs)')\n",
    "                        else:\n",
    "                            exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_act'+str(i)+')')\n",
    "                    else:\n",
    "                        if j==0:\n",
    "                            exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_pool'+str(i-1)+')')\n",
    "                        else:\n",
    "                            exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_act'+str(i)+')')\n",
    "                    exec('vgg_norm'+str(i)+'=BatchNormalization(axis=-1)(vgg_conv'+str(i)+')')\n",
    "                    exec('vgg_act'+str(i)+'=Activation(\"relu\")(vgg_norm'+str(i)+')')\n",
    "                if i!=Vgg_deeps-1:\n",
    "                    exec('vgg_pool'+str(i)+'=MaxPooling2D(pool_size=(2,2),strides=2,padding=\"valid\")(vgg_act'+str(i)+')')\n",
    "                else:\n",
    "                    vgg_output=eval('MaxPooling2D(pool_size=(2,2),strides=2,padding=\"valid\")(vgg_act'+str(i)+')')\n",
    "            return Model(inputs=vgg_inputs, outputs=vgg_output)\n",
    "        if if_best_mode=='no':\n",
    "            generator=build_generator(trainy,trainx,model_deep,conv_core_num,upscale,se_radio,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2)\n",
    "        else:\n",
    "            generator=load_model(modelpath+'_generator')\n",
    "        generator_outputs=generator(trainx)\n",
    "        if if_best_mode=='no':\n",
    "            discriminator=build_discriminator(trainy,generator_outputs,model_deep,upscale,conv_core_num,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2)\n",
    "        else:\n",
    "            discriminator=load_model(modelpath+'_discriminator')\n",
    "        discriminator_outputs=discriminator(generator_outputs)\n",
    "        if if_best_mode=='no':\n",
    "            Vgg_19=build_Vgg_19(generator_outputs,Vgg_deep)\n",
    "        else:\n",
    "            Vgg_19=load_model(modelpath+'_Vgg_19')\n",
    "        Vgg_outputs=Vgg_19(generator_outputs)\n",
    "        ground_truth_trainy=[]\n",
    "        ground_truth_testy=[]\n",
    "        def generator_loss(y_true,y_pred):\n",
    "            import tensorflow as tf\n",
    "            \n",
    "            y_true=tf.cast(y_true,dtype=tf.float32)\n",
    "            y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "            y_true_mean=tf.reduce_mean(y_true,axis=0)\n",
    "            y_pred_mean=tf.reduce_mean(y_pred,axis=0)\n",
    "            cov=tf.reduce_sum((y_true-y_true_mean)*(y_pred-y_pred_mean),axis=0)\n",
    "            y_true_v=tf.reduce_sum(tf.square((y_true-y_true_mean)),axis=0)\n",
    "            y_pred_v=tf.reduce_sum(tf.square((y_pred-y_pred_mean)),axis=0)\n",
    "            y_true_v=tf.sqrt(y_true_v)\n",
    "            y_pred_v=tf.sqrt(y_pred_v)\n",
    "            pearson=tf.reduce_mean(cov/(y_true_v*y_pred_v))\n",
    "            result_true=discriminator(y_true)\n",
    "            result_false=discriminator(y_pred)\n",
    "            valid=np.ones((result_true.shape[0],result_true.shape[1]))\n",
    "            vgg_false=Vgg_19(y_pred)\n",
    "            vgg_true=Vgg_19(y_true)\n",
    "            bc=tf.keras.losses.BinaryCrossentropy()\n",
    "            bc_loss=tf.reduce_mean(bc(valid,tf.sigmoid(result_false - tf.reduce_mean(result_true,axis=0))))\n",
    "            mae=tf.keras.losses.MeanAbsoluteError()\n",
    "            mae_feature_loss=tf.reduce_mean(mae(vgg_true,vgg_false))\n",
    "            mae_loss=tf.reduce_mean(mae(y_true,y_pred))\n",
    "            ssim_loss=tf.reduce_mean(tf.image.ssim(y_pred,y_true,max_val=(tf.reduce_max(y_true)-tf.reduce_min(y_true))))\n",
    "            if loss_function=='default' or loss_function=='Vgg+SSIM' or loss_function=='SSIM+Vgg':\n",
    "                return (1-ssim_loss)+mae_feature_loss+0.005*bc_loss+0.01*mae_loss\n",
    "            elif loss_function=='Vgg':\n",
    "                return mae_feature_loss+0.005*bc_loss+0.01*mae_loss\n",
    "            elif loss_function=='SSIM':\n",
    "                return (1-ssim_loss)+0.005*bc_loss+0.01*mae_loss\n",
    "            elif loss_function=='Pearson':\n",
    "                return (1-pearson)+0.005*bc_loss+0.01*mae_loss\n",
    "            elif loss_function=='Pearson+Vgg' or loss_function=='Vgg+Pearson':\n",
    "                return (1-pearson)+mae_feature_loss+0.005*bc_loss+0.01*mae_loss\n",
    "        def generator_metrics(y_true,y_pred):\n",
    "            import tensorflow as tf\n",
    "            y_true=tf.cast(y_true,dtype=tf.float32)\n",
    "            y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "            y_true_mean=tf.reduce_mean(y_true,axis=0)\n",
    "            y_pred_mean=tf.reduce_mean(y_pred,axis=0)\n",
    "            cov=tf.reduce_sum((y_true-y_true_mean)*(y_pred-y_pred_mean),axis=0)\n",
    "            y_true_v=tf.reduce_sum(tf.square((y_true-y_true_mean)),axis=0)\n",
    "            y_pred_v=tf.reduce_sum(tf.square((y_pred-y_pred_mean)),axis=0)\n",
    "            y_true_v=tf.sqrt(y_true_v)\n",
    "            y_pred_v=tf.sqrt(y_pred_v)\n",
    "            pearson=tf.reduce_mean(cov/(y_true_v*y_pred_v))\n",
    "            return pearson\n",
    "        def discriminator_loss(y_true,y_pred):\n",
    "            import tensorflow as tf\n",
    "            y_true=tf.cast(y_true,dtype=tf.float32)\n",
    "            y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "            result_true=y_pred[:int(y_pred.shape[0]/2.0)]\n",
    "            result_false=y_pred[int(y_pred.shape[0]/2.0):]\n",
    "            bc=tf.keras.losses.BinaryCrossentropy()\n",
    "            bc_loss_false=tf.reduce_mean(bc(y_true[int(y_pred.shape[0]/2.0):],tf.sigmoid(result_false - tf.reduce_mean(result_true,axis=0))))\n",
    "            bc_loss_true=tf.reduce_mean(bc(y_true[:int(y_pred.shape[0]/2.0)],tf.sigmoid(result_true - tf.reduce_mean(result_false,axis=0))))\n",
    "            return (bc_loss_false+bc_loss_true)/2.0\n",
    "        generator.compile(loss=generator_loss,optimizer=d_opt,metrics=generator_metrics)\n",
    "        discriminator.compile(loss=discriminator_loss,optimizer=d_opt,metrics=['accuracy'])\n",
    "        if if_print_model=='yes':\n",
    "            print(discriminator.summary())\n",
    "            print(generator.summary())\n",
    "            print(Vgg_19.summary())\n",
    "        valid_train=np.ones((trainx.shape[0],vy.shape[3]))\n",
    "        valid_test=np.ones((testx.shape[0],vy.shape[3]))\n",
    "        fake_train=np.zeros((trainx.shape[0],vy.shape[3]))\n",
    "        fake_test=np.zeros((testx.shape[0],vy.shape[3]))\n",
    "        for i in range(epochs):\n",
    "            generator_result=generator.predict(trainx)\n",
    "            generator_predict=generator.predict(testx)\n",
    "            label_train=np.append(valid_train,fake_train,axis=0)\n",
    "            factor_train=np.append(trainy,generator_result,axis=0)\n",
    "            label_test=np.append(valid_test,fake_test,axis=0)\n",
    "            factor_test=np.append(testy,generator_predict,axis=0)\n",
    "            d_loss_train=discriminator.train_on_batch(factor_train,label_train)\n",
    "            for j in range(g_train_time):\n",
    "                g_loss_train=generator.train_on_batch(trainx,trainy)\n",
    "            d_predict=discriminator.predict(factor_test)\n",
    "            d_loss_test=discriminator_loss(label_test,d_predict)\n",
    "            d_acc_test=accuracy_score(label_test,np.where(tf.sigmoid(d_predict)>=0.5,1.0,0.0))\n",
    "            g_loss_test=generator_loss(testy,generator_predict)\n",
    "            g_pearson_test=generator_metrics(testy,generator_predict)\n",
    "            if ifmute=='no':\n",
    "                print('第',i+1,'次训练','D loss_train:',d_loss_train[0],'D acc_train:',100*d_loss_train[1],'G loss_train:',g_loss_train[0],'G pearson_train:',g_loss_train[1])\n",
    "                print('第',i+1,'次测试','D loss_test:',np.array(d_loss_test),'D acc_test:',100*d_acc_test,'G loss_test:',np.array(g_loss_test),'G pearson_test:',np.array(g_pearson_test))\n",
    "        predicty=np.array(generator.predict(testx)).reshape(testy.shape[0],testy.shape[1],testy.shape[2],testy.shape[3])\n",
    "        r=np.zeros((testy.shape[1],testy.shape[2],testy.shape[3]))\n",
    "        p=np.zeros((testy.shape[1],testy.shape[2],testy.shape[3]))\n",
    "        for i in range(testy.shape[1]):\n",
    "            for j in range(testy.shape[2]):\n",
    "                for k in range(testy.shape[3]):\n",
    "                    r[i,j,k],p[i,j,k]=pearsonr(predicty[:,i,j,k],testy[:,i,j,k])\n",
    "        print('相关系数',np.nanmean(r,axis=(0,1)))\n",
    "        if ifsave=='yes':\n",
    "            generator.save(savepath+'_generator')\n",
    "            discriminator.save(savepath+'_discriminator')\n",
    "            Vgg_19.save(savepath+'_Vgg_19')\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            if optimizer == 'SGD':\n",
    "                g_opt = SGD(lr = g_learning_rate)\n",
    "                d_opt = SGD(lr = d_learning_rate)\n",
    "            elif optimizer == 'Adam':\n",
    "                g_opt = Adam(lr = g_learning_rate)\n",
    "                d_opt = Adam(lr = d_learning_rate)\n",
    "            def build_generator(trainy,generator_input,model_deep,conv_core_num,upscale,se_radio,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2):\n",
    "                import tensorflow as tf\n",
    "                from keras.models import Sequential,Model\n",
    "                import math\n",
    "                from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "                from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                import numpy as np\n",
    "                from tensorflow.keras.optimizers import SGD,Adam\n",
    "                from scipy.stats import pearsonr\n",
    "                from keras.models import load_model\n",
    "                import os\n",
    "                generator_inputs=Input(shape=(generator_input.shape[1],generator_input.shape[2],vx.shape[3]))\n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\")(generator_inputs)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_inputs)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_inputs)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('generator_conv_start_1=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_inputs)')\n",
    "                exec('generator_act_start_1=Activation(\"leaky_relu\")(generator_conv_start_1)')\n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\")(generator_act_start_1)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act_start_1)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_act_start_1)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('generator_conv_start_2=Conv2D(conv_core_num,(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act_start_1)')\n",
    "                exec('generator_act_start_2=Activation(\"leaky_relu\")(generator_conv_start_2)')\n",
    "                exec('segap_start=GlobalAveragePooling2D()(generator_act_start_2)')\n",
    "                exec('sefc_start_1=Dense(int(conv_core_num*se_radio))(segap_start)')\n",
    "                exec('seact_start_1=Activation(\"leaky_relu\")(sefc_start_1)')\n",
    "                exec('sefc_start_2=Dense(conv_core_num)(seact_start_1)')\n",
    "                exec('seact_start_2=Activation(\"leaky_relu\")(sefc_start_2)')\n",
    "                exec('semulti_start=Multiply()([generator_act_start_2,seact_start_2])')\n",
    "                exec('seadd_start=Add()([semulti_start,generator_act_start_2])')\n",
    "                for i in range(model_deep):\n",
    "                    if i==0:\n",
    "                        exec('generator_upsample_'+str(i+1)+'=UpSampling2D(size=(upscale,upscale))(seadd_start)')\n",
    "                    else:\n",
    "                        exec('generator_upsample_'+str(i+1)+'=UpSampling2D(size=(upscale,upscale))(generator_act'+str(i)+'_2)')             \n",
    "                    if if_weight_initialize=='no':\n",
    "                        exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\")(generator_upsample_'+str(i+1)+')')\n",
    "                    else:\n",
    "                        if weight_initialize_method=='RandomNormal':\n",
    "                            exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_upsample_'+str(i+1)+')')\n",
    "                        elif weight_initialize_method=='RandomUniform':\n",
    "                            exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_upsample_'+str(i+1)+')')\n",
    "                        elif weight_initialize_method=='TruncatedNormal':\n",
    "                            exec('generator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_upsample_'+str(i+1)+')')\n",
    "                    exec('generator_act'+str(i+1)+'_1=Activation(\"leaky_relu\")(generator_conv'+str(i+1)+'_1)')\n",
    "                    if if_weight_initialize=='no':\n",
    "                        exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\")(generator_act'+str(i+1)+'_1)')\n",
    "                    else:\n",
    "                        if weight_initialize_method=='RandomNormal':\n",
    "                            exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_1)')\n",
    "                        elif weight_initialize_method=='RandomUniform':\n",
    "                            exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_act'+str(i+1)+'_1)')\n",
    "                        elif weight_initialize_method=='TruncatedNormal':\n",
    "                            exec('generator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(i+1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_1)')\n",
    "                    exec('generator_act'+str(i+1)+'_2=Activation(\"leaky_relu\")(generator_conv'+str(i+1)+'_2)')\n",
    "                if if_weight_initialize=='no':\n",
    "                    generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\")(generator_act'+str(i+1)+'_2)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_2)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(generator_act'+str(i+1)+'_2)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        generator_output=eval('Conv2D(trainy.shape[3],(1,1),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(generator_act'+str(i+1)+'_2)')\n",
    "                return Model(inputs=generator_inputs, outputs=generator_output)\n",
    "            def build_discriminator(trainy,discriminator_input,model_deep,upscale,conv_core_num,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2):\n",
    "                import tensorflow as tf\n",
    "                from keras.models import Sequential,Model\n",
    "                import math\n",
    "                from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "                from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                import numpy as np\n",
    "                from tensorflow.keras.optimizers import SGD,Adam\n",
    "                from scipy.stats import pearsonr\n",
    "                from keras.models import load_model\n",
    "                import os\n",
    "                from keras.layers import Layer, InputSpec\n",
    "                from keras import initializers\n",
    "                from keras import regularizers\n",
    "                from keras import constraints\n",
    "                from keras import backend as K\n",
    "\n",
    "                from keras.utils.generic_utils import get_custom_objects\n",
    "                class GroupNormalization(Layer):\n",
    "                    \"\"\"Group normalization layer\n",
    "\n",
    "                    Group Normalization divides the channels into groups and computes within each group\n",
    "                    the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
    "                    and its accuracy is stable in a wide range of batch sizes\n",
    "\n",
    "                    # Arguments\n",
    "                        groups: Integer, the number of groups for Group Normalization.\n",
    "                        axis: Integer, the axis that should be normalized\n",
    "                            (typically the features axis).\n",
    "                            For instance, after a `Conv2D` layer with\n",
    "                            `data_format=\"channels_first\"`,\n",
    "                            set `axis=1` in `BatchNormalization`.\n",
    "                        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "                        center: If True, add offset of `beta` to normalized tensor.\n",
    "                            If False, `beta` is ignored.\n",
    "                        scale: If True, multiply by `gamma`.\n",
    "                            If False, `gamma` is not used.\n",
    "                            When the next layer is linear (also e.g. `nn.relu`),\n",
    "                            this can be disabled since the scaling\n",
    "                            will be done by the next layer.\n",
    "                        beta_initializer: Initializer for the beta weight.\n",
    "                        gamma_initializer: Initializer for the gamma weight.\n",
    "                        beta_regularizer: Optional regularizer for the beta weight.\n",
    "                        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "                        beta_constraint: Optional constraint for the beta weight.\n",
    "                        gamma_constraint: Optional constraint for the gamma weight.\n",
    "\n",
    "                    # Input shape\n",
    "                        Arbitrary. Use the keyword argument `input_shape`\n",
    "                        (tuple of integers, does not include the samples axis)\n",
    "                        when using this layer as the first layer in a model.\n",
    "\n",
    "                    # Output shape\n",
    "                        Same shape as input.\n",
    "\n",
    "                    # References\n",
    "                        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "                    \"\"\"\n",
    "\n",
    "                    def __init__(self,\n",
    "                                 groups=2,\n",
    "                                 axis=-1,\n",
    "                                 epsilon=1e-5,\n",
    "                                 center=True,\n",
    "                                 scale=True,\n",
    "                                 beta_initializer='zeros',\n",
    "                                 gamma_initializer='ones',\n",
    "                                 beta_regularizer=None,\n",
    "                                 gamma_regularizer=None,\n",
    "                                 beta_constraint=None,\n",
    "                                 gamma_constraint=None,\n",
    "                                 **kwargs):\n",
    "                        super(GroupNormalization, self).__init__(**kwargs)\n",
    "                        self.supports_masking = True\n",
    "                        self.groups = groups\n",
    "                        self.axis = axis\n",
    "                        self.epsilon = epsilon\n",
    "                        self.center = center\n",
    "                        self.scale = scale\n",
    "                        self.beta_initializer = initializers.get(beta_initializer)\n",
    "                        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "                        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "                        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "                        self.beta_constraint = constraints.get(beta_constraint)\n",
    "                        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "                    def build(self, input_shape):\n",
    "                        dim = input_shape[self.axis]\n",
    "\n",
    "                        if dim is None:\n",
    "                            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                                             'input tensor should have a defined dimension '\n",
    "                                             'but the layer received an input with shape ' +\n",
    "                                             str(input_shape) + '.')\n",
    "\n",
    "                        if dim < self.groups:\n",
    "                            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                                             'more than the number of channels (' +\n",
    "                                             str(dim) + ').')\n",
    "\n",
    "                        if dim % self.groups != 0:\n",
    "                            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
    "                                             'multiple of the number of channels (' +\n",
    "                                             str(dim) + ').')\n",
    "\n",
    "                        self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                                    axes={self.axis: dim})\n",
    "                        shape = (dim,)\n",
    "\n",
    "                        if self.scale:\n",
    "                            self.gamma = self.add_weight(shape=shape,\n",
    "                                                         name='gamma',\n",
    "                                                         initializer=self.gamma_initializer,\n",
    "                                                         regularizer=self.gamma_regularizer,\n",
    "                                                         constraint=self.gamma_constraint)\n",
    "                        else:\n",
    "                            self.gamma = None\n",
    "                        if self.center:\n",
    "                            self.beta = self.add_weight(shape=shape,\n",
    "                                                        name='beta',\n",
    "                                                        initializer=self.beta_initializer,\n",
    "                                                        regularizer=self.beta_regularizer,\n",
    "                                                        constraint=self.beta_constraint)\n",
    "                        else:\n",
    "                            self.beta = None\n",
    "                        self.built = True\n",
    "\n",
    "                    def call(self, inputs, **kwargs):\n",
    "                        input_shape = K.int_shape(inputs)\n",
    "                        tensor_input_shape = K.shape(inputs)\n",
    "\n",
    "                        # Prepare broadcasting shape.\n",
    "                        reduction_axes = list(range(len(input_shape)))\n",
    "                        del reduction_axes[self.axis]\n",
    "                        broadcast_shape = [1] * len(input_shape)\n",
    "                        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "                        broadcast_shape.insert(1, self.groups)\n",
    "\n",
    "                        reshape_group_shape = K.shape(inputs)\n",
    "                        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "                        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "                        group_axes.insert(1, self.groups)\n",
    "\n",
    "                        # reshape inputs to new group shape\n",
    "                        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "                        group_shape = K.stack(group_shape)\n",
    "                        inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "                        group_reduction_axes = list(range(len(group_axes)))\n",
    "                        group_reduction_axes = group_reduction_axes[2:]\n",
    "\n",
    "                        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "                        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\n",
    "                        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "                        # prepare broadcast shape\n",
    "                        inputs = K.reshape(inputs, group_shape)\n",
    "                        outputs = inputs\n",
    "\n",
    "                        # In this case we must explicitly broadcast all parameters.\n",
    "                        if self.scale:\n",
    "                            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                            outputs = outputs * broadcast_gamma\n",
    "\n",
    "                        if self.center:\n",
    "                            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                            outputs = outputs + broadcast_beta\n",
    "\n",
    "                        outputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "                        return outputs\n",
    "\n",
    "                    def get_config(self):\n",
    "                        config = {\n",
    "                            'groups': self.groups,\n",
    "                            'axis': self.axis,\n",
    "                            'epsilon': self.epsilon,\n",
    "                            'center': self.center,\n",
    "                            'scale': self.scale,\n",
    "                            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "                            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "                            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "                            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "                            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "                            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "                        }\n",
    "                        base_config = super(GroupNormalization, self).get_config()\n",
    "                        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "                    def compute_output_shape(self, input_shape):\n",
    "                        return input_shape\n",
    "\n",
    "                discriminator_inputs=Input(shape=(discriminator_input.shape[1],discriminator_input.shape[2],discriminator_input.shape[3]))\n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\")(discriminator_inputs)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_inputs)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_inputs)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('discriminator_conv_start_1=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_inputs)')\n",
    "                exec('discriminator_act_start_1=Activation(\"leaky_relu\")(discriminator_conv_start_1)')\n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\")(discriminator_act_start_1)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_1)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_act_start_1)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('discriminator_conv_start_2=Conv2D(int(conv_core_num/(2**(model_deep))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_1)')\n",
    "                exec('discriminator_act_start_2=Activation(\"leaky_relu\")(discriminator_conv_start_2)')\n",
    "                exec('discriminator_norm_start=GroupNormalization(groups=int(conv_core_num/(2**(model_deep))),axis=-1, epsilon=0.1)(discriminator_act_start_2)')\n",
    "                if if_weight_initialize=='no':\n",
    "                    exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\")(discriminator_norm_start)')\n",
    "                else:\n",
    "                    if weight_initialize_method=='RandomNormal':\n",
    "                        exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_start)')\n",
    "                    elif weight_initialize_method=='RandomUniform':\n",
    "                        exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm_start)')\n",
    "                    elif weight_initialize_method=='TruncatedNormal':\n",
    "                        exec('discriminator_conv_start_3=Conv2D(int(conv_core_num/(2**(model_deep-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_start)')\n",
    "                exec('discriminator_act_start_3=Activation(\"leaky_relu\")(discriminator_conv_start_3)')\n",
    "                exec('discriminator_pool_start_3=AveragePooling2D(pool_size=(upscale, upscale), strides=upscale, padding=\"valid\")(discriminator_act_start_3)')\n",
    "                exec('discriminator_act_start_4=Activation(\"leaky_relu\")(discriminator_pool_start_3)')\n",
    "                exec('discriminator_conc=Flatten()(discriminator_act_start_4)')\n",
    "                for i in range(model_deep):\n",
    "                    if i!= model_deep-1:  \n",
    "                        if i==0:\n",
    "                            if if_weight_initialize=='no':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\")(discriminator_act_start_4)')\n",
    "                            else:\n",
    "                                if weight_initialize_method=='RandomNormal':\n",
    "                                    exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_4)')\n",
    "                                elif weight_initialize_method=='RandomUniform':\n",
    "                                    exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_act_start_4)')\n",
    "                                elif weight_initialize_method=='TruncatedNormal':\n",
    "                                    exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act_start_4)')\n",
    "                        else:\n",
    "                            if if_weight_initialize=='no':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\")(discriminator_act'+str(i)+'_3)')\n",
    "                            else:\n",
    "                                if weight_initialize_method=='RandomNormal':\n",
    "                                    exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act'+str(i)+'_3)')\n",
    "                                elif weight_initialize_method=='RandomUniform':\n",
    "                                    exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_act'+str(i)+'_3)')\n",
    "                                elif weight_initialize_method=='TruncatedNormal':\n",
    "                                    exec('discriminator_conv'+str(i+1)+'_1=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_act'+str(i)+'_3)')\n",
    "                        exec('discriminator_act'+str(i+1)+'_1=Activation(\"leaky_relu\")(discriminator_conv'+str(i+1)+'_1)')\n",
    "                        exec('discriminator_norm'+str(i+1)+'=GroupNormalization(groups=int(conv_core_num/(2**(model_deep-i-2))),axis=-1, epsilon=0.1)(discriminator_act'+str(i+1)+'_1)')\n",
    "                        if if_weight_initialize=='no':\n",
    "                            exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\")(discriminator_norm'+str(i+1)+')')\n",
    "                        else:\n",
    "                            if weight_initialize_method=='RandomNormal':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm'+str(i+1)+')')\n",
    "                            elif weight_initialize_method=='RandomUniform':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm'+str(i+1)+')')\n",
    "                            elif weight_initialize_method=='TruncatedNormal':\n",
    "                                exec('discriminator_conv'+str(i+1)+'_2=Conv2D(int(conv_core_num/(2**(model_deep-i-2))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm'+str(i+1)+')')\n",
    "                        exec('discriminator_act'+str(i+1)+'_2=Activation(\"leaky_relu\")(discriminator_conv'+str(i+1)+'_2)')\n",
    "                        exec('discriminator_pool'+str(i+1)+'=AveragePooling2D(pool_size=(upscale, upscale), strides=upscale, padding=\"valid\")(discriminator_act'+str(i+1)+'_2)')\n",
    "                        exec('discriminator_act'+str(i+1)+'_3=Activation(\"leaky_relu\")(discriminator_pool'+str(i+1)+')')\n",
    "                        exec('discriminator_conc=Concatenate()([discriminator_conc,Flatten()(discriminator_act'+str(i+1)+'_3)])')\n",
    "                    else:\n",
    "                        if i==0:\n",
    "                            exec('discriminator_norm_last_1=BatchNormalization()(discriminator_act_start_4)')\n",
    "                        else:\n",
    "                            exec('discriminator_norm_last_1=BatchNormalization()(discriminator_act'+str(i)+'_3)')\n",
    "                        if if_weight_initialize=='no':\n",
    "                            exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\")(discriminator_norm_last_1)')\n",
    "                        else:\n",
    "                            if weight_initialize_method=='RandomNormal':\n",
    "                                exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_1)')\n",
    "                            elif weight_initialize_method=='RandomUniform':\n",
    "                                exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm_last_1)')\n",
    "                            elif weight_initialize_method=='TruncatedNormal':\n",
    "                                exec('discriminator_conv_last_1=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_1)')\n",
    "                        exec('discriminator_act_last_1=Activation(\"leaky_relu\")(discriminator_conv_last_1)')\n",
    "                        exec('discriminator_norm_last_2=GroupNormalization(groups=int(conv_core_num/(2**(model_deep-i-1))),axis=-1, epsilon=0.1)(discriminator_act_last_1)')\n",
    "                        if if_weight_initialize=='no':\n",
    "                            exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\")(discriminator_norm_last_2)')\n",
    "                        else:\n",
    "                            if weight_initialize_method=='RandomNormal':\n",
    "                                exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_2)')\n",
    "                            elif weight_initialize_method=='RandomUniform':\n",
    "                                exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = RandomUniform(minval=weight_initialize_parameter1,maxval=weight_initialize_parameter2))(discriminator_norm_last_2)')\n",
    "                            elif weight_initialize_method=='TruncatedNormal':\n",
    "                                exec('discriminator_conv_last_2=Conv2D(int(conv_core_num/(2**(model_deep-i-1))),(3,3),strides=1,padding=\"same\",kernel_initializer = TruncatedNormal(mean=weight_initialize_parameter1,stddev=weight_initialize_parameter2))(discriminator_norm_last_2)')\n",
    "                        exec('discriminator_act_last_2=Activation(\"leaky_relu\")(discriminator_conv_last_2)')\n",
    "                        exec('discriminator_conc=Concatenate()([discriminator_conc,Flatten()(discriminator_act_last_2)])')\n",
    "                        exec('discriminator_fc_1=Dense(int(conv_core_num/(2**(model_deep-i-1))))(discriminator_conc)')\n",
    "                        exec('discriminator_act_last_3=Activation(\"leaky_relu\")(discriminator_fc_1)')\n",
    "                        discriminator_output=eval('Dense(trainy.shape[3])(discriminator_act_last_3)')\n",
    "\n",
    "                return Model(inputs=discriminator_inputs, outputs=discriminator_output)\n",
    "            def build_Vgg_19(vgg_input,Vgg_deep):\n",
    "                import tensorflow as tf\n",
    "                from keras.models import Sequential,Model\n",
    "                import math\n",
    "                from keras.initializers import TruncatedNormal,RandomNormal,RandomUniform\n",
    "                from keras.layers import BatchNormalization,LayerNormalization,LocallyConnected2D,Conv2D,MaxPooling2D,AveragePooling2D,Input,UpSampling2D,ZeroPadding2D,UpSampling2D,Add,Flatten,Activation,Dropout,Dense,Concatenate,GlobalAveragePooling2D,Multiply\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                import numpy as np\n",
    "                from tensorflow.keras.optimizers import SGD,Adam\n",
    "                from scipy.stats import pearsonr\n",
    "                from keras.models import load_model\n",
    "                import os\n",
    "\n",
    "                vgg_inputs=Input(shape=(vgg_input.shape[1],vgg_input.shape[2],vgg_input.shape[3]))\n",
    "                hight=trainx.shape[1]\n",
    "                weight=trainx.shape[2]\n",
    "                if Vgg_deep>=5:\n",
    "                    Vgg_deeps=5\n",
    "                else:\n",
    "                    Vgg_deeps=Vgg_deep\n",
    "                for i in range(Vgg_deeps):\n",
    "                    conv_core_nums=[64,128,256,512,512]\n",
    "                    if i!=0 or i!=1:\n",
    "                        conv_block_len=4\n",
    "                    else:\n",
    "                        conv_block_len=2\n",
    "                    for j in range(conv_block_len):\n",
    "                        if i ==0:\n",
    "                            if j==0:\n",
    "                                exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_inputs)')\n",
    "                            else:\n",
    "                                exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_act'+str(i)+')')\n",
    "                        else:\n",
    "                            if j==0:\n",
    "                                exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_pool'+str(i-1)+')')\n",
    "                            else:\n",
    "                                exec('vgg_conv'+str(i)+'=Conv2D(conv_core_nums[i],(3,3),strides=1,padding=\"same\")(vgg_act'+str(i)+')')\n",
    "                        exec('vgg_norm'+str(i)+'=BatchNormalization(axis=-1)(vgg_conv'+str(i)+')')\n",
    "                        exec('vgg_act'+str(i)+'=Activation(\"relu\")(vgg_norm'+str(i)+')')\n",
    "                    if i!=Vgg_deeps-1:\n",
    "                        exec('vgg_pool'+str(i)+'=MaxPooling2D(pool_size=(2,2),strides=2,padding=\"valid\")(vgg_act'+str(i)+')')\n",
    "                    else:\n",
    "                        vgg_output=eval('MaxPooling2D(pool_size=(2,2),strides=2,padding=\"valid\")(vgg_act'+str(i)+')')\n",
    "                return Model(inputs=vgg_inputs, outputs=vgg_output)\n",
    "            if if_best_mode=='no':\n",
    "                generator=build_generator(trainy,trainx,model_deep,conv_core_num,upscale,se_radio,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2)\n",
    "            else:\n",
    "                generator=load_model(modelpath+'_generator')\n",
    "            generator_outputs=generator(trainx)\n",
    "            if if_best_mode=='no':\n",
    "                discriminator=build_discriminator(trainy,generator_outputs,model_deep,upscale,conv_core_num,if_weight_initialize,weight_initialize_parameter1,weight_initialize_parameter2)\n",
    "            else:\n",
    "                discriminator=load_model(modelpath+'_discriminator')\n",
    "            discriminator_outputs=discriminator(generator_outputs)\n",
    "            if if_best_mode=='no':\n",
    "                Vgg_19=build_Vgg_19(generator_outputs,Vgg_deep)\n",
    "            else:\n",
    "                Vgg_19=load_model(modelpath+'_Vgg_19')\n",
    "            Vgg_outputs=Vgg_19(generator_outputs)\n",
    "            ground_truth_trainy=[]\n",
    "            ground_truth_testy=[]\n",
    "            def generator_loss(y_true,y_pred):\n",
    "                import tensorflow as tf\n",
    "\n",
    "                y_true=tf.cast(y_true,dtype=tf.float32)\n",
    "                y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "                y_true_mean=tf.reduce_mean(y_true,axis=0)\n",
    "                y_pred_mean=tf.reduce_mean(y_pred,axis=0)\n",
    "                cov=tf.reduce_sum((y_true-y_true_mean)*(y_pred-y_pred_mean),axis=0)\n",
    "                y_true_v=tf.reduce_sum(tf.square((y_true-y_true_mean)),axis=0)\n",
    "                y_pred_v=tf.reduce_sum(tf.square((y_pred-y_pred_mean)),axis=0)\n",
    "                y_true_v=tf.sqrt(y_true_v)\n",
    "                y_pred_v=tf.sqrt(y_pred_v)\n",
    "                pearson=tf.reduce_mean(cov/(y_true_v*y_pred_v))\n",
    "                result_true=discriminator(y_true)\n",
    "                result_false=discriminator(y_pred)\n",
    "                valid=np.ones((result_true.shape[0],result_true.shape[1]))\n",
    "                vgg_false=Vgg_19(y_pred)\n",
    "                vgg_true=Vgg_19(y_true)\n",
    "                bc=tf.keras.losses.BinaryCrossentropy()\n",
    "                bc_loss=tf.reduce_mean(bc(valid,tf.sigmoid(result_false - tf.reduce_mean(result_true,axis=0))))\n",
    "                mae=tf.keras.losses.MeanAbsoluteError()\n",
    "                mae_feature_loss=tf.reduce_mean(mae(vgg_true,vgg_false))\n",
    "                mae_loss=tf.reduce_mean(mae(y_true,y_pred))\n",
    "                ssim_loss=tf.reduce_mean(tf.image.ssim(y_pred,y_true,max_val=(tf.reduce_max(y_true)-tf.reduce_min(y_true))))\n",
    "                if loss_function=='default' or loss_function=='Vgg+SSIM' or loss_function=='SSIM+Vgg':\n",
    "                    return (1-ssim_loss)+mae_feature_loss+0.005*bc_loss+0.01*mae_loss\n",
    "                elif loss_function=='Vgg':\n",
    "                    return mae_feature_loss+0.005*bc_loss+0.01*mae_loss\n",
    "                elif loss_function=='SSIM':\n",
    "                    return (1-ssim_loss)+0.005*bc_loss+0.01*mae_loss\n",
    "                elif loss_function=='Pearson':\n",
    "                    return (1-pearson)+0.005*bc_loss+0.01*mae_loss\n",
    "                elif loss_function=='Pearson+Vgg' or loss_function=='Vgg+Pearson':\n",
    "                    return (1-pearson)+mae_feature_loss+0.005*bc_loss+0.01*mae_loss\n",
    "            def generator_metrics(y_true,y_pred):\n",
    "                import tensorflow as tf\n",
    "                y_true=tf.cast(y_true,dtype=tf.float32)\n",
    "                y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "                y_true_mean=tf.reduce_mean(y_true,axis=0)\n",
    "                y_pred_mean=tf.reduce_mean(y_pred,axis=0)\n",
    "                cov=tf.reduce_sum((y_true-y_true_mean)*(y_pred-y_pred_mean),axis=0)\n",
    "                y_true_v=tf.reduce_sum(tf.square((y_true-y_true_mean)),axis=0)\n",
    "                y_pred_v=tf.reduce_sum(tf.square((y_pred-y_pred_mean)),axis=0)\n",
    "                y_true_v=tf.sqrt(y_true_v)\n",
    "                y_pred_v=tf.sqrt(y_pred_v)\n",
    "                pearson=tf.reduce_mean(cov/(y_true_v*y_pred_v))\n",
    "                return pearson\n",
    "            def discriminator_loss(y_true,y_pred):\n",
    "                import tensorflow as tf\n",
    "                y_true=tf.cast(y_true,dtype=tf.float32)\n",
    "                y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "                result_true=y_pred[:int(y_pred.shape[0]/2.0)]\n",
    "                result_false=y_pred[int(y_pred.shape[0]/2.0):]\n",
    "                bc=tf.keras.losses.BinaryCrossentropy()\n",
    "                bc_loss_false=tf.reduce_mean(bc(y_true[int(y_pred.shape[0]/2.0):],tf.sigmoid(result_false - tf.reduce_mean(result_true,axis=0))))\n",
    "                bc_loss_true=tf.reduce_mean(bc(y_true[:int(y_pred.shape[0]/2.0)],tf.sigmoid(result_true - tf.reduce_mean(result_false,axis=0))))\n",
    "                return (bc_loss_false+bc_loss_true)/2.0\n",
    "            generator.compile(loss=generator_loss,optimizer=d_opt,metrics=generator_metrics)\n",
    "            discriminator.compile(loss=discriminator_loss,optimizer=d_opt,metrics=['accuracy'])\n",
    "            if if_print_model=='yes':\n",
    "                print(discriminator.summary())\n",
    "                print(generator.summary())\n",
    "                print(Vgg_19.summary())\n",
    "            valid_train=np.ones((trainx.shape[0],vy.shape[3]))\n",
    "            valid_test=np.ones((testx.shape[0],vy.shape[3]))\n",
    "            fake_train=np.zeros((trainx.shape[0],vy.shape[3]))\n",
    "            fake_test=np.zeros((testx.shape[0],vy.shape[3]))\n",
    "            for i in range(epochs):\n",
    "                generator_result=generator.predict(trainx)\n",
    "                generator_predict=generator.predict(testx)\n",
    "                label_train=np.append(valid_train,fake_train,axis=0)\n",
    "                factor_train=np.append(trainy,generator_result,axis=0)\n",
    "                label_test=np.append(valid_test,fake_test,axis=0)\n",
    "                factor_test=np.append(testy,generator_predict,axis=0)\n",
    "                d_loss_train=discriminator.train_on_batch(factor_train,label_train)\n",
    "                for j in range(g_train_time):\n",
    "                    g_loss_train=generator.train_on_batch(trainx,trainy)\n",
    "                d_predict=discriminator.predict(factor_test)\n",
    "                d_loss_test=discriminator_loss(label_test,d_predict)\n",
    "                d_acc_test=accuracy_score(label_test,np.where(tf.sigmoid(d_predict)>=0.5,1.0,0.0))\n",
    "                g_loss_test=generator_loss(testy,generator_predict)\n",
    "                g_pearson_test=generator_metrics(testy,generator_predict)\n",
    "                if ifmute=='no':\n",
    "                    print('第',i+1,'次训练','D loss_train:',d_loss_train[0],'D acc_train:',100*d_loss_train[1],'G loss_train:',g_loss_train[0],'G pearson_train:',g_loss_train[1])\n",
    "                    print('第',i+1,'次测试','D loss_test:',np.array(d_loss_test),'D acc_test:',100*d_acc_test,'G loss_test:',np.array(g_loss_test),'G pearson_test:',np.array(g_pearson_test))\n",
    "            predicty=np.array(generator.predict(testx)).reshape(testy.shape[0],testy.shape[1],testy.shape[2],testy.shape[3])\n",
    "            r=np.zeros((testy.shape[1],testy.shape[2],testy.shape[3]))\n",
    "            p=np.zeros((testy.shape[1],testy.shape[2],testy.shape[3]))\n",
    "            for i in range(testy.shape[1]):\n",
    "                for j in range(testy.shape[2]):\n",
    "                    for k in range(testy.shape[3]):\n",
    "                        r[i,j,k],p[i,j,k]=pearsonr(predicty[:,i,j,k],testy[:,i,j,k])\n",
    "            print('相关系数',np.nanmean(r,axis=(0,1)))\n",
    "            if ifsave=='yes':\n",
    "                generator.save(savepath+'_generator')\n",
    "                discriminator.save(savepath+'_discriminator')\n",
    "                Vgg_19.save(savepath+'_Vgg_19')\n",
    "    return generator,discriminator,Vgg_19,predicty,testy,r,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb399180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T06:06:08.788034Z",
     "start_time": "2024-04-12T06:06:08.663535Z"
    }
   },
   "outputs": [],
   "source": [
    "#打开nc文件\n",
    "def open_data_nc(ncmode,filename,v_name,iftime,timename,timestart,timeend,iflon,lonname,iflat,latname,latlow,lattop,lonleft,lonright,latresolution,lonresolution,ifexper,iflevel,levelname,level,changeresolution=1,timespace=1,ifchange_west_east='no',ifinterpolate='no'):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from netCDF4 import Dataset as net\n",
    "    import xarray as xr\n",
    "    from datetime import datetime,timedelta\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    import os\n",
    "    #from wrf import getvar,interplevel\n",
    "    \n",
    "    plt.rcParams['font.sans-serif']=['SimHei'] #正常显示中文\n",
    "    plt.rcParams['axes.unicode_minus']=False #正常显示正负号\n",
    "    if ncmode == 'one':\n",
    "        file = xr.open_dataset(filename)\n",
    "        if ifinterpolate == 'yes':\n",
    "            inter = str('file.interp('+latname+'=np.arange('+str(latlow)+','+str(lattop+latresolution)+','+str(latresolution)+'),'+lonname+'=np.arange('+str(lonleft)+','+str(lonright+lonresolution)+','+str(lonresolution)+'))')\n",
    "            files=eval(inter)\n",
    "            file = files\n",
    "        if iftime  == 'yes' or iftime == 'self':\n",
    "            times = np.array(file[timename])\n",
    "        if iflon == 'yes':\n",
    "            lon = np.array(file[lonname])\n",
    "        if iflat == 'yes':\n",
    "            lat = np.array(file[latname])\n",
    "        v = file[v_name]\n",
    "        if iflevel != 'no':\n",
    "            levels = np.array(file[levelname])\n",
    "    elif ncmode == 'more_time' or ncmode =='more_level':\n",
    "        direc = os.listdir(filename)\n",
    "        path = []\n",
    "        file = []\n",
    "        v = []\n",
    "        lat = []\n",
    "        lon = []\n",
    "        times = []\n",
    "        levels = []\n",
    "        for i in range(len(direc)):\n",
    "            if filename[-1] == '/':  \n",
    "                path.append(filename+str(direc[i]))\n",
    "            else:\n",
    "                path.append(filename+'/'+str(direc[i]))\n",
    "            file_xr = xr.open_dataset(path[i])\n",
    "            if ifinterpolate == 'yes':\n",
    "                inter = str('file_xr.interp('+latname+'=np.arange('+str(latlow)+','+str(lattop)+','+str(latresolution)+'),'+lonname+'=np.arange('+str(lonleft)+','+str(lonright)+','+str(lonresolution)+'))')\n",
    "                files=eval(inter)\n",
    "                file_xr = files\n",
    "            file.append(file_xr)\n",
    "            if ncmode == 'more_time':\n",
    "                vs=np.array(file[i][v_name])\n",
    "                if iftime =='yes':\n",
    "                    timelist=np.array(file[i][timename])\n",
    "                if i != 0:\n",
    "                    if iftime =='yes':\n",
    "                        v=np.concatenate((v,vs))\n",
    "                        times=np.concatenate((times,timelist))\n",
    "                    elif iftime =='create':\n",
    "                        if iflevel !='no':\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1],vs.shape[2]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                        else:\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1))\n",
    "                        v=np.concatenate((v,vs))\n",
    "                else:\n",
    "                    if iftime == 'create':\n",
    "                        if iflevel !='no':\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1],vs.shape[2]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                        else:\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1))\n",
    "                        v=vs\n",
    "                    elif iftime == 'yes':\n",
    "                        v = vs\n",
    "                        times=timelist\n",
    "            if ncmode == 'more_level':\n",
    "                if iflevel == 'create':\n",
    "                    vs=np.array(file[i][v_name])\n",
    "                    levels=level\n",
    "                elif iflevel == 'yes' or iflevel =='all' or iflevel =='self' or iflevel =='selfchose':\n",
    "                    if iftime !='no':\n",
    "                        if iflat !='no':\n",
    "                            if iflon !='no':\n",
    "                                vs=np.array(file[i][v_name]).transpose(1,0,2,3)\n",
    "                            else:\n",
    "                                vs=np.array(file[i][v_name]).transpose(1,0,2)\n",
    "                        else:\n",
    "                            if iflon !='no':\n",
    "                                vs=np.array(file[i][v_name]).transpose(1,0,2)\n",
    "                            else:\n",
    "                                vs=np.array(file[i][v_name]).transpose(1,0)\n",
    "                    levellist=np.array(file[i][levelname])      \n",
    "                if i != 0:\n",
    "                    if iflevel == 'yes' or iflevel =='all' or iflevel =='self' or iflevel =='selfchose':\n",
    "                        v=np.concatenate((v,vs))\n",
    "                        levels=np.concatenate((levels,levellist))\n",
    "                    elif iflevel =='create':\n",
    "                        if iftime !='no':\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1],vs.shape[2]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                        else:\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1))\n",
    "                        v=np.concatenate((v,vs))\n",
    "                else:\n",
    "                    if iflevel == 'create':\n",
    "                        if iftime !='no':\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1],vs.shape[2]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                        else:\n",
    "                            if iflat !='no':\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0],vs.shape[1]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                            else:\n",
    "                                if iflon !='no':\n",
    "                                    vs = vs.reshape((1,vs.shape[0]))\n",
    "                                else:\n",
    "                                    vs = vs.reshape((1))\n",
    "                        v=vs\n",
    "                    elif iflevel == 'yes' or iflevel =='all' or iflevel =='self' or iflevel =='selfchose':\n",
    "                        v=vs\n",
    "                        levels=levellist\n",
    "        if ncmode == 'more_time':\n",
    "            if iflon =='yes':\n",
    "                lon = file[0][lonname]\n",
    "            if iflat =='yes':\n",
    "                lat = file[0][latname]\n",
    "            if iflevel != 'no':\n",
    "                levels = np.array(file[0][levelname])\n",
    "        if ncmode == 'more_level':\n",
    "            if iflon =='yes':\n",
    "                lon = file[0][lonname]\n",
    "            if iflat =='yes':\n",
    "                lat = file[0][latname]\n",
    "            if iftime != 'no':\n",
    "                times = np.array(file[0][timename])\n",
    "            if iftime !='no':\n",
    "                if iflat !='no':\n",
    "                    if iflon !='no':\n",
    "                        v=v.transpose(1,0,2,3)\n",
    "                    else:\n",
    "                        v=v.transpose(1,0,2)\n",
    "                else:\n",
    "                    if iflon !='no':\n",
    "                        v=v.transpose(1,0,2)\n",
    "                    else:\n",
    "                        v=v.transpose(1,0)\n",
    "    elif ncmode == 'one_wrf':\n",
    "        file = xr.open_dataset(filename)\n",
    "        ncfile = net(filename)\n",
    "        times = np.array(file[timename])\n",
    "        lon = np.array(file[lonname][0,0,:])\n",
    "        lat = np.array(file[latname][0,:,0])\n",
    "        if iflevel == 'no':\n",
    "            v = np.zeros((times.shape[0],lat.shape[0],lon.shape[0]))\n",
    "            for i in range(times.shape[0]):\n",
    "                v[i,:,:] = np.array(getvar(ncfile,v_name,i))\n",
    "        elif iflevel == 'yes':\n",
    "            levels = np.array(file[levelname])[0,:]\n",
    "            p = np.zeros((times.shape[0],levels.shape[0],lat.shape[0],lon.shape[0]))\n",
    "            v = np.zeros((times.shape[0],levels.shape[0],lat.shape[0],lon.shape[0]))\n",
    "            for i in range(times.shape[0]):\n",
    "                if v_name == 'U':\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))[:,:,:-1]\n",
    "                elif v_name == 'V':\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))[:,:-1,:]\n",
    "                elif v_name == 'W' or v_name == 'PH' or v_name == 'PHB':\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))[:-1,:,:]\n",
    "                else:\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))\n",
    "                p[i,:,:,:] = np.array(getvar(ncfile,'pressure',i))\n",
    "            vs = np.zeros((times.shape[0],lat.shape[0],lon.shape[0]))\n",
    "            for i in range(times.shape[0]):\n",
    "                vs[i,:,:] = interplevel(v[i,:,:,:],p[i,:,:,:],level)\n",
    "        else:\n",
    "            levels = np.array(file[levelname])[0,:]\n",
    "            p = np.zeros((times.shape[0],levels.shape[0],lat.shape[0],lon.shape[0]))\n",
    "            v = np.zeros((times.shape[0],levels.shape[0],lat.shape[0],lon.shape[0]))\n",
    "            for i in range(times.shape[0]):\n",
    "                if v_name == 'U':\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))[:,:,:-1]\n",
    "                elif v_name == 'V':\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))[:,:-1,:]\n",
    "                elif v_name == 'W' or v_name == 'PH' or v_name == 'PHB':\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))[:-1,:,:]\n",
    "                else:\n",
    "                    v[i,:,:,:] = np.array(getvar(ncfile,v_name,i))\n",
    "                p[i,:,:,:] = np.array(getvar(ncfile,'pressure',i))\n",
    "            vs = np.zeros((times.shape[0],len(level),lat.shape[0],lon.shape[0]))\n",
    "            for i in range(times.shape[0]):\n",
    "                vs[i,:,:,:] = interplevel(v[i,:,:,:],p[i,:,:,:],level)\n",
    "        if iflevel !='no':\n",
    "            levels = level\n",
    "            v = vs\n",
    "    if iftime =='yes' or iftime == 'create':\n",
    "        if len(timestart) == 4 :\n",
    "            if iftime =='yes':\n",
    "                for i in range(len(times)):\n",
    "                    if timestart == pd.to_datetime(str(np.array(times[i]))).strftime('%Y'):\n",
    "                        startpoint = i\n",
    "                    if timeend == pd.to_datetime(str(np.array(times[i]))).strftime('%Y'):\n",
    "                        endpoint = i\n",
    "            if iftime =='create':\n",
    "                for i in range(v.shape[0]):\n",
    "                    times.append(datetime(int(pd.to_datetime(str(timestart)).strftime('%Y')),int(pd.to_datetime(str(timestart)).strftime('%m')),int(pd.to_datetime(str(timestart)).strftime('%d')))+ timespace*i * relativedelta(years=+1))\n",
    "                    times[i]=pd.to_datetime(str(times[i])).strftime('%Y-%m-%d')\n",
    "                times = np.array(times,dtype = np.datetime64)\n",
    "        elif len(timestart) == 7 :\n",
    "            if iftime =='yes':\n",
    "                for i in range(len(times)):\n",
    "                    if timestart == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m'):\n",
    "                        startpoint = i\n",
    "                    if timeend == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m'):\n",
    "                        endpoint = i\n",
    "            if iftime =='create':\n",
    "                for i in range(v.shape[0]):\n",
    "                    times.append(datetime(int(pd.to_datetime(str(timestart)).strftime('%Y')),int(pd.to_datetime(str(timestart)).strftime('%m')),int(pd.to_datetime(str(timestart)).strftime('%d')))+ timespace*i * relativedelta(months=+1))\n",
    "                    times[i]=pd.to_datetime(str(times[i])).strftime('%Y-%m-%d')\n",
    "                times = np.array(times,dtype = np.datetime64)\n",
    "        elif len(timestart) == 10 :\n",
    "            if iftime =='yes':\n",
    "                for i in range(len(times)):\n",
    "                    if timestart == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d'):\n",
    "                        startpoint = i\n",
    "                    if timeend == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d'):\n",
    "                        endpoint = i\n",
    "            if iftime =='create':\n",
    "                for i in range(v.shape[0]):\n",
    "                    times.append(datetime(int(pd.to_datetime(str(timestart)).strftime('%Y')),int(pd.to_datetime(str(timestart)).strftime('%m')),int(pd.to_datetime(str(timestart)).strftime('%d')))+ timespace*i * timedelta(days=1))\n",
    "                    times[i]=pd.to_datetime(str(times[i])).strftime('%Y-%m-%d')\n",
    "                times = np.array(times,dtype = np.datetime64)\n",
    "        elif len(timestart) == 13 :\n",
    "            if iftime =='yes':\n",
    "                for i in range(len(times)):\n",
    "                    if timestart == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d-%H'):\n",
    "                        startpoint = i\n",
    "                    if timeend == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d-%H'):\n",
    "                        endpoint = i\n",
    "            if iftime =='create':\n",
    "                for i in range(v.shape[0]):\n",
    "                    times.append(datetime(int(pd.to_datetime(str(timestart)).strftime('%Y')),int(pd.to_datetime(str(timestart)).strftime('%m')),int(pd.to_datetime(str(timestart)).strftime('%d')),int(pd.to_datetime(str(timestart)).strftime('%H')))+ timespace*i * timedelta(hours=1))\n",
    "                    times[i]=pd.to_datetime(str(times[i])).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                times = np.array(times,dtype = np.datetime64)\n",
    "        elif len(timestart) == 16 :\n",
    "            if iftime =='yes':\n",
    "                for i in range(len(times)):\n",
    "                    if timestart == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d-%H-%M'):\n",
    "                        startpoint = i\n",
    "                    if timeend == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d-%H-%M'):\n",
    "                        endpoint = i\n",
    "            if iftime =='create':\n",
    "                for i in range(v.shape[0]):\n",
    "                    times.append(datetime(int(pd.to_datetime(str(timestart)).strftime('%Y')),int(pd.to_datetime(str(timestart)).strftime('%m')),int(pd.to_datetime(str(timestart)).strftime('%d')),int(pd.to_datetime(str(timestart)).strftime('%H')),int(pd.to_datetime(str(timestart)).strftime('%M')))+ timespace*i * timedelta(minutes=1))\n",
    "                    times[i]=pd.to_datetime(str(times[i])).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                times = np.array(times,dtype = np.datetime64)\n",
    "        elif len(timestart) == 19 :\n",
    "            if iftime =='yes':\n",
    "                for i in range(len(times)):\n",
    "                    if timestart == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d-%H-%M-%S'):\n",
    "                        startpoint = i\n",
    "                    if timeend == pd.to_datetime(str(np.array(times[i]))).strftime('%Y-%m-%d-%H-%M-%S'):\n",
    "                        endpoint = i\n",
    "            if iftime =='create':\n",
    "                for i in range(v.shape[0]):\n",
    "                    times.append(datetime(int(pd.to_datetime(str(timestart)).strftime('%Y')),int(pd.to_datetime(str(timestart)).strftime('%m')),int(pd.to_datetime(str(timestart)).strftime('%d')),int(pd.to_datetime(str(timestart)).strftime('%H')),int(pd.to_datetime(str(timestart)).strftime('%M')),int(pd.to_datetime(str(timestart)).strftime('%S')))+ timespace*i * timedelta(seconds=1))\n",
    "                    times[i]=pd.to_datetime(str(times[i])).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                times = np.array(times,dtype = np.datetime64)\n",
    "    if iftime=='self':\n",
    "        for i in range(len(times)):\n",
    "            if timestart == times[i]:\n",
    "                startpoint = i\n",
    "            if timeend == times[i]:\n",
    "                endpoint = i\n",
    "    if iftime =='yes' or iftime=='self':\n",
    "        times = times[startpoint:endpoint+1]\n",
    "    elif iftime =='create':\n",
    "        startpoint = 0\n",
    "        endpoint = times.shape[0]\n",
    "    if iflat == 'yes':\n",
    "        if float(lat[0])>float(lat[1]):\n",
    "            lowpoint = int((np.nanmax(lat)-latlow)/latresolution)\n",
    "            toppoint = int((np.nanmax(lat)-lattop)/latresolution)\n",
    "        else:\n",
    "            lowpoint = int((-np.nanmin(lat)+latlow)/latresolution)\n",
    "            toppoint = int((-np.nanmin(lat)+lattop)/latresolution)\n",
    "    if iflon == 'yes':\n",
    "        leftpoint = int((-np.nanmin(lon)+lonleft)/lonresolution)\n",
    "        rightpoint = int((-np.nanmin(lon)+lonright)/lonresolution)\n",
    "    if ncmode != 'one_wrf':\n",
    "        if iflevel == 'yes':\n",
    "            for i in range(0,len(levels)):\n",
    "                if int(level) == int(levels[i]):\n",
    "                    levelpoint = i\n",
    "            if ifexper == 'yes':\n",
    "                if float(lat[0])>float(lat[1]):\n",
    "                    v = v[startpoint:endpoint+1,0,levelpoint,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                else:\n",
    "                    v = v[startpoint:endpoint+1,0,levelpoint,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "            elif ifexper ==  'no':\n",
    "                if iftime != 'no':\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,levelpoint,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,levelpoint,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,levelpoint,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,levelpoint,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,levelpoint,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,levelpoint]\n",
    "                            v = np.array(v)\n",
    "                else:\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[levelpoint,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[levelpoint,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[levelpoint,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[levelpoint,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[::changeresolution])\n",
    "                            else:\n",
    "                                v = v[levelpoint,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[::changeresolution])\n",
    "                        else:\n",
    "                            v = v[levelpoint]\n",
    "                            v = np.array(v)\n",
    "        elif iflevel == 'no':\n",
    "            if ifexper == 'yes':\n",
    "                if float(lat[0])>float(lat[1]):\n",
    "                    v = v[startpoint:endpoint+1,0,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                else:\n",
    "                    v = v[startpoint:endpoint+1,0,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "            elif ifexper ==  'no':\n",
    "                if iftime != 'no':\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1]\n",
    "                            v = np.array(v)\n",
    "                else:\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[toppoint:lowpoint+1]\n",
    "                                v = np.array(v[::changeresolution])\n",
    "                            else:\n",
    "                                v = v[lowpoint:toppoint+1]\n",
    "                                v = np.array(v[::changeresolution])\n",
    "                        else:\n",
    "                            v = None\n",
    "        elif iflevel == 'all' or iflevel =='create':\n",
    "            if ifexper == 'yes':\n",
    "                if float(lat[0])>float(lat[1]):\n",
    "                    v = v[startpoint:endpoint+1,0,:,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                else:\n",
    "                    v = v[startpoint:endpoint+1,0,:,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "            elif ifexper == 'no':\n",
    "                if iftime != 'no':\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,:,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,:,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,:,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,:,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,:,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,:]\n",
    "                            v = np.array(v)\n",
    "                else:\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[:,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[:,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[:,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[:,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[:,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[:]\n",
    "                            v = np.array(v)\n",
    "        elif iflevel == 'self':\n",
    "            levelstart = 0\n",
    "            levelend = 0\n",
    "            for i in range(len(levels)):\n",
    "                if int(levels[i]) == level[0]:\n",
    "                    levelstart = i\n",
    "                if int(levels[i]) == level[1]:\n",
    "                    levelend = i\n",
    "            if ifexper == 'yes':\n",
    "                if float(lat[0])>float(lat[1]):\n",
    "                    v = v[startpoint:endpoint+1,0,levelstart:levelend+1,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                else:\n",
    "                    v = v[startpoint:endpoint+1,0,levelstart:levelend+1,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "            elif ifexper == 'no':\n",
    "                if iftime != 'no':\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,levelstart:levelend+1,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,levelstart:levelend+1,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,levelstart:levelend+1,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,levelstart:levelend+1,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,levelstart:levelend+1,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,levelstart:levelend+1]\n",
    "                            v = np.array(v)\n",
    "                else:\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[levelstart:levelend+1,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[levelstart:levelend+1,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[levelstart:levelend+1,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[levelstart:levelend+1,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[levelstart:levelend+1,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[levelstart:levelend+1]\n",
    "                            v = np.array(v)\n",
    "            levels = levels[levelstart:levelend+1]\n",
    "        elif iflevel == 'selfchose':\n",
    "            selflevel = []\n",
    "            j=0\n",
    "            for i in range(len(levels)):\n",
    "                if j>= len(level):\n",
    "                    break\n",
    "                if int(levels[i]) == level[j]:\n",
    "                    selflevel.append(i)\n",
    "                    j=j+1\n",
    "            if ifexper == 'yes':\n",
    "                if float(lat[0])>float(lat[1]):\n",
    "                    v = v[startpoint:endpoint+1,0,selflevel,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                else:\n",
    "                    v = v[startpoint:endpoint+1,0,selflevel,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                    v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "            elif ifexper == 'no':\n",
    "                if iftime != 'no':\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,selflevel,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,selflevel,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,selflevel,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[startpoint:endpoint+1,selflevel,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[startpoint:endpoint+1,selflevel,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[startpoint:endpoint+1,selflevel]\n",
    "                            v = np.array(v)\n",
    "                else:\n",
    "                    if iflon != 'no':\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[selflevel,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[selflevel,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[selflevel,leftpoint:rightpoint+1]\n",
    "                            v = np.array(v[:,::changeresolution])\n",
    "                    else:\n",
    "                        if iflat != 'no':\n",
    "                            if float(lat[0])>float(lat[1]):\n",
    "                                v = v[selflevel,toppoint:lowpoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                            else:\n",
    "                                v = v[selflevel,lowpoint:toppoint+1]\n",
    "                                v = np.array(v[:,::changeresolution])\n",
    "                        else:\n",
    "                            v = v[selflevel]\n",
    "                            v = np.array(v)\n",
    "            levels = levels[selflevel]\n",
    "    else:\n",
    "        if iflevel == 'yes' or iflevel == 'no':\n",
    "            if float(lat[0])>float(lat[1]):\n",
    "                v = v[startpoint:endpoint+1,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "            else:\n",
    "                v = v[startpoint:endpoint+1,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                v = np.array(v[:,::changeresolution,::changeresolution])\n",
    "        else:\n",
    "            if float(lat[0])>float(lat[1]):\n",
    "                v = v[startpoint:endpoint+1,:,toppoint:lowpoint+1,leftpoint:rightpoint+1]\n",
    "                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "            else:\n",
    "                v = v[startpoint:endpoint+1,:,lowpoint:toppoint+1,leftpoint:rightpoint+1]\n",
    "                v = np.array(v[:,:,::changeresolution,::changeresolution])\n",
    "    if iflon !='no':\n",
    "        lon = lon[leftpoint:rightpoint+1:changeresolution]\n",
    "    if iflat !='no':\n",
    "        if float(lat[0])>float(lat[1]):\n",
    "            lat = lat[toppoint:lowpoint+1:changeresolution]\n",
    "        else:\n",
    "            lat = lat[lowpoint:toppoint+1:changeresolution]\n",
    "    if ifchange_west_east =='yes':\n",
    "        if np.nanmin(lon)<0:\n",
    "            right = 360.0 - changeresolution*lonresolution\n",
    "            if iflevel == 'all' or iflevel == 'self' or iflevel == 'selfchose' or iflevel == 'create':\n",
    "                if iftime !='no':\n",
    "                    mid = int(v.shape[3]/2)\n",
    "                    lon = np.linspace(0.0,right,v.shape[3])\n",
    "                    vwest = v[:,:,:,0:mid]\n",
    "                    veast = v[:,:,:,mid:]\n",
    "                    v = np.concatenate((veast,vwest),axis=3)\n",
    "                    lonleft = 0.0\n",
    "                    lonright = right\n",
    "                else:\n",
    "                    mid = int(v.shape[2]/2)\n",
    "                    lon = np.linspace(0.0,right,v.shape[2])\n",
    "                    vwest = v[:,:,0:mid]\n",
    "                    veast = v[:,:,mid:]\n",
    "                    v = np.concatenate((veast,vwest),axis=2)\n",
    "                    lonleft = 0.0\n",
    "                    lonright = right\n",
    "            else:\n",
    "                if iftime !='no':\n",
    "                    mid = int(v.shape[2]/2)\n",
    "                    lon = np.linspace(0.0,right,v.shape[2])\n",
    "                    vwest = v[:,:,0:mid]\n",
    "                    veast = v[:,:,mid:]\n",
    "                    v = np.concatenate((veast,vwest),axis=2)\n",
    "                    lonleft = 0.0\n",
    "                    lonright = right\n",
    "                else:\n",
    "                    mid = int(v.shape[1]/2)\n",
    "                    lon = np.linspace(0.0,right,v.shape[1])\n",
    "                    vwest = v[:,0:mid]\n",
    "                    veast = v[:,mid:]\n",
    "                    v = np.concatenate((veast,vwest),axis=1)\n",
    "                    lonleft = 0.0\n",
    "                    lonright = right\n",
    "        else:\n",
    "            right = 180.0 - changeresolution*lonresolution\n",
    "            if iflevel == 'all' or iflevel == 'self' or iflevel == 'selfchose' or iflevel =='create':\n",
    "                if iftime !='no':\n",
    "                    mid = int(v.shape[3]/2)\n",
    "                    lon = np.linspace(-180.0,right,v.shape[3])\n",
    "                    veast = v[:,:,:,0:mid]\n",
    "                    vwest = v[:,:,:,mid:]\n",
    "                    v = np.concatenate((vwest,veast),axis=3)\n",
    "                    lonleft = -180.0\n",
    "                    lonright = right\n",
    "                else:\n",
    "                    mid = int(v.shape[2]/2)\n",
    "                    lon = np.linspace(-180.0,right,v.shape[2])\n",
    "                    veast = v[:,:,0:mid]\n",
    "                    vwest = v[:,:,mid:]\n",
    "                    v = np.concatenate((vwest,veast),axis=2)\n",
    "                    lonleft = -180.0\n",
    "                    lonright = right\n",
    "            else:\n",
    "                if iftime !='no':\n",
    "                    mid = int(v.shape[2]/2)\n",
    "                    lon = np.linspace(-180.0,right,v.shape[2])\n",
    "                    veast = v[:,:,0:mid]\n",
    "                    vwest = v[:,:,mid:]\n",
    "                    v = np.concatenate((vwest,veast),axis=2)\n",
    "                    lonleft = -180.0\n",
    "                    lonright = right\n",
    "                else:\n",
    "                    mid = int(v.shape[1]/2)\n",
    "                    lon = np.linspace(-180.0,right,v.shape[1])\n",
    "                    veast = v[:,0:mid]\n",
    "                    vwest = v[:,mid:]\n",
    "                    v = np.concatenate((vwest,veast),axis=1)\n",
    "                    lonleft = -180.0\n",
    "                    lonright = right\n",
    "    if iflevel == 'all' or iflevel == 'self' or iflevel == 'selfchose' or iflevel =='create':\n",
    "        if iftime !='no':\n",
    "            if iflat !='no':\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(timename,times),(levelname,levels),(latname,lat),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(timename,times),(levelname,levels),(latname,lat)])\n",
    "            else:\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(timename,times),(levelname,levels),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(timename,times),(levelname,levels)])\n",
    "        else:\n",
    "            if iflat !='no':\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(levelname,levels),(latname,lat),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(levelname,levels),(latname,lat)])\n",
    "            else:\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(levelname,levels),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(levelname,levels)])\n",
    "        levels = v[levelname]\n",
    "    else:\n",
    "        if iftime !='no':\n",
    "            if iflat !='no':\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(timename,times),(latname,lat),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(timename,times),(latname,lat)])\n",
    "            else:\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(timename,times),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(timename,times)])\n",
    "        else:\n",
    "            if iflat !='no':\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(latname,lat),(lonname,lon)])\n",
    "                else:\n",
    "                    v = xr.DataArray(v, [(latname,lat)])\n",
    "            else:\n",
    "                if iflon !='no':\n",
    "                    v = xr.DataArray(v, [(lonname,lon)])\n",
    "                else:\n",
    "                    v = None\n",
    "        levels = None\n",
    "    if iftime !='no':\n",
    "        times = v[timename]\n",
    "    else:\n",
    "        times = None\n",
    "    if iflon !='no':\n",
    "        lon = v[lonname]\n",
    "    else:\n",
    "        lon = None\n",
    "    if iflat !='no':\n",
    "        lat = v[latname]\n",
    "    else:\n",
    "        lat = None\n",
    "    return v,lon,lat,levels,latlow,lattop,lonleft,lonright,times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd7c258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T06:06:20.631640Z",
     "start_time": "2024-04-12T06:06:08.789546Z"
    }
   },
   "outputs": [],
   "source": [
    "sm,lon,lat,levels,latlow,lattop,lonleft,lonright,times=open_data_nc('one',r'D:\\ERA5 dayly data on pressure levels from 1979 to present\\single 0.25\\Volumetric soil water layer 1-day.nc','swvl1','yes','time','2019-01-01','2019-12-31','yes','longitude','yes','latitude',35.0,45.0,110.0,120.0,0.25,0.25,'no','no',None,None,changeresolution=1,timespace=1,ifchange_west_east='no',ifinterpolate='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65131c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T06:06:20.647640Z",
     "start_time": "2024-04-12T06:06:20.634639Z"
    }
   },
   "outputs": [],
   "source": [
    "sm_HR=sm[:,:-1,:-1]\n",
    "sm_LR=sm[:,:-1:2,:-1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96073d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T06:06:20.678640Z",
     "start_time": "2024-04-12T06:06:20.648639Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sm_HR=(sm_HR-np.nanmean(sm_HR,axis=0))/np.nanstd(sm_HR,axis=0)\n",
    "sm_LR=(sm_LR-np.nanmean(sm_LR,axis=0))/np.nanstd(sm_LR,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4ad15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T06:06:20.695156Z",
     "start_time": "2024-04-12T06:06:20.681646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "print(sm_HR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527e49f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T07:36:11.628502Z",
     "start_time": "2024-04-12T06:06:20.697165Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 40, 40, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 40, 40, 256)  2560        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 40, 40, 256)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 40, 40, 256)  590080      ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 40, 40, 256)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " group_normalization (GroupNorm  (None, 40, 40, 256)  512        ['activation_7[0][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 40, 40, 512)  1180160     ['group_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 40, 40, 512)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 20, 20, 512)  0          ['activation_8[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 20, 20, 512)  0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 20, 20, 512)  2048       ['activation_9[0][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 20, 20, 512)  2359808     ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 20, 20, 512)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " group_normalization_1 (GroupNo  (None, 20, 20, 512)  1024       ['activation_10[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 20, 20, 512)  2359808     ['group_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 20, 20, 512)  0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 204800)       0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 204800)       0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 409600)       0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          209715712   ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            513         ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 216,212,225\n",
      "Trainable params: 216,211,201\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 20, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 20, 20, 512)  5120        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 20, 20, 512)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 20, 20, 512)  2359808     ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 20, 20, 512)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 512)         0           ['activation_1[0][0]']           \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          131328      ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          131584      ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 20, 20, 512)  0           ['activation_1[0][0]',           \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " add (Add)                      (None, 20, 20, 512)  0           ['multiply[0][0]',               \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 40, 40, 512)  0          ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 40, 40, 256)  1179904     ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 40, 40, 256)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 40, 40, 256)  590080      ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 40, 40, 256)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 40, 40, 1)    257         ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,398,081\n",
      "Trainable params: 4,398,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 40, 40, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 40, 40, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 40, 40, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 40, 40, 64)        0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 40, 40, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 40, 40, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 40, 40, 64)        0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 40, 40, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 40, 40, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 40, 40, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 40, 40, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 40, 40, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 40, 40, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 20, 20, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 20, 20, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 20, 20, 128)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 20, 20, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 20, 20, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 20, 20, 128)       0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 20, 20, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 20, 20, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 20, 20, 128)       0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 20, 20, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 20, 20, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 20, 20, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 631,104\n",
      "Trainable params: 629,568\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "None\n",
      "9/9 [==============================] - 1s 45ms/step\n",
      "3/3 [==============================] - 0s 197ms/step\n",
      "6/6 [==============================] - 1s 80ms/step\n",
      "第 1 次训练 D loss_train: 1.1632431745529175 D acc_train: 50.36630034446716 G loss_train: 0.5732321739196777 G pearson_train: 0.945688009262085\n",
      "第 1 次测试 D loss_test: 0.00076465245 D acc_test: 50.0 G loss_test: 1.001198 G pearson_test: -0.75037473\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 2 次训练 D loss_train: 0.1449037343263626 D acc_train: 78.57142686843872 G loss_train: 0.3105255961418152 G pearson_train: 0.9500643014907837\n",
      "第 2 次测试 D loss_test: 0.02402143 D acc_test: 50.0 G loss_test: 0.5760614 G pearson_test: 0.9405859\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 3 次训练 D loss_train: 0.7583994269371033 D acc_train: 70.69597244262695 G loss_train: 0.27610498666763306 G pearson_train: 0.9548080563545227\n",
      "第 3 次测试 D loss_test: 3.9733443 D acc_test: 50.0 G loss_test: 0.3648531 G pearson_test: 0.9452698\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 4 次训练 D loss_train: 22.650102615356445 D acc_train: 49.450549483299255 G loss_train: 0.26984962821006775 G pearson_train: 0.9585222005844116\n",
      "第 4 次测试 D loss_test: 4.291511 D acc_test: 50.0 G loss_test: 0.3533548 G pearson_test: 0.9504097\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 5 次训练 D loss_train: 17.14834976196289 D acc_train: 50.732600688934326 G loss_train: 0.2840960621833801 G pearson_train: 0.9631906747817993\n",
      "第 5 次测试 D loss_test: 6.412999 D acc_test: 50.0 G loss_test: 0.309626 G pearson_test: 0.95433867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 6 次训练 D loss_train: 34.082061767578125 D acc_train: 47.069597244262695 G loss_train: 0.23032255470752716 G pearson_train: 0.9657071828842163\n",
      "第 6 次测试 D loss_test: 5.137532 D acc_test: 50.0 G loss_test: 0.2983744 G pearson_test: 0.9594227\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 7 次训练 D loss_train: 9.409281730651855 D acc_train: 49.81684982776642 G loss_train: 0.19897985458374023 G pearson_train: 0.9683102369308472\n",
      "第 7 次测试 D loss_test: 4.2076073 D acc_test: 50.54347826086957 G loss_test: 0.2565452 G pearson_test: 0.9621157\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 8 次训练 D loss_train: 4.5667619705200195 D acc_train: 49.81684982776642 G loss_train: 0.1679457426071167 G pearson_train: 0.9701232314109802\n",
      "第 8 次测试 D loss_test: 1.1592402 D acc_test: 52.71739130434783 G loss_test: 0.21938683 G pearson_test: 0.96491\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 9 次训练 D loss_train: 0.9805628657341003 D acc_train: 53.66300344467163 G loss_train: 0.15533320605754852 G pearson_train: 0.9715496301651001\n",
      "第 9 次测试 D loss_test: 0.6717917 D acc_test: 50.0 G loss_test: 0.20130578 G pearson_test: 0.9668429\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 10 次训练 D loss_train: 0.9263092279434204 D acc_train: 51.46520137786865 G loss_train: 0.14841419458389282 G pearson_train: 0.9727151393890381\n",
      "第 10 次测试 D loss_test: 0.96452534 D acc_test: 51.63043478260869 G loss_test: 0.19010071 G pearson_test: 0.968362\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 11 次训练 D loss_train: 0.7769243717193604 D acc_train: 53.29670310020447 G loss_train: 0.13959191739559174 G pearson_train: 0.9736602902412415\n",
      "第 11 次测试 D loss_test: 0.61212844 D acc_test: 50.0 G loss_test: 0.17815636 G pearson_test: 0.9696345\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 12 次训练 D loss_train: 0.7271316051483154 D acc_train: 54.029303789138794 G loss_train: 0.18119925260543823 G pearson_train: 0.973328709602356\n",
      "第 12 次测试 D loss_test: 0.83719385 D acc_test: 51.08695652173913 G loss_test: 0.17084809 G pearson_test: 0.9706843\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 13 次训练 D loss_train: 0.5956555604934692 D acc_train: 57.875460386276245 G loss_train: 0.16621293127536774 G pearson_train: 0.9739902019500732\n",
      "第 13 次测试 D loss_test: 0.5665908 D acc_test: 50.0 G loss_test: 0.21133703 G pearson_test: 0.97083014\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 14 次训练 D loss_train: 0.5427041053771973 D acc_train: 59.15750861167908 G loss_train: 0.1600034236907959 G pearson_train: 0.9745601415634155\n",
      "第 14 次测试 D loss_test: 0.6967172 D acc_test: 52.17391304347826 G loss_test: 0.19757871 G pearson_test: 0.97138625\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 15 次训练 D loss_train: 0.5414249897003174 D acc_train: 62.08791136741638 G loss_train: 0.1525723785161972 G pearson_train: 0.9750528931617737\n",
      "第 15 次测试 D loss_test: 0.5546759 D acc_test: 50.0 G loss_test: 0.18781984 G pearson_test: 0.971963\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 16 次训练 D loss_train: 0.4992365539073944 D acc_train: 61.17216348648071 G loss_train: 0.14891394972801208 G pearson_train: 0.9754821062088013\n",
      "第 16 次测试 D loss_test: 0.65319324 D acc_test: 54.891304347826086 G loss_test: 0.18292256 G pearson_test: 0.97247225\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 17 次训练 D loss_train: 0.4892493188381195 D acc_train: 63.553112745285034 G loss_train: 0.14355073869228363 G pearson_train: 0.9758723378181458\n",
      "第 17 次测试 D loss_test: 0.5414202 D acc_test: 50.0 G loss_test: 0.17620152 G pearson_test: 0.97293854\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 18 次训练 D loss_train: 0.4581090211868286 D acc_train: 61.17216348648071 G loss_train: 0.14072220027446747 G pearson_train: 0.9762178659439087\n",
      "第 18 次测试 D loss_test: 0.6143391 D acc_test: 57.608695652173914 G loss_test: 0.17253436 G pearson_test: 0.97334945\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 19 次训练 D loss_train: 0.4452369809150696 D acc_train: 63.9194130897522 G loss_train: 0.13647109270095825 G pearson_train: 0.9765360951423645\n",
      "第 19 次测试 D loss_test: 0.530262 D acc_test: 50.0 G loss_test: 0.16727215 G pearson_test: 0.973728\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 20 次训练 D loss_train: 0.4236864447593689 D acc_train: 63.736265897750854 G loss_train: 0.13409046828746796 G pearson_train: 0.9768189191818237\n",
      "第 20 次测试 D loss_test: 0.58393323 D acc_test: 55.434782608695656 G loss_test: 0.16415654 G pearson_test: 0.97406495\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 21 次训练 D loss_train: 0.411291241645813 D acc_train: 64.46886658668518 G loss_train: 0.13059258460998535 G pearson_train: 0.9770817756652832\n",
      "第 21 次测试 D loss_test: 0.5208542 D acc_test: 50.0 G loss_test: 0.15988065 G pearson_test: 0.97437644\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 22 次训练 D loss_train: 0.39586159586906433 D acc_train: 65.56776762008667 G loss_train: 0.12849943339824677 G pearson_train: 0.9773167371749878\n",
      "第 22 次测试 D loss_test: 0.56031 D acc_test: 55.434782608695656 G loss_test: 0.1571282 G pearson_test: 0.9746563\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 23 次训练 D loss_train: 0.38491201400756836 D acc_train: 65.75091481208801 G loss_train: 0.1255781650543213 G pearson_train: 0.977536141872406\n",
      "第 23 次测试 D loss_test: 0.5126979 D acc_test: 50.0 G loss_test: 0.1535699 G pearson_test: 0.97491527\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 24 次训练 D loss_train: 0.3731388747692108 D acc_train: 64.46886658668518 G loss_train: 0.12374935299158096 G pearson_train: 0.9777327179908752\n",
      "第 24 次测试 D loss_test: 0.5414698 D acc_test: 53.2608695652174 G loss_test: 0.15111762 G pearson_test: 0.97514963\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 25 次训练 D loss_train: 0.36360713839530945 D acc_train: 64.46886658668518 G loss_train: 0.12131955474615097 G pearson_train: 0.9779166579246521\n",
      "第 25 次测试 D loss_test: 0.5052124 D acc_test: 50.0 G loss_test: 0.14815827 G pearson_test: 0.97536606\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 26 次训练 D loss_train: 0.3540392518043518 D acc_train: 63.18681240081787 G loss_train: 0.1197180449962616 G pearson_train: 0.9780819416046143\n",
      "第 26 次测试 D loss_test: 0.5258788 D acc_test: 52.71739130434783 G loss_test: 0.14600877 G pearson_test: 0.9755632\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 27 次训练 D loss_train: 0.345611572265625 D acc_train: 63.18681240081787 G loss_train: 0.1176748275756836 G pearson_train: 0.9782372117042542\n",
      "第 27 次测试 D loss_test: 0.4978288 D acc_test: 50.0 G loss_test: 0.14352715 G pearson_test: 0.9757453\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 28 次训练 D loss_train: 0.3373534679412842 D acc_train: 59.8901093006134 G loss_train: 0.11626690626144409 G pearson_train: 0.9783768653869629\n",
      "第 28 次测试 D loss_test: 0.512535 D acc_test: 52.71739130434783 G loss_test: 0.14163375 G pearson_test: 0.97591215\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 29 次训练 D loss_train: 0.3297082781791687 D acc_train: 61.72161102294922 G loss_train: 0.1145501509308815 G pearson_train: 0.9785077571868896\n",
      "第 29 次测试 D loss_test: 0.4904623 D acc_test: 50.0 G loss_test: 0.1395322 G pearson_test: 0.9760665\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 30 次训练 D loss_train: 0.32229089736938477 D acc_train: 58.42490792274475 G loss_train: 0.11330927908420563 G pearson_train: 0.9786263108253479\n",
      "第 30 次测试 D loss_test: 0.5007231 D acc_test: 52.71739130434783 G loss_test: 0.13788441 G pearson_test: 0.9762083\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 31 次训练 D loss_train: 0.315265417098999 D acc_train: 59.8901093006134 G loss_train: 0.11183005571365356 G pearson_train: 0.9787379503250122\n",
      "第 31 次测试 D loss_test: 0.4828242 D acc_test: 50.0 G loss_test: 0.13608709 G pearson_test: 0.97633994\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 32 次训练 D loss_train: 0.3083444833755493 D acc_train: 56.04395866394043 G loss_train: 0.11072912067174911 G pearson_train: 0.9788393974304199\n",
      "第 32 次测试 D loss_test: 0.48994932 D acc_test: 52.71739130434783 G loss_test: 0.1346207 G pearson_test: 0.97646135\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 33 次训练 D loss_train: 0.30177760124206543 D acc_train: 57.875460386276245 G loss_train: 0.10945142060518265 G pearson_train: 0.9789349436759949\n",
      "第 33 次测试 D loss_test: 0.47513354 D acc_test: 50.54347826086957 G loss_test: 0.13306564 G pearson_test: 0.9765741\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 34 次训练 D loss_train: 0.29527103900909424 D acc_train: 55.3113579750061 G loss_train: 0.10848022997379303 G pearson_train: 0.9790219068527222\n",
      "第 34 次测试 D loss_test: 0.47987473 D acc_test: 53.80434782608695 G loss_test: 0.13176884 G pearson_test: 0.9766786\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 35 次训练 D loss_train: 0.2890170216560364 D acc_train: 55.494505167007446 G loss_train: 0.10737939924001694 G pearson_train: 0.979104220867157\n",
      "第 35 次测试 D loss_test: 0.4672711 D acc_test: 51.63043478260869 G loss_test: 0.13042293 G pearson_test: 0.9767758\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 36 次训练 D loss_train: 0.2828555107116699 D acc_train: 54.76190447807312 G loss_train: 0.10651865601539612 G pearson_train: 0.979179859161377\n",
      "第 36 次测试 D loss_test: 0.47028387 D acc_test: 53.80434782608695 G loss_test: 0.12928468 G pearson_test: 0.97686625\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 37 次训练 D loss_train: 0.2768627405166626 D acc_train: 54.94505763053894 G loss_train: 0.10556758940219879 G pearson_train: 0.9792513251304626\n",
      "第 37 次测试 D loss_test: 0.45938274 D acc_test: 52.71739130434783 G loss_test: 0.12810981 G pearson_test: 0.9769507\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 38 次训练 D loss_train: 0.27101147174835205 D acc_train: 53.47985625267029 G loss_train: 0.1047976016998291 G pearson_train: 0.9793174862861633\n",
      "第 38 次测试 D loss_test: 0.46094722 D acc_test: 53.80434782608695 G loss_test: 0.12710862 G pearson_test: 0.97702956\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 39 次训练 D loss_train: 0.26531559228897095 D acc_train: 53.66300344467163 G loss_train: 0.10396847873926163 G pearson_train: 0.979380190372467\n",
      "第 39 次测试 D loss_test: 0.45139268 D acc_test: 53.2608695652174 G loss_test: 0.12607259 G pearson_test: 0.9771034\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 40 次训练 D loss_train: 0.259732723236084 D acc_train: 52.3809552192688 G loss_train: 0.1032939925789833 G pearson_train: 0.9794380068778992\n",
      "第 40 次测试 D loss_test: 0.45187598 D acc_test: 54.347826086956516 G loss_test: 0.1251858 G pearson_test: 0.9771727\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 41 次训练 D loss_train: 0.25424858927726746 D acc_train: 52.74725556373596 G loss_train: 0.1025664359331131 G pearson_train: 0.9794930815696716\n",
      "第 41 次测试 D loss_test: 0.44342804 D acc_test: 53.80434782608695 G loss_test: 0.12428598 G pearson_test: 0.9772375\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 42 次训练 D loss_train: 0.24889051914215088 D acc_train: 52.014654874801636 G loss_train: 0.10196621716022491 G pearson_train: 0.9795439839363098\n",
      "第 42 次测试 D loss_test: 0.44300398 D acc_test: 54.347826086956516 G loss_test: 0.123495415 G pearson_test: 0.9772987\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 43 次训练 D loss_train: 0.24361123144626617 D acc_train: 52.19780206680298 G loss_train: 0.10132890939712524 G pearson_train: 0.9795924425125122\n",
      "第 43 次测试 D loss_test: 0.435365 D acc_test: 54.347826086956516 G loss_test: 0.12270355 G pearson_test: 0.977356\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 44 次训练 D loss_train: 0.23846742510795593 D acc_train: 52.014654874801636 G loss_train: 0.10079096257686615 G pearson_train: 0.979637622833252\n",
      "第 44 次测试 D loss_test: 0.43424734 D acc_test: 54.347826086956516 G loss_test: 0.12200093 G pearson_test: 0.97740996\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 45 次训练 D loss_train: 0.23340019583702087 D acc_train: 51.831501722335815 G loss_train: 0.10022380948066711 G pearson_train: 0.9796804785728455\n",
      "第 45 次测试 D loss_test: 0.42725164 D acc_test: 54.347826086956516 G loss_test: 0.12129133 G pearson_test: 0.97746086\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 46 次训练 D loss_train: 0.22845226526260376 D acc_train: 51.28205418586731 G loss_train: 0.09974215179681778 G pearson_train: 0.9797205924987793\n",
      "第 46 次测试 D loss_test: 0.42563018 D acc_test: 55.97826086956522 G loss_test: 0.12065975 G pearson_test: 0.97750884\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 47 次训练 D loss_train: 0.2235766351222992 D acc_train: 51.28205418586731 G loss_train: 0.0992451012134552 G pearson_train: 0.9797588586807251\n",
      "第 47 次测试 D loss_test: 0.41906142 D acc_test: 54.891304347826086 G loss_test: 0.12002905 G pearson_test: 0.9775541\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 48 次训练 D loss_train: 0.21883365511894226 D acc_train: 51.09890103340149 G loss_train: 0.09882232546806335 G pearson_train: 0.979794442653656\n",
      "第 48 次测试 D loss_test: 0.41696525 D acc_test: 58.152173913043484 G loss_test: 0.1194685 G pearson_test: 0.9775968\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 49 次训练 D loss_train: 0.21412882208824158 D acc_train: 51.09890103340149 G loss_train: 0.09838049113750458 G pearson_train: 0.9798286557197571\n",
      "第 49 次测试 D loss_test: 0.41079193 D acc_test: 58.152173913043484 G loss_test: 0.11891572 G pearson_test: 0.9776372\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 50 次训练 D loss_train: 0.2095329761505127 D acc_train: 50.91575384140015 G loss_train: 0.09799934923648834 G pearson_train: 0.9798610210418701\n",
      "第 50 次测试 D loss_test: 0.40846965 D acc_test: 57.608695652173914 G loss_test: 0.118412614 G pearson_test: 0.97767556\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 51 次训练 D loss_train: 0.20502780377864838 D acc_train: 51.09890103340149 G loss_train: 0.09761171042919159 G pearson_train: 0.9798915386199951\n",
      "第 51 次测试 D loss_test: 0.4026114 D acc_test: 57.608695652173914 G loss_test: 0.11791288 G pearson_test: 0.9777118\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 52 次训练 D loss_train: 0.20059122145175934 D acc_train: 50.732600688934326 G loss_train: 0.09727732092142105 G pearson_train: 0.9799203276634216\n",
      "第 52 次测试 D loss_test: 0.39995968 D acc_test: 57.608695652173914 G loss_test: 0.117468715 G pearson_test: 0.97774625\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 53 次训练 D loss_train: 0.19621294736862183 D acc_train: 50.54945349693298 G loss_train: 0.09693285077810287 G pearson_train: 0.979948103427887\n",
      "第 53 次测试 D loss_test: 0.3945011 D acc_test: 58.69565217391305 G loss_test: 0.117030844 G pearson_test: 0.9777789\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 54 次训练 D loss_train: 0.1919647753238678 D acc_train: 50.54945349693298 G loss_train: 0.09663020819425583 G pearson_train: 0.9799744486808777\n",
      "第 54 次测试 D loss_test: 0.39153495 D acc_test: 59.78260869565217 G loss_test: 0.11662839 G pearson_test: 0.9778102\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 55 次训练 D loss_train: 0.18778398633003235 D acc_train: 50.54945349693298 G loss_train: 0.09632791578769684 G pearson_train: 0.9799995422363281\n",
      "第 55 次测试 D loss_test: 0.38635057 D acc_test: 60.86956521739131 G loss_test: 0.116232485 G pearson_test: 0.9778398\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 56 次训练 D loss_train: 0.18369238078594208 D acc_train: 50.54945349693298 G loss_train: 0.09605998545885086 G pearson_train: 0.9800235629081726\n",
      "第 56 次测试 D loss_test: 0.38314006 D acc_test: 61.41304347826087 G loss_test: 0.115877785 G pearson_test: 0.977868\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 57 次训练 D loss_train: 0.17966020107269287 D acc_train: 50.54945349693298 G loss_train: 0.09578688442707062 G pearson_train: 0.9800466299057007\n",
      "第 57 次测试 D loss_test: 0.37820137 D acc_test: 61.95652173913043 G loss_test: 0.115528315 G pearson_test: 0.977895\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 20ms/step\n",
      "第 58 次训练 D loss_train: 0.17570342123508453 D acc_train: 50.36630034446716 G loss_train: 0.09554329514503479 G pearson_train: 0.980068564414978\n",
      "第 58 次测试 D loss_test: 0.3748327 D acc_test: 64.13043478260869 G loss_test: 0.11520164 G pearson_test: 0.97792083\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 59 次训练 D loss_train: 0.17178639769554138 D acc_train: 50.36630034446716 G loss_train: 0.09529798477888107 G pearson_train: 0.9800899028778076\n",
      "第 59 次测试 D loss_test: 0.37011552 D acc_test: 65.76086956521739 G loss_test: 0.11488428 G pearson_test: 0.9779455\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 60 次训练 D loss_train: 0.16792812943458557 D acc_train: 50.18315315246582 G loss_train: 0.09507708251476288 G pearson_train: 0.9801099896430969\n",
      "第 60 次测试 D loss_test: 0.36659825 D acc_test: 66.84782608695652 G loss_test: 0.114587195 G pearson_test: 0.9779691\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 61 次训练 D loss_train: 0.16414275765419006 D acc_train: 50.18315315246582 G loss_train: 0.09485866874456406 G pearson_train: 0.9801294803619385\n",
      "第 61 次测试 D loss_test: 0.36204353 D acc_test: 67.3913043478261 G loss_test: 0.11429947 G pearson_test: 0.97799164\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 62 次训练 D loss_train: 0.16043436527252197 D acc_train: 50.18315315246582 G loss_train: 0.09466402977705002 G pearson_train: 0.9801480770111084\n",
      "第 62 次测试 D loss_test: 0.35835868 D acc_test: 69.56521739130434 G loss_test: 0.11403172 G pearson_test: 0.9780134\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 63 次训练 D loss_train: 0.1568058431148529 D acc_train: 50.18315315246582 G loss_train: 0.09446896612644196 G pearson_train: 0.9801660180091858\n",
      "第 63 次测试 D loss_test: 0.35400856 D acc_test: 71.19565217391305 G loss_test: 0.113778204 G pearson_test: 0.9780341\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 64 次训练 D loss_train: 0.15327739715576172 D acc_train: 50.18315315246582 G loss_train: 0.09428678452968597 G pearson_train: 0.9801835417747498\n",
      "第 64 次测试 D loss_test: 0.3502236 D acc_test: 73.91304347826086 G loss_test: 0.11353479 G pearson_test: 0.97805405\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 65 次训练 D loss_train: 0.14986714720726013 D acc_train: 50.18315315246582 G loss_train: 0.09411828219890594 G pearson_train: 0.9802001714706421\n",
      "第 65 次测试 D loss_test: 0.34609133 D acc_test: 74.45652173913044 G loss_test: 0.11329775 G pearson_test: 0.9780733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 66 次训练 D loss_train: 0.14653867483139038 D acc_train: 50.18315315246582 G loss_train: 0.093961700797081 G pearson_train: 0.980216383934021\n",
      "第 66 次测试 D loss_test: 0.34225002 D acc_test: 77.71739130434783 G loss_test: 0.11308345 G pearson_test: 0.9780917\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 67 次训练 D loss_train: 0.14328643679618835 D acc_train: 50.18315315246582 G loss_train: 0.09380879253149033 G pearson_train: 0.9802320003509521\n",
      "第 67 次测试 D loss_test: 0.33827025 D acc_test: 79.8913043478261 G loss_test: 0.11287942 G pearson_test: 0.97810936\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 68 次训练 D loss_train: 0.14009925723075867 D acc_train: 50.18315315246582 G loss_train: 0.09366437047719955 G pearson_train: 0.9802471399307251\n",
      "第 68 次测试 D loss_test: 0.3345127 D acc_test: 80.97826086956522 G loss_test: 0.11268147 G pearson_test: 0.9781265\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 69 次训练 D loss_train: 0.13698428869247437 D acc_train: 50.18315315246582 G loss_train: 0.09352003037929535 G pearson_train: 0.9802619218826294\n",
      "第 69 次测试 D loss_test: 0.33059433 D acc_test: 82.6086956521739 G loss_test: 0.112490185 G pearson_test: 0.9781429\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 70 次训练 D loss_train: 0.13395178318023682 D acc_train: 50.18315315246582 G loss_train: 0.09338392317295074 G pearson_train: 0.980276346206665\n",
      "第 70 次测试 D loss_test: 0.32694373 D acc_test: 82.06521739130434 G loss_test: 0.11230337 G pearson_test: 0.978159\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 71 次训练 D loss_train: 0.13098014891147614 D acc_train: 50.18315315246582 G loss_train: 0.09325690567493439 G pearson_train: 0.9802901744842529\n",
      "第 71 次测试 D loss_test: 0.32311517 D acc_test: 82.6086956521739 G loss_test: 0.11212258 G pearson_test: 0.97817457\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 72 次训练 D loss_train: 0.12807422876358032 D acc_train: 50.18315315246582 G loss_train: 0.09314361959695816 G pearson_train: 0.980303168296814\n",
      "第 72 次测试 D loss_test: 0.31947297 D acc_test: 82.06521739130434 G loss_test: 0.11195617 G pearson_test: 0.9781895\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 73 次训练 D loss_train: 0.12522439658641815 D acc_train: 50.18315315246582 G loss_train: 0.09303642809391022 G pearson_train: 0.9803159832954407\n",
      "第 73 次测试 D loss_test: 0.31578994 D acc_test: 83.15217391304348 G loss_test: 0.11180417 G pearson_test: 0.978204\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 74 次训练 D loss_train: 0.12244757264852524 D acc_train: 50.18315315246582 G loss_train: 0.09293471276760101 G pearson_train: 0.980328381061554\n",
      "第 74 次测试 D loss_test: 0.3121678 D acc_test: 83.69565217391305 G loss_test: 0.11166133 G pearson_test: 0.978218\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 75 次训练 D loss_train: 0.11973120272159576 D acc_train: 50.18315315246582 G loss_train: 0.09283968806266785 G pearson_train: 0.9803404211997986\n",
      "第 75 次测试 D loss_test: 0.3085646 D acc_test: 83.69565217391305 G loss_test: 0.1115235 G pearson_test: 0.97823143\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 76 次训练 D loss_train: 0.11706971377134323 D acc_train: 50.18315315246582 G loss_train: 0.09274724125862122 G pearson_train: 0.9803521633148193\n",
      "第 76 次测试 D loss_test: 0.30497426 D acc_test: 83.15217391304348 G loss_test: 0.11139468 G pearson_test: 0.97824454\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 77 次训练 D loss_train: 0.11445844173431396 D acc_train: 50.0 G loss_train: 0.09266354143619537 G pearson_train: 0.9803634881973267\n",
      "第 77 次测试 D loss_test: 0.30138153 D acc_test: 82.06521739130434 G loss_test: 0.111268386 G pearson_test: 0.9782573\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 78 次训练 D loss_train: 0.11191342771053314 D acc_train: 50.0 G loss_train: 0.09258228540420532 G pearson_train: 0.9803745150566101\n",
      "第 78 次测试 D loss_test: 0.29794702 D acc_test: 80.97826086956522 G loss_test: 0.11115149 G pearson_test: 0.97826964\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 79 次训练 D loss_train: 0.1094561293721199 D acc_train: 50.0 G loss_train: 0.09250278025865555 G pearson_train: 0.9803853631019592\n",
      "第 79 次测试 D loss_test: 0.29456106 D acc_test: 79.8913043478261 G loss_test: 0.111036025 G pearson_test: 0.97828174\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 80 次训练 D loss_train: 0.10706847906112671 D acc_train: 50.0 G loss_train: 0.09242460876703262 G pearson_train: 0.980396032333374\n",
      "第 80 次测试 D loss_test: 0.29120505 D acc_test: 78.80434782608695 G loss_test: 0.11092375 G pearson_test: 0.97829336\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 81 次训练 D loss_train: 0.1047448217868805 D acc_train: 50.0 G loss_train: 0.09235060960054398 G pearson_train: 0.9804064035415649\n",
      "第 81 次测试 D loss_test: 0.28788766 D acc_test: 78.26086956521739 G loss_test: 0.11081422 G pearson_test: 0.9783048\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 82 次训练 D loss_train: 0.10247631371021271 D acc_train: 50.0 G loss_train: 0.09228385984897614 G pearson_train: 0.9804165363311768\n",
      "第 82 次测试 D loss_test: 0.28466842 D acc_test: 77.17391304347827 G loss_test: 0.11071031 G pearson_test: 0.97831595\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 83 次训练 D loss_train: 0.10026440024375916 D acc_train: 50.0 G loss_train: 0.09222312271595001 G pearson_train: 0.9804264307022095\n",
      "第 83 次测试 D loss_test: 0.2814728 D acc_test: 78.80434782608695 G loss_test: 0.1106156 G pearson_test: 0.97832686\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 84 次训练 D loss_train: 0.09811832755804062 D acc_train: 50.0 G loss_train: 0.09216564148664474 G pearson_train: 0.9804360866546631\n",
      "第 84 次测试 D loss_test: 0.27840805 D acc_test: 78.80434782608695 G loss_test: 0.11052751 G pearson_test: 0.9783374\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 85 次训练 D loss_train: 0.09603434801101685 D acc_train: 50.0 G loss_train: 0.09210878610610962 G pearson_train: 0.9804455637931824\n",
      "第 85 次测试 D loss_test: 0.2753182 D acc_test: 78.26086956521739 G loss_test: 0.11044279 G pearson_test: 0.97834784\n",
      "9/9 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 86 次训练 D loss_train: 0.09400738775730133 D acc_train: 50.0 G loss_train: 0.09205716848373413 G pearson_train: 0.9804549217224121\n",
      "第 86 次测试 D loss_test: 0.27232048 D acc_test: 77.17391304347827 G loss_test: 0.110360794 G pearson_test: 0.9783579\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 87 次训练 D loss_train: 0.09203332662582397 D acc_train: 50.0 G loss_train: 0.09201071411371231 G pearson_train: 0.9804638624191284\n",
      "第 87 次测试 D loss_test: 0.26931232 D acc_test: 76.08695652173914 G loss_test: 0.11028317 G pearson_test: 0.9783679\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 88 次训练 D loss_train: 0.09010547399520874 D acc_train: 50.0 G loss_train: 0.09196580946445465 G pearson_train: 0.9804727435112\n",
      "第 88 次测试 D loss_test: 0.26643437 D acc_test: 76.63043478260869 G loss_test: 0.11021311 G pearson_test: 0.9783775\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 89 次训练 D loss_train: 0.08822578191757202 D acc_train: 50.0 G loss_train: 0.0919267013669014 G pearson_train: 0.9804813265800476\n",
      "第 89 次测试 D loss_test: 0.26351967 D acc_test: 75.54347826086956 G loss_test: 0.110143416 G pearson_test: 0.9783868\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 90 次训练 D loss_train: 0.08639000356197357 D acc_train: 50.0 G loss_train: 0.09188985079526901 G pearson_train: 0.9804896712303162\n",
      "第 90 次测试 D loss_test: 0.26067024 D acc_test: 75.54347826086956 G loss_test: 0.110082164 G pearson_test: 0.978396\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 91 次训练 D loss_train: 0.08459816873073578 D acc_train: 50.0 G loss_train: 0.09185607731342316 G pearson_train: 0.9804978966712952\n",
      "第 91 次测试 D loss_test: 0.25788286 D acc_test: 74.45652173913044 G loss_test: 0.1100231 G pearson_test: 0.97840494\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 92 次训练 D loss_train: 0.08285100758075714 D acc_train: 50.0 G loss_train: 0.09182031452655792 G pearson_train: 0.9805060625076294\n",
      "第 92 次测试 D loss_test: 0.25511104 D acc_test: 73.91304347826086 G loss_test: 0.109967716 G pearson_test: 0.97841376\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 93 次训练 D loss_train: 0.08115164935588837 D acc_train: 50.0 G loss_train: 0.0917876660823822 G pearson_train: 0.9805139899253845\n",
      "第 93 次测试 D loss_test: 0.25246206 D acc_test: 72.28260869565217 G loss_test: 0.10990933 G pearson_test: 0.9784222\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 94 次训练 D loss_train: 0.07949594408273697 D acc_train: 50.0 G loss_train: 0.0917583554983139 G pearson_train: 0.980521559715271\n",
      "第 94 次测试 D loss_test: 0.24981682 D acc_test: 72.28260869565217 G loss_test: 0.109854825 G pearson_test: 0.9784308\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 95 次训练 D loss_train: 0.07787835597991943 D acc_train: 50.0 G loss_train: 0.09173161536455154 G pearson_train: 0.9805291891098022\n",
      "第 95 次测试 D loss_test: 0.24723554 D acc_test: 71.73913043478261 G loss_test: 0.1098052 G pearson_test: 0.97843903\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 96 次训练 D loss_train: 0.07629632949829102 D acc_train: 50.0 G loss_train: 0.09170742332935333 G pearson_train: 0.9805365800857544\n",
      "第 96 次测试 D loss_test: 0.2447297 D acc_test: 70.1086956521739 G loss_test: 0.10975748 G pearson_test: 0.978447\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 97 次训练 D loss_train: 0.07474671304225922 D acc_train: 50.0 G loss_train: 0.09168735146522522 G pearson_train: 0.9805439114570618\n",
      "第 97 次测试 D loss_test: 0.24221887 D acc_test: 69.56521739130434 G loss_test: 0.10971366 G pearson_test: 0.97845495\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 98 次训练 D loss_train: 0.0732312947511673 D acc_train: 50.0 G loss_train: 0.09167037904262543 G pearson_train: 0.9805509448051453\n",
      "第 98 次测试 D loss_test: 0.2398307 D acc_test: 69.02173913043478 G loss_test: 0.10967438 G pearson_test: 0.9784627\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 99 次训练 D loss_train: 0.0717591866850853 D acc_train: 50.0 G loss_train: 0.09165345877408981 G pearson_train: 0.9805578589439392\n",
      "第 99 次测试 D loss_test: 0.23740914 D acc_test: 67.93478260869566 G loss_test: 0.109639496 G pearson_test: 0.9784703\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 100 次训练 D loss_train: 0.07032228261232376 D acc_train: 50.0 G loss_train: 0.09163600951433182 G pearson_train: 0.9805648922920227\n",
      "第 100 次测试 D loss_test: 0.23508285 D acc_test: 67.3913043478261 G loss_test: 0.109604254 G pearson_test: 0.9784778\n",
      "9/9 [==============================] - 0s 14ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 101 次训练 D loss_train: 0.0689273253083229 D acc_train: 50.0 G loss_train: 0.09161766618490219 G pearson_train: 0.9805715084075928\n",
      "第 101 次测试 D loss_test: 0.23281442 D acc_test: 66.30434782608695 G loss_test: 0.109568946 G pearson_test: 0.97848517\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 102 次训练 D loss_train: 0.06757371127605438 D acc_train: 50.0 G loss_train: 0.09160330146551132 G pearson_train: 0.9805783033370972\n",
      "第 102 次测试 D loss_test: 0.23059481 D acc_test: 65.76086956521739 G loss_test: 0.1095328 G pearson_test: 0.97849244\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 103 次训练 D loss_train: 0.06625765562057495 D acc_train: 50.0 G loss_train: 0.09158868342638016 G pearson_train: 0.9805848002433777\n",
      "第 103 次测试 D loss_test: 0.22839817 D acc_test: 62.5 G loss_test: 0.109501615 G pearson_test: 0.97849965\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 104 次训练 D loss_train: 0.06498582661151886 D acc_train: 50.0 G loss_train: 0.09157746285200119 G pearson_train: 0.9805911779403687\n",
      "第 104 次测试 D loss_test: 0.22624683 D acc_test: 61.41304347826087 G loss_test: 0.10947076 G pearson_test: 0.9785068\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 105 次训练 D loss_train: 0.06374427676200867 D acc_train: 50.0 G loss_train: 0.0915662869811058 G pearson_train: 0.9805976152420044\n",
      "第 105 次测试 D loss_test: 0.22415008 D acc_test: 60.86956521739131 G loss_test: 0.10944437 G pearson_test: 0.97851366\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 106 次训练 D loss_train: 0.06254179775714874 D acc_train: 50.0 G loss_train: 0.09155716747045517 G pearson_train: 0.980603814125061\n",
      "第 106 次测试 D loss_test: 0.22212292 D acc_test: 60.86956521739131 G loss_test: 0.10941834 G pearson_test: 0.9785205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 107 次训练 D loss_train: 0.06137282773852348 D acc_train: 50.0 G loss_train: 0.09154827892780304 G pearson_train: 0.9806098937988281\n",
      "第 107 次测试 D loss_test: 0.22012074 D acc_test: 59.78260869565217 G loss_test: 0.10939368 G pearson_test: 0.9785271\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 108 次训练 D loss_train: 0.060235053300857544 D acc_train: 50.0 G loss_train: 0.09153587371110916 G pearson_train: 0.9806159734725952\n",
      "第 108 次测试 D loss_test: 0.21814805 D acc_test: 59.2391304347826 G loss_test: 0.1093706 G pearson_test: 0.97853386\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 109 次训练 D loss_train: 0.059130750596523285 D acc_train: 50.0 G loss_train: 0.09152737259864807 G pearson_train: 0.9806220531463623\n",
      "第 109 次测试 D loss_test: 0.21624959 D acc_test: 58.69565217391305 G loss_test: 0.10934333 G pearson_test: 0.9785405\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 110 次训练 D loss_train: 0.05804364010691643 D acc_train: 50.0 G loss_train: 0.09152567386627197 G pearson_train: 0.9806278347969055\n",
      "第 110 次测试 D loss_test: 0.21433389 D acc_test: 58.69565217391305 G loss_test: 0.10932121 G pearson_test: 0.9785469\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 21ms/step\n",
      "第 111 次训练 D loss_train: 0.056963443756103516 D acc_train: 50.0 G loss_train: 0.09152454882860184 G pearson_train: 0.9806336164474487\n",
      "第 111 次测试 D loss_test: 0.21249202 D acc_test: 58.152173913043484 G loss_test: 0.10930696 G pearson_test: 0.9785532\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 112 次训练 D loss_train: 0.055892299860715866 D acc_train: 50.0 G loss_train: 0.09152332693338394 G pearson_train: 0.9806392788887024\n",
      "第 112 次测试 D loss_test: 0.21069843 D acc_test: 57.065217391304344 G loss_test: 0.10929425 G pearson_test: 0.9785593\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 113 次训练 D loss_train: 0.054834187030792236 D acc_train: 50.0 G loss_train: 0.09152424335479736 G pearson_train: 0.9806447625160217\n",
      "第 113 次测试 D loss_test: 0.20894054 D acc_test: 57.065217391304344 G loss_test: 0.1092804 G pearson_test: 0.9785656\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 114 次训练 D loss_train: 0.05381853133440018 D acc_train: 50.0 G loss_train: 0.09152757376432419 G pearson_train: 0.9806501269340515\n",
      "第 114 次测试 D loss_test: 0.20721036 D acc_test: 57.608695652173914 G loss_test: 0.109269895 G pearson_test: 0.9785715\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 115 次训练 D loss_train: 0.05285138636827469 D acc_train: 50.0 G loss_train: 0.09153120964765549 G pearson_train: 0.9806554317474365\n",
      "第 115 次测试 D loss_test: 0.20548323 D acc_test: 57.065217391304344 G loss_test: 0.10926399 G pearson_test: 0.9785774\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 116 次训练 D loss_train: 0.051912449300289154 D acc_train: 50.0 G loss_train: 0.09153635799884796 G pearson_train: 0.9806606769561768\n",
      "第 116 次测试 D loss_test: 0.20380759 D acc_test: 57.065217391304344 G loss_test: 0.10925797 G pearson_test: 0.9785832\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 117 次训练 D loss_train: 0.050997838377952576 D acc_train: 50.0 G loss_train: 0.09154076129198074 G pearson_train: 0.9806658029556274\n",
      "第 117 次测试 D loss_test: 0.2021791 D acc_test: 57.065217391304344 G loss_test: 0.10925327 G pearson_test: 0.978589\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 118 次训练 D loss_train: 0.050115372985601425 D acc_train: 50.0 G loss_train: 0.09154633432626724 G pearson_train: 0.980670690536499\n",
      "第 118 次测试 D loss_test: 0.20058814 D acc_test: 56.52173913043478 G loss_test: 0.10924861 G pearson_test: 0.97859466\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 119 次训练 D loss_train: 0.04926423728466034 D acc_train: 50.0 G loss_train: 0.09155208617448807 G pearson_train: 0.9806756377220154\n",
      "第 119 次测试 D loss_test: 0.19906184 D acc_test: 55.97826086956522 G loss_test: 0.10924374 G pearson_test: 0.97860014\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 120 次训练 D loss_train: 0.048443879932165146 D acc_train: 50.0 G loss_train: 0.09155645966529846 G pearson_train: 0.9806804656982422\n",
      "第 120 次测试 D loss_test: 0.19759396 D acc_test: 55.434782608695656 G loss_test: 0.10924072 G pearson_test: 0.9786058\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 121 次训练 D loss_train: 0.04765297472476959 D acc_train: 50.0 G loss_train: 0.09155693650245667 G pearson_train: 0.9806853532791138\n",
      "第 121 次测试 D loss_test: 0.19617057 D acc_test: 55.434782608695656 G loss_test: 0.10923412 G pearson_test: 0.97861105\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 122 次训练 D loss_train: 0.04689019173383713 D acc_train: 50.0 G loss_train: 0.0915602594614029 G pearson_train: 0.980690062046051\n",
      "第 122 次测试 D loss_test: 0.19480616 D acc_test: 54.891304347826086 G loss_test: 0.10922405 G pearson_test: 0.97861654\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 123 次训练 D loss_train: 0.04615184664726257 D acc_train: 50.0 G loss_train: 0.09156342595815659 G pearson_train: 0.9806946516036987\n",
      "第 123 次测试 D loss_test: 0.1934605 D acc_test: 54.347826086956516 G loss_test: 0.10921811 G pearson_test: 0.97862184\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 124 次训练 D loss_train: 0.045438338071107864 D acc_train: 50.0 G loss_train: 0.09156675636768341 G pearson_train: 0.9806992411613464\n",
      "第 124 次测试 D loss_test: 0.19216597 D acc_test: 54.347826086956516 G loss_test: 0.10921114 G pearson_test: 0.978627\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 125 次训练 D loss_train: 0.04474230110645294 D acc_train: 50.0 G loss_train: 0.09157080948352814 G pearson_train: 0.980703592300415\n",
      "第 125 次测试 D loss_test: 0.19091734 D acc_test: 54.347826086956516 G loss_test: 0.109205954 G pearson_test: 0.97863203\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 126 次训练 D loss_train: 0.04406622797250748 D acc_train: 50.0 G loss_train: 0.09157784283161163 G pearson_train: 0.9807080030441284\n",
      "第 126 次测试 D loss_test: 0.18968786 D acc_test: 54.347826086956516 G loss_test: 0.10920154 G pearson_test: 0.978637\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 127 次训练 D loss_train: 0.043407559394836426 D acc_train: 50.0 G loss_train: 0.09157998114824295 G pearson_train: 0.9807121753692627\n",
      "第 127 次测试 D loss_test: 0.18851028 D acc_test: 54.347826086956516 G loss_test: 0.10920072 G pearson_test: 0.97864187\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 128 次训练 D loss_train: 0.04276971518993378 D acc_train: 50.0 G loss_train: 0.09158606827259064 G pearson_train: 0.9807165265083313\n",
      "第 128 次测试 D loss_test: 0.18737714 D acc_test: 54.347826086956516 G loss_test: 0.10919362 G pearson_test: 0.97864676\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 129 次训练 D loss_train: 0.042145270854234695 D acc_train: 50.0 G loss_train: 0.0915912389755249 G pearson_train: 0.9807206988334656\n",
      "第 129 次测试 D loss_test: 0.18626186 D acc_test: 53.2608695652174 G loss_test: 0.109191336 G pearson_test: 0.9786514\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 130 次训练 D loss_train: 0.041535891592502594 D acc_train: 50.0 G loss_train: 0.09160147607326508 G pearson_train: 0.980724573135376\n",
      "第 130 次测试 D loss_test: 0.18518052 D acc_test: 53.2608695652174 G loss_test: 0.109186895 G pearson_test: 0.97865605\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 131 次训练 D loss_train: 0.0409378781914711 D acc_train: 50.0 G loss_train: 0.09161089360713959 G pearson_train: 0.9807283878326416\n",
      "第 131 次测试 D loss_test: 0.18409432 D acc_test: 52.71739130434783 G loss_test: 0.10919112 G pearson_test: 0.9786605\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 132 次训练 D loss_train: 0.04035579413175583 D acc_train: 50.0 G loss_train: 0.09161757677793503 G pearson_train: 0.980732262134552\n",
      "第 132 次测试 D loss_test: 0.18305337 D acc_test: 52.17391304347826 G loss_test: 0.10919366 G pearson_test: 0.978665\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 133 次训练 D loss_train: 0.0397796668112278 D acc_train: 50.0 G loss_train: 0.09162770956754684 G pearson_train: 0.9807360172271729\n",
      "第 133 次测试 D loss_test: 0.18203679 D acc_test: 51.63043478260869 G loss_test: 0.10919258 G pearson_test: 0.9786693\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 134 次训练 D loss_train: 0.039219725877046585 D acc_train: 50.0 G loss_train: 0.09163840860128403 G pearson_train: 0.9807396531105042\n",
      "第 134 次测试 D loss_test: 0.18102062 D acc_test: 51.63043478260869 G loss_test: 0.1091967 G pearson_test: 0.9786736\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 135 次训练 D loss_train: 0.03868024796247482 D acc_train: 50.0 G loss_train: 0.09164699167013168 G pearson_train: 0.9807433485984802\n",
      "第 135 次测试 D loss_test: 0.18002464 D acc_test: 51.63043478260869 G loss_test: 0.109202296 G pearson_test: 0.97867775\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 136 次训练 D loss_train: 0.03814943507313728 D acc_train: 50.0 G loss_train: 0.09165918827056885 G pearson_train: 0.9807467460632324\n",
      "第 136 次测试 D loss_test: 0.17906913 D acc_test: 51.08695652173913 G loss_test: 0.10920455 G pearson_test: 0.97868186\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 137 次训练 D loss_train: 0.03762926161289215 D acc_train: 50.0 G loss_train: 0.09167493134737015 G pearson_train: 0.9807501435279846\n",
      "第 137 次测试 D loss_test: 0.17811207 D acc_test: 51.08695652173913 G loss_test: 0.109211504 G pearson_test: 0.9786859\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 138 次训练 D loss_train: 0.03712891787290573 D acc_train: 50.0 G loss_train: 0.0916915014386177 G pearson_train: 0.980753481388092\n",
      "第 138 次测试 D loss_test: 0.17721829 D acc_test: 51.08695652173913 G loss_test: 0.109224044 G pearson_test: 0.9786899\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 139 次训练 D loss_train: 0.036644164472818375 D acc_train: 50.0 G loss_train: 0.09170437604188919 G pearson_train: 0.9807567596435547\n",
      "第 139 次测试 D loss_test: 0.17635743 D acc_test: 51.08695652173913 G loss_test: 0.10923745 G pearson_test: 0.9786938\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 140 次训练 D loss_train: 0.0361778661608696 D acc_train: 50.0 G loss_train: 0.09171631932258606 G pearson_train: 0.9807600378990173\n",
      "第 140 次测试 D loss_test: 0.17555772 D acc_test: 51.08695652173913 G loss_test: 0.10924565 G pearson_test: 0.9786976\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 141 次训练 D loss_train: 0.03572774678468704 D acc_train: 50.0 G loss_train: 0.09172751009464264 G pearson_train: 0.9807632565498352\n",
      "第 141 次测试 D loss_test: 0.17475894 D acc_test: 51.08695652173913 G loss_test: 0.10925379 G pearson_test: 0.97870135\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 142 次训练 D loss_train: 0.03528713434934616 D acc_train: 50.0 G loss_train: 0.09173764288425446 G pearson_train: 0.9807664752006531\n",
      "第 142 次测试 D loss_test: 0.17400801 D acc_test: 51.08695652173913 G loss_test: 0.10926243 G pearson_test: 0.97870505\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 143 次训练 D loss_train: 0.034848086535930634 D acc_train: 50.0 G loss_train: 0.09175286442041397 G pearson_train: 0.9807695150375366\n",
      "第 143 次测试 D loss_test: 0.17330454 D acc_test: 51.08695652173913 G loss_test: 0.10926645 G pearson_test: 0.97870874\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 144 次训练 D loss_train: 0.0344233363866806 D acc_train: 50.0 G loss_train: 0.09176851809024811 G pearson_train: 0.9807722568511963\n",
      "第 144 次测试 D loss_test: 0.17262442 D acc_test: 51.08695652173913 G loss_test: 0.10927976 G pearson_test: 0.97871214\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 145 次训练 D loss_train: 0.0340135432779789 D acc_train: 50.0 G loss_train: 0.09178512543439865 G pearson_train: 0.9807751178741455\n",
      "第 145 次测试 D loss_test: 0.1719585 D acc_test: 51.08695652173913 G loss_test: 0.10929275 G pearson_test: 0.9787157\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 146 次训练 D loss_train: 0.03361619636416435 D acc_train: 50.0 G loss_train: 0.09180319309234619 G pearson_train: 0.9807778000831604\n",
      "第 146 次测试 D loss_test: 0.17133158 D acc_test: 51.08695652173913 G loss_test: 0.10930717 G pearson_test: 0.978719\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 147 次训练 D loss_train: 0.033228375017642975 D acc_train: 50.0 G loss_train: 0.09181767702102661 G pearson_train: 0.9807807207107544\n",
      "第 147 次测试 D loss_test: 0.17074582 D acc_test: 51.08695652173913 G loss_test: 0.10932441 G pearson_test: 0.9787224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 148 次训练 D loss_train: 0.03285253047943115 D acc_train: 50.0 G loss_train: 0.09183502197265625 G pearson_train: 0.9807832837104797\n",
      "第 148 次测试 D loss_test: 0.17016345 D acc_test: 51.08695652173913 G loss_test: 0.10933524 G pearson_test: 0.9787257\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 149 次训练 D loss_train: 0.03248395398259163 D acc_train: 50.0 G loss_train: 0.09184771776199341 G pearson_train: 0.9807860851287842\n",
      "第 149 次测试 D loss_test: 0.16961569 D acc_test: 51.08695652173913 G loss_test: 0.10935271 G pearson_test: 0.9787289\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 150 次训练 D loss_train: 0.032126933336257935 D acc_train: 50.0 G loss_train: 0.09186374396085739 G pearson_train: 0.9807886481285095\n",
      "第 150 次测试 D loss_test: 0.16909947 D acc_test: 51.08695652173913 G loss_test: 0.10936269 G pearson_test: 0.97873205\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 151 次训练 D loss_train: 0.0317738875746727 D acc_train: 50.0 G loss_train: 0.09187720715999603 G pearson_train: 0.9807911515235901\n",
      "第 151 次测试 D loss_test: 0.16860041 D acc_test: 51.08695652173913 G loss_test: 0.109377846 G pearson_test: 0.97873515\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 152 次训练 D loss_train: 0.031431060284376144 D acc_train: 50.0 G loss_train: 0.09188755601644516 G pearson_train: 0.9807937145233154\n",
      "第 152 次测试 D loss_test: 0.16811752 D acc_test: 51.08695652173913 G loss_test: 0.109389715 G pearson_test: 0.9787382\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 153 次训练 D loss_train: 0.031098585575819016 D acc_train: 50.0 G loss_train: 0.09190426766872406 G pearson_train: 0.980796217918396\n",
      "第 153 次测试 D loss_test: 0.16765954 D acc_test: 51.08695652173913 G loss_test: 0.10939716 G pearson_test: 0.97874117\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 154 次训练 D loss_train: 0.0307723768055439 D acc_train: 50.0 G loss_train: 0.09192077070474625 G pearson_train: 0.9807984232902527\n",
      "第 154 次测试 D loss_test: 0.1672052 D acc_test: 51.08695652173913 G loss_test: 0.10941386 G pearson_test: 0.97874403\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 155 次训练 D loss_train: 0.03045722469687462 D acc_train: 50.0 G loss_train: 0.09193597733974457 G pearson_train: 0.9808008074760437\n",
      "第 155 次测试 D loss_test: 0.16677123 D acc_test: 51.08695652173913 G loss_test: 0.10943 G pearson_test: 0.97874695\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 156 次训练 D loss_train: 0.03015187382698059 D acc_train: 50.0 G loss_train: 0.09195129573345184 G pearson_train: 0.9808030724525452\n",
      "第 156 次测试 D loss_test: 0.16637367 D acc_test: 50.54347826086957 G loss_test: 0.10944352 G pearson_test: 0.9787497\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 157 次训练 D loss_train: 0.02985551580786705 D acc_train: 50.0 G loss_train: 0.09196934103965759 G pearson_train: 0.9808050394058228\n",
      "第 157 次测试 D loss_test: 0.16599585 D acc_test: 50.54347826086957 G loss_test: 0.10945842 G pearson_test: 0.9787525\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 158 次训练 D loss_train: 0.029569242149591446 D acc_train: 50.0 G loss_train: 0.09198751300573349 G pearson_train: 0.9808071851730347\n",
      "第 158 次测试 D loss_test: 0.16562155 D acc_test: 50.54347826086957 G loss_test: 0.10947664 G pearson_test: 0.9787552\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 159 次训练 D loss_train: 0.02929205819964409 D acc_train: 50.0 G loss_train: 0.09200610220432281 G pearson_train: 0.9808090925216675\n",
      "第 159 次测试 D loss_test: 0.16526753 D acc_test: 50.54347826086957 G loss_test: 0.109494865 G pearson_test: 0.9787577\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 160 次训练 D loss_train: 0.029019653797149658 D acc_train: 50.0 G loss_train: 0.09202779829502106 G pearson_train: 0.9808109998703003\n",
      "第 160 次测试 D loss_test: 0.16492721 D acc_test: 50.54347826086957 G loss_test: 0.10951505 G pearson_test: 0.97876024\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 161 次训练 D loss_train: 0.02875015139579773 D acc_train: 50.0 G loss_train: 0.09204748272895813 G pearson_train: 0.9808128476142883\n",
      "第 161 次测试 D loss_test: 0.16457577 D acc_test: 50.54347826086957 G loss_test: 0.10953961 G pearson_test: 0.9787628\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 162 次训练 D loss_train: 0.028484690934419632 D acc_train: 50.0 G loss_train: 0.09206993877887726 G pearson_train: 0.9808145761489868\n",
      "第 162 次测试 D loss_test: 0.16424452 D acc_test: 50.54347826086957 G loss_test: 0.10956205 G pearson_test: 0.97876513\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 163 次训练 D loss_train: 0.02822285145521164 D acc_train: 50.0 G loss_train: 0.09208304435014725 G pearson_train: 0.9808164834976196\n",
      "第 163 次测试 D loss_test: 0.16392356 D acc_test: 50.54347826086957 G loss_test: 0.10958661 G pearson_test: 0.97876745\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 164 次训练 D loss_train: 0.027973350137472153 D acc_train: 50.0 G loss_train: 0.09209316968917847 G pearson_train: 0.9808183908462524\n",
      "第 164 次测试 D loss_test: 0.16365817 D acc_test: 50.54347826086957 G loss_test: 0.10959764 G pearson_test: 0.97877\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 165 次训练 D loss_train: 0.027732819318771362 D acc_train: 50.0 G loss_train: 0.09211189299821854 G pearson_train: 0.9808200597763062\n",
      "第 165 次测试 D loss_test: 0.1634027 D acc_test: 50.54347826086957 G loss_test: 0.109607406 G pearson_test: 0.97877234\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 166 次训练 D loss_train: 0.027493111789226532 D acc_train: 50.0 G loss_train: 0.09213020652532578 G pearson_train: 0.9808216094970703\n",
      "第 166 次测试 D loss_test: 0.16316524 D acc_test: 50.54347826086957 G loss_test: 0.10962834 G pearson_test: 0.97877455\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 167 次训练 D loss_train: 0.0272538922727108 D acc_train: 50.0 G loss_train: 0.09214848279953003 G pearson_train: 0.9808230400085449\n",
      "第 167 次测试 D loss_test: 0.16295114 D acc_test: 50.54347826086957 G loss_test: 0.1096472 G pearson_test: 0.97877663\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 168 次训练 D loss_train: 0.027008291333913803 D acc_train: 50.0 G loss_train: 0.09216570854187012 G pearson_train: 0.9808245897293091\n",
      "第 168 次测试 D loss_test: 0.16273156 D acc_test: 50.54347826086957 G loss_test: 0.109667644 G pearson_test: 0.97877866\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 169 次训练 D loss_train: 0.02675265446305275 D acc_train: 50.0 G loss_train: 0.09219379723072052 G pearson_train: 0.9808260202407837\n",
      "第 169 次测试 D loss_test: 0.16248289 D acc_test: 50.54347826086957 G loss_test: 0.10968543 G pearson_test: 0.97878075\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 170 次训练 D loss_train: 0.026468217372894287 D acc_train: 50.0 G loss_train: 0.09222777187824249 G pearson_train: 0.9808271527290344\n",
      "第 170 次测试 D loss_test: 0.1620995 D acc_test: 50.54347826086957 G loss_test: 0.10971976 G pearson_test: 0.9787825\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 171 次训练 D loss_train: 0.02617892250418663 D acc_train: 50.0 G loss_train: 0.09225856512784958 G pearson_train: 0.9808282256126404\n",
      "第 171 次测试 D loss_test: 0.16163504 D acc_test: 50.54347826086957 G loss_test: 0.10975679 G pearson_test: 0.97878426\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 172 次训练 D loss_train: 0.025897599756717682 D acc_train: 50.0 G loss_train: 0.09229134023189545 G pearson_train: 0.9808290600776672\n",
      "第 172 次测试 D loss_test: 0.16132924 D acc_test: 50.54347826086957 G loss_test: 0.10978617 G pearson_test: 0.9787862\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 173 次训练 D loss_train: 0.025641294196248055 D acc_train: 50.0 G loss_train: 0.09232588112354279 G pearson_train: 0.9808299541473389\n",
      "第 173 次测试 D loss_test: 0.16105145 D acc_test: 50.54347826086957 G loss_test: 0.10982171 G pearson_test: 0.97878766\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 174 次训练 D loss_train: 0.025383535772562027 D acc_train: 50.0 G loss_train: 0.09235373884439468 G pearson_train: 0.9808306694030762\n",
      "第 174 次测试 D loss_test: 0.16087478 D acc_test: 50.54347826086957 G loss_test: 0.109857015 G pearson_test: 0.9787894\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 175 次训练 D loss_train: 0.025153983384370804 D acc_train: 50.0 G loss_train: 0.09238249808549881 G pearson_train: 0.9808316230773926\n",
      "第 175 次测试 D loss_test: 0.16068357 D acc_test: 50.54347826086957 G loss_test: 0.10988749 G pearson_test: 0.978791\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 176 次训练 D loss_train: 0.024925537407398224 D acc_train: 50.0 G loss_train: 0.09240645170211792 G pearson_train: 0.9808323383331299\n",
      "第 176 次测试 D loss_test: 0.16045634 D acc_test: 50.54347826086957 G loss_test: 0.109921284 G pearson_test: 0.97879255\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 177 次训练 D loss_train: 0.024706382304430008 D acc_train: 50.0 G loss_train: 0.09242329001426697 G pearson_train: 0.9808332920074463\n",
      "第 177 次测试 D loss_test: 0.16026288 D acc_test: 50.54347826086957 G loss_test: 0.10994816 G pearson_test: 0.9787943\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 178 次训练 D loss_train: 0.0245064664632082 D acc_train: 50.0 G loss_train: 0.0924379974603653 G pearson_train: 0.9808341264724731\n",
      "第 178 次测试 D loss_test: 0.16019493 D acc_test: 50.54347826086957 G loss_test: 0.10996399 G pearson_test: 0.97879595\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 179 次训练 D loss_train: 0.024330761283636093 D acc_train: 50.0 G loss_train: 0.09245055168867111 G pearson_train: 0.9808349609375\n",
      "第 179 次测试 D loss_test: 0.16022448 D acc_test: 50.54347826086957 G loss_test: 0.10997763 G pearson_test: 0.9787974\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 180 次训练 D loss_train: 0.024175330996513367 D acc_train: 50.0 G loss_train: 0.09246724843978882 G pearson_train: 0.9808355569839478\n",
      "第 180 次测试 D loss_test: 0.16030255 D acc_test: 50.54347826086957 G loss_test: 0.109990805 G pearson_test: 0.9787988\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 181 次训练 D loss_train: 0.024029308930039406 D acc_train: 50.0 G loss_train: 0.09248737245798111 G pearson_train: 0.9808360934257507\n",
      "第 181 次测试 D loss_test: 0.16040799 D acc_test: 50.54347826086957 G loss_test: 0.11001018 G pearson_test: 0.9788002\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 182 次训练 D loss_train: 0.02388814091682434 D acc_train: 50.0 G loss_train: 0.09250576794147491 G pearson_train: 0.9808366298675537\n",
      "第 182 次测试 D loss_test: 0.16052866 D acc_test: 50.54347826086957 G loss_test: 0.11003413 G pearson_test: 0.9788015\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 183 次训练 D loss_train: 0.023754127323627472 D acc_train: 50.0 G loss_train: 0.092523954808712 G pearson_train: 0.9808370471000671\n",
      "第 183 次测试 D loss_test: 0.16067295 D acc_test: 50.54347826086957 G loss_test: 0.110056 G pearson_test: 0.9788026\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 184 次训练 D loss_train: 0.023627664893865585 D acc_train: 50.0 G loss_train: 0.09254614263772964 G pearson_train: 0.980837345123291\n",
      "第 184 次测试 D loss_test: 0.16083205 D acc_test: 50.54347826086957 G loss_test: 0.1100782 G pearson_test: 0.9788038\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 185 次训练 D loss_train: 0.0235030185431242 D acc_train: 50.0 G loss_train: 0.09256641566753387 G pearson_train: 0.9808375835418701\n",
      "第 185 次测试 D loss_test: 0.16099748 D acc_test: 50.54347826086957 G loss_test: 0.1101072 G pearson_test: 0.978805\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 186 次训练 D loss_train: 0.023383477702736855 D acc_train: 50.0 G loss_train: 0.09258593618869781 G pearson_train: 0.980837881565094\n",
      "第 186 次测试 D loss_test: 0.1611769 D acc_test: 50.54347826086957 G loss_test: 0.110133015 G pearson_test: 0.9788061\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 187 次训练 D loss_train: 0.023269187659025192 D acc_train: 50.0 G loss_train: 0.09260152280330658 G pearson_train: 0.9808382391929626\n",
      "第 187 次测试 D loss_test: 0.16137657 D acc_test: 50.54347826086957 G loss_test: 0.11015689 G pearson_test: 0.9788072\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 188 次训练 D loss_train: 0.023160532116889954 D acc_train: 50.0 G loss_train: 0.09261388331651688 G pearson_train: 0.9808385372161865\n",
      "第 188 次测试 D loss_test: 0.1615959 D acc_test: 50.0 G loss_test: 0.1101769 G pearson_test: 0.9788083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 189 次训练 D loss_train: 0.02305527776479721 D acc_train: 50.0 G loss_train: 0.0926317572593689 G pearson_train: 0.9808389544487\n",
      "第 189 次测试 D loss_test: 0.16183102 D acc_test: 50.0 G loss_test: 0.1101928 G pearson_test: 0.9788094\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 190 次训练 D loss_train: 0.022951260209083557 D acc_train: 50.0 G loss_train: 0.09264475852251053 G pearson_train: 0.9808390736579895\n",
      "第 190 次测试 D loss_test: 0.16207105 D acc_test: 50.0 G loss_test: 0.11021658 G pearson_test: 0.97881037\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 191 次训练 D loss_train: 0.0228518508374691 D acc_train: 50.0 G loss_train: 0.09265685081481934 G pearson_train: 0.9808392524719238\n",
      "第 191 次测试 D loss_test: 0.1623247 D acc_test: 50.0 G loss_test: 0.1102345 G pearson_test: 0.97881144\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 192 次训练 D loss_train: 0.02275630086660385 D acc_train: 50.0 G loss_train: 0.09267374873161316 G pearson_train: 0.9808393716812134\n",
      "第 192 次测试 D loss_test: 0.16259137 D acc_test: 50.0 G loss_test: 0.11025045 G pearson_test: 0.9788123\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 193 次训练 D loss_train: 0.022661961615085602 D acc_train: 50.0 G loss_train: 0.09269072115421295 G pearson_train: 0.9808393716812134\n",
      "第 193 次测试 D loss_test: 0.16284956 D acc_test: 50.0 G loss_test: 0.1102741 G pearson_test: 0.9788132\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 194 次训练 D loss_train: 0.022568680346012115 D acc_train: 50.0 G loss_train: 0.09270725399255753 G pearson_train: 0.9808394908905029\n",
      "第 194 次测试 D loss_test: 0.16312096 D acc_test: 50.0 G loss_test: 0.110297665 G pearson_test: 0.9788139\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 195 次训练 D loss_train: 0.022478178143501282 D acc_train: 50.0 G loss_train: 0.09272310137748718 G pearson_train: 0.9808393716812134\n",
      "第 195 次测试 D loss_test: 0.16340402 D acc_test: 50.0 G loss_test: 0.11031999 G pearson_test: 0.9788148\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 196 次训练 D loss_train: 0.022391516715288162 D acc_train: 50.0 G loss_train: 0.09273450076580048 G pearson_train: 0.9808393120765686\n",
      "第 196 次测试 D loss_test: 0.16370468 D acc_test: 50.0 G loss_test: 0.11034215 G pearson_test: 0.97881544\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 197 次训练 D loss_train: 0.022309402003884315 D acc_train: 50.0 G loss_train: 0.09274562448263168 G pearson_train: 0.9808393716812134\n",
      "第 197 次测试 D loss_test: 0.16401616 D acc_test: 50.0 G loss_test: 0.11035934 G pearson_test: 0.9788162\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 198 次训练 D loss_train: 0.022229598835110664 D acc_train: 50.0 G loss_train: 0.0927593931555748 G pearson_train: 0.9808392524719238\n",
      "第 198 次测试 D loss_test: 0.1643534 D acc_test: 50.0 G loss_test: 0.110374905 G pearson_test: 0.978817\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 199 次训练 D loss_train: 0.02215001732110977 D acc_train: 50.0 G loss_train: 0.09277701377868652 G pearson_train: 0.9808389544487\n",
      "第 199 次测试 D loss_test: 0.16469222 D acc_test: 50.0 G loss_test: 0.1103953 G pearson_test: 0.9788176\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 200 次训练 D loss_train: 0.022068597376346588 D acc_train: 50.0 G loss_train: 0.09279116243124008 G pearson_train: 0.9808387756347656\n",
      "第 200 次测试 D loss_test: 0.16504312 D acc_test: 50.0 G loss_test: 0.11041984 G pearson_test: 0.9788182\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 201 次训练 D loss_train: 0.021990405395627022 D acc_train: 50.0 G loss_train: 0.09280557185411453 G pearson_train: 0.9808384776115417\n",
      "第 201 次测试 D loss_test: 0.1654155 D acc_test: 50.0 G loss_test: 0.110438615 G pearson_test: 0.97881866\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 202 次训练 D loss_train: 0.021915830671787262 D acc_train: 50.0 G loss_train: 0.09281760454177856 G pearson_train: 0.9808381795883179\n",
      "第 202 次测试 D loss_test: 0.16579571 D acc_test: 50.0 G loss_test: 0.11045876 G pearson_test: 0.97881913\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 203 次训练 D loss_train: 0.021847549825906754 D acc_train: 50.0 G loss_train: 0.09282815456390381 G pearson_train: 0.9808379411697388\n",
      "第 203 次测试 D loss_test: 0.16620834 D acc_test: 50.0 G loss_test: 0.1104747 G pearson_test: 0.97881967\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 204 次训练 D loss_train: 0.02178439497947693 D acc_train: 50.0 G loss_train: 0.09284515678882599 G pearson_train: 0.9808375835418701\n",
      "第 204 次测试 D loss_test: 0.16663048 D acc_test: 50.0 G loss_test: 0.110490315 G pearson_test: 0.9788201\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 205 次训练 D loss_train: 0.021723438054323196 D acc_train: 50.0 G loss_train: 0.09286068379878998 G pearson_train: 0.9808369278907776\n",
      "第 205 次测试 D loss_test: 0.16706891 D acc_test: 50.0 G loss_test: 0.11051511 G pearson_test: 0.97882044\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 206 次训练 D loss_train: 0.021668413653969765 D acc_train: 50.0 G loss_train: 0.09287165105342865 G pearson_train: 0.9808363914489746\n",
      "第 206 次测试 D loss_test: 0.16753037 D acc_test: 50.0 G loss_test: 0.11053658 G pearson_test: 0.9788208\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 207 次训练 D loss_train: 0.02161945402622223 D acc_train: 50.0 G loss_train: 0.09288087487220764 G pearson_train: 0.980836033821106\n",
      "第 207 次测试 D loss_test: 0.1680263 D acc_test: 50.0 G loss_test: 0.11055257 G pearson_test: 0.97882104\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 208 次训练 D loss_train: 0.02157406136393547 D acc_train: 50.0 G loss_train: 0.09289252758026123 G pearson_train: 0.9808353185653687\n",
      "第 208 次测试 D loss_test: 0.16853006 D acc_test: 50.0 G loss_test: 0.110567786 G pearson_test: 0.97882134\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 209 次训练 D loss_train: 0.021531330421566963 D acc_train: 50.0 G loss_train: 0.09290408343076706 G pearson_train: 0.9808347225189209\n",
      "第 209 次测试 D loss_test: 0.1690513 D acc_test: 50.0 G loss_test: 0.110584825 G pearson_test: 0.9788215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 210 次训练 D loss_train: 0.021491456776857376 D acc_train: 50.0 G loss_train: 0.0929146558046341 G pearson_train: 0.9808340668678284\n",
      "第 210 次测试 D loss_test: 0.1695885 D acc_test: 50.0 G loss_test: 0.11060254 G pearson_test: 0.9788217\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 211 次训练 D loss_train: 0.021455224603414536 D acc_train: 50.0 G loss_train: 0.09292880445718765 G pearson_train: 0.9808332324028015\n",
      "第 211 次测试 D loss_test: 0.17014937 D acc_test: 50.0 G loss_test: 0.11061829 G pearson_test: 0.9788218\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 212 次训练 D loss_train: 0.02142084389925003 D acc_train: 50.0 G loss_train: 0.09293930977582932 G pearson_train: 0.9808325171470642\n",
      "第 212 次测试 D loss_test: 0.17070988 D acc_test: 50.0 G loss_test: 0.110639766 G pearson_test: 0.9788219\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 213 次训练 D loss_train: 0.021391762420535088 D acc_train: 50.0 G loss_train: 0.09294852614402771 G pearson_train: 0.9808316230773926\n",
      "第 213 次测试 D loss_test: 0.1712971 D acc_test: 50.0 G loss_test: 0.110655695 G pearson_test: 0.9788219\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 214 次训练 D loss_train: 0.021365057677030563 D acc_train: 50.0 G loss_train: 0.09296268224716187 G pearson_train: 0.9808306694030762\n",
      "第 214 次测试 D loss_test: 0.17189993 D acc_test: 50.0 G loss_test: 0.11067098 G pearson_test: 0.9788219\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 215 次训练 D loss_train: 0.021340226754546165 D acc_train: 50.0 G loss_train: 0.09297709167003632 G pearson_train: 0.9808297157287598\n",
      "第 215 次测试 D loss_test: 0.17250328 D acc_test: 50.0 G loss_test: 0.110693276 G pearson_test: 0.9788217\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 216 次训练 D loss_train: 0.021318409591913223 D acc_train: 50.0 G loss_train: 0.09298921376466751 G pearson_train: 0.9808287024497986\n",
      "第 216 次测试 D loss_test: 0.1731256 D acc_test: 50.0 G loss_test: 0.1107152 G pearson_test: 0.9788216\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 217 次训练 D loss_train: 0.021299414336681366 D acc_train: 50.0 G loss_train: 0.09299946576356888 G pearson_train: 0.9808276295661926\n",
      "第 217 次测试 D loss_test: 0.1737554 D acc_test: 50.0 G loss_test: 0.11073423 G pearson_test: 0.97882134\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 218 次训练 D loss_train: 0.021284038200974464 D acc_train: 50.0 G loss_train: 0.09301038086414337 G pearson_train: 0.9808265566825867\n",
      "第 218 次测试 D loss_test: 0.17439845 D acc_test: 50.0 G loss_test: 0.11075099 G pearson_test: 0.97882104\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 219 次训练 D loss_train: 0.021271176636219025 D acc_train: 50.0 G loss_train: 0.09302042424678802 G pearson_train: 0.9808254241943359\n",
      "第 219 次测试 D loss_test: 0.17506099 D acc_test: 50.0 G loss_test: 0.110768266 G pearson_test: 0.9788208\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 220 次训练 D loss_train: 0.021260950714349747 D acc_train: 50.0 G loss_train: 0.0930316299200058 G pearson_train: 0.9808241128921509\n",
      "第 220 次测试 D loss_test: 0.17572996 D acc_test: 50.0 G loss_test: 0.1107855 G pearson_test: 0.97882044\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 221 次训练 D loss_train: 0.0212537944316864 D acc_train: 50.0 G loss_train: 0.09304166585206985 G pearson_train: 0.9808229207992554\n",
      "第 221 次测试 D loss_test: 0.17641702 D acc_test: 50.0 G loss_test: 0.11080398 G pearson_test: 0.9788202\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 222 次训练 D loss_train: 0.021248869597911835 D acc_train: 50.0 G loss_train: 0.09305241703987122 G pearson_train: 0.9808215498924255\n",
      "第 222 次测试 D loss_test: 0.17711054 D acc_test: 50.0 G loss_test: 0.110821955 G pearson_test: 0.9788197\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 22ms/step\n",
      "第 223 次训练 D loss_train: 0.021246038377285004 D acc_train: 50.0 G loss_train: 0.09306304156780243 G pearson_train: 0.9808201789855957\n",
      "第 223 次测试 D loss_test: 0.17782098 D acc_test: 50.0 G loss_test: 0.11083944 G pearson_test: 0.9788192\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 224 次训练 D loss_train: 0.021245434880256653 D acc_train: 50.0 G loss_train: 0.09307938814163208 G pearson_train: 0.9808186292648315\n",
      "第 224 次测试 D loss_test: 0.17853212 D acc_test: 50.0 G loss_test: 0.11085895 G pearson_test: 0.97881883\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 225 次训练 D loss_train: 0.021245691925287247 D acc_train: 50.0 G loss_train: 0.0930892750620842 G pearson_train: 0.9808170795440674\n",
      "第 225 次测试 D loss_test: 0.17926145 D acc_test: 50.0 G loss_test: 0.11088421 G pearson_test: 0.978818\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 226 次训练 D loss_train: 0.02124915085732937 D acc_train: 50.0 G loss_train: 0.0930987298488617 G pearson_train: 0.9808155298233032\n",
      "第 226 次测试 D loss_test: 0.18000025 D acc_test: 50.0 G loss_test: 0.110901915 G pearson_test: 0.97881734\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 227 次训练 D loss_train: 0.02125481329858303 D acc_train: 50.0 G loss_train: 0.09311169385910034 G pearson_train: 0.9808139204978943\n",
      "第 227 次测试 D loss_test: 0.18075596 D acc_test: 50.0 G loss_test: 0.11091784 G pearson_test: 0.97881675\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 228 次训练 D loss_train: 0.021260865032672882 D acc_train: 50.0 G loss_train: 0.09312450140714645 G pearson_train: 0.9808122515678406\n",
      "第 228 次测试 D loss_test: 0.18150748 D acc_test: 50.0 G loss_test: 0.11094016 G pearson_test: 0.9788159\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 229 次训练 D loss_train: 0.02126919850707054 D acc_train: 50.0 G loss_train: 0.09313587844371796 G pearson_train: 0.9808105230331421\n",
      "第 229 次测试 D loss_test: 0.1822765 D acc_test: 50.0 G loss_test: 0.11096045 G pearson_test: 0.97881514\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 230 次训练 D loss_train: 0.021279599517583847 D acc_train: 50.0 G loss_train: 0.09314662963151932 G pearson_train: 0.9808086156845093\n",
      "第 230 次测试 D loss_test: 0.18304282 D acc_test: 50.0 G loss_test: 0.1109806 G pearson_test: 0.97881436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 231 次训练 D loss_train: 0.021292567253112793 D acc_train: 50.0 G loss_train: 0.09315966069698334 G pearson_train: 0.9808067083358765\n",
      "第 231 次测试 D loss_test: 0.18384203 D acc_test: 50.0 G loss_test: 0.110999346 G pearson_test: 0.97881347\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 232 次训练 D loss_train: 0.021307192742824554 D acc_train: 50.0 G loss_train: 0.09317301958799362 G pearson_train: 0.9808048009872437\n",
      "第 232 次测试 D loss_test: 0.18463247 D acc_test: 50.0 G loss_test: 0.111021325 G pearson_test: 0.9788126\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 233 次训练 D loss_train: 0.02132266014814377 D acc_train: 50.0 G loss_train: 0.09317922592163086 G pearson_train: 0.9808030724525452\n",
      "第 233 次测试 D loss_test: 0.18545252 D acc_test: 50.0 G loss_test: 0.11104308 G pearson_test: 0.97881156\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 234 次训练 D loss_train: 0.021340740844607353 D acc_train: 50.0 G loss_train: 0.09318824857473373 G pearson_train: 0.9808011651039124\n",
      "第 234 次测试 D loss_test: 0.18629299 D acc_test: 50.0 G loss_test: 0.11105764 G pearson_test: 0.97881067\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 235 次训练 D loss_train: 0.021360822021961212 D acc_train: 50.0 G loss_train: 0.09319676458835602 G pearson_train: 0.9807990193367004\n",
      "第 235 次测试 D loss_test: 0.18714291 D acc_test: 50.0 G loss_test: 0.11107409 G pearson_test: 0.9788097\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 236 次训练 D loss_train: 0.02138204127550125 D acc_train: 50.0 G loss_train: 0.09320579469203949 G pearson_train: 0.9807969927787781\n",
      "第 236 次测试 D loss_test: 0.18800431 D acc_test: 50.0 G loss_test: 0.111090705 G pearson_test: 0.97880876\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 237 次训练 D loss_train: 0.021405145525932312 D acc_train: 50.0 G loss_train: 0.09321637451648712 G pearson_train: 0.9807948470115662\n",
      "第 237 次测试 D loss_test: 0.1888875 D acc_test: 50.0 G loss_test: 0.111107826 G pearson_test: 0.9788076\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 238 次训练 D loss_train: 0.02142951264977455 D acc_train: 50.0 G loss_train: 0.09322547167539597 G pearson_train: 0.9807925224304199\n",
      "第 238 次测试 D loss_test: 0.1897571 D acc_test: 50.0 G loss_test: 0.11112707 G pearson_test: 0.9788064\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 239 次训练 D loss_train: 0.021455861628055573 D acc_train: 50.0 G loss_train: 0.09323438256978989 G pearson_train: 0.9807903170585632\n",
      "第 239 次测试 D loss_test: 0.1906679 D acc_test: 50.0 G loss_test: 0.11114354 G pearson_test: 0.9788053\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 240 次训练 D loss_train: 0.021483436226844788 D acc_train: 50.0 G loss_train: 0.0932459905743599 G pearson_train: 0.980787992477417\n",
      "第 240 次测试 D loss_test: 0.191563 D acc_test: 50.0 G loss_test: 0.11116129 G pearson_test: 0.97880393\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 241 次训练 D loss_train: 0.021510951220989227 D acc_train: 50.0 G loss_train: 0.09325575828552246 G pearson_train: 0.9807856678962708\n",
      "第 241 次测试 D loss_test: 0.19249552 D acc_test: 50.0 G loss_test: 0.11118112 G pearson_test: 0.97880274\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 242 次训练 D loss_train: 0.021539602428674698 D acc_train: 50.0 G loss_train: 0.09327170252799988 G pearson_train: 0.9807829856872559\n",
      "第 242 次测试 D loss_test: 0.19342035 D acc_test: 50.0 G loss_test: 0.11119913 G pearson_test: 0.9788015\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 243 次训练 D loss_train: 0.021567881107330322 D acc_train: 50.0 G loss_train: 0.09328524023294449 G pearson_train: 0.9807804822921753\n",
      "第 243 次测试 D loss_test: 0.19436578 D acc_test: 50.0 G loss_test: 0.111224554 G pearson_test: 0.9788001\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 244 次训练 D loss_train: 0.021598361432552338 D acc_train: 50.0 G loss_train: 0.0932975560426712 G pearson_train: 0.98077791929245\n",
      "第 244 次测试 D loss_test: 0.19531251 D acc_test: 50.0 G loss_test: 0.11124789 G pearson_test: 0.97879875\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 245 次训练 D loss_train: 0.02163080871105194 D acc_train: 50.0 G loss_train: 0.09330649673938751 G pearson_train: 0.9807755351066589\n",
      "第 245 次测试 D loss_test: 0.19629975 D acc_test: 50.0 G loss_test: 0.111270145 G pearson_test: 0.9787973\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 246 次训练 D loss_train: 0.021665778011083603 D acc_train: 50.0 G loss_train: 0.09331571310758591 G pearson_train: 0.9807729125022888\n",
      "第 246 次测试 D loss_test: 0.19727267 D acc_test: 50.0 G loss_test: 0.111286774 G pearson_test: 0.978796\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 247 次训练 D loss_train: 0.021701756864786148 D acc_train: 50.0 G loss_train: 0.09332578629255295 G pearson_train: 0.9807703495025635\n",
      "第 247 次测试 D loss_test: 0.19827497 D acc_test: 50.0 G loss_test: 0.11130449 G pearson_test: 0.97879446\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 248 次训练 D loss_train: 0.021739773452281952 D acc_train: 50.0 G loss_train: 0.09333787113428116 G pearson_train: 0.9807676076889038\n",
      "第 248 次测试 D loss_test: 0.19928524 D acc_test: 50.0 G loss_test: 0.111323714 G pearson_test: 0.97879297\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 249 次训练 D loss_train: 0.021777212619781494 D acc_train: 50.0 G loss_train: 0.09334859997034073 G pearson_train: 0.9807649850845337\n",
      "第 249 次测试 D loss_test: 0.20028704 D acc_test: 50.0 G loss_test: 0.111345395 G pearson_test: 0.9787914\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 250 次训练 D loss_train: 0.02181667648255825 D acc_train: 50.0 G loss_train: 0.09335752576589584 G pearson_train: 0.9807623028755188\n",
      "第 250 次测试 D loss_test: 0.20134121 D acc_test: 50.0 G loss_test: 0.11136412 G pearson_test: 0.9787898\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 251 次训练 D loss_train: 0.021857600659132004 D acc_train: 50.0 G loss_train: 0.09336753934621811 G pearson_train: 0.9807595610618591\n",
      "第 251 次测试 D loss_test: 0.20234975 D acc_test: 50.0 G loss_test: 0.11138288 G pearson_test: 0.9787882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 252 次训练 D loss_train: 0.021900512278079987 D acc_train: 50.0 G loss_train: 0.0933758094906807 G pearson_train: 0.9807568192481995\n",
      "第 252 次测试 D loss_test: 0.20343608 D acc_test: 50.0 G loss_test: 0.11140102 G pearson_test: 0.97878665\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 253 次训练 D loss_train: 0.021945219486951828 D acc_train: 50.0 G loss_train: 0.09338469058275223 G pearson_train: 0.980754017829895\n",
      "第 253 次测试 D loss_test: 0.20445287 D acc_test: 50.0 G loss_test: 0.111418344 G pearson_test: 0.97878504\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 254 次训练 D loss_train: 0.021992435678839684 D acc_train: 50.0 G loss_train: 0.09339265525341034 G pearson_train: 0.9807513356208801\n",
      "第 254 次测试 D loss_test: 0.20559174 D acc_test: 50.0 G loss_test: 0.11143459 G pearson_test: 0.97878337\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 255 次训练 D loss_train: 0.022040270268917084 D acc_train: 50.0 G loss_train: 0.09340343624353409 G pearson_train: 0.9807483553886414\n",
      "第 255 次测试 D loss_test: 0.20661032 D acc_test: 50.0 G loss_test: 0.111453205 G pearson_test: 0.97878164\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 256 次训练 D loss_train: 0.022089876234531403 D acc_train: 50.0 G loss_train: 0.09341040253639221 G pearson_train: 0.9807455539703369\n",
      "第 256 次测试 D loss_test: 0.20778535 D acc_test: 50.0 G loss_test: 0.11147076 G pearson_test: 0.97878\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 257 次训练 D loss_train: 0.02214076742529869 D acc_train: 50.0 G loss_train: 0.09342040121555328 G pearson_train: 0.9807425141334534\n",
      "第 257 次测试 D loss_test: 0.20876746 D acc_test: 50.0 G loss_test: 0.11148987 G pearson_test: 0.97877824\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 258 次训练 D loss_train: 0.022195182740688324 D acc_train: 50.0 G loss_train: 0.09342940151691437 G pearson_train: 0.980739414691925\n",
      "第 258 次测试 D loss_test: 0.21007173 D acc_test: 50.0 G loss_test: 0.111505054 G pearson_test: 0.9787766\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 259 次训练 D loss_train: 0.022248409688472748 D acc_train: 50.0 G loss_train: 0.09344509989023209 G pearson_train: 0.9807363152503967\n",
      "第 259 次测试 D loss_test: 0.21100336 D acc_test: 50.0 G loss_test: 0.11152797 G pearson_test: 0.9787747\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 260 次训练 D loss_train: 0.022305727005004883 D acc_train: 50.0 G loss_train: 0.09345085918903351 G pearson_train: 0.9807333946228027\n",
      "第 260 次测试 D loss_test: 0.21242529 D acc_test: 50.0 G loss_test: 0.11154741 G pearson_test: 0.9787728\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 21ms/step\n",
      "第 261 次训练 D loss_train: 0.022362887859344482 D acc_train: 50.0 G loss_train: 0.09346754848957062 G pearson_train: 0.9807299971580505\n",
      "第 261 次测试 D loss_test: 0.21319598 D acc_test: 50.0 G loss_test: 0.11156999 G pearson_test: 0.97877073\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 262 次训练 D loss_train: 0.022426683455705643 D acc_train: 50.0 G loss_train: 0.09347173571586609 G pearson_train: 0.9807270765304565\n",
      "第 262 次测试 D loss_test: 0.21494004 D acc_test: 50.0 G loss_test: 0.11158504 G pearson_test: 0.9787688\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 263 次训练 D loss_train: 0.022487010806798935 D acc_train: 50.0 G loss_train: 0.0934947282075882 G pearson_train: 0.9807234406471252\n",
      "第 263 次测试 D loss_test: 0.21537083 D acc_test: 50.0 G loss_test: 0.11161364 G pearson_test: 0.9787667\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 264 次训练 D loss_train: 0.02256113290786743 D acc_train: 50.0 G loss_train: 0.09349128603935242 G pearson_train: 0.9807207584381104\n",
      "第 264 次测试 D loss_test: 0.21768376 D acc_test: 50.0 G loss_test: 0.111625634 G pearson_test: 0.9787646\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 265 次训练 D loss_train: 0.02262960374355316 D acc_train: 50.0 G loss_train: 0.09352069348096848 G pearson_train: 0.9807167649269104\n",
      "第 265 次测试 D loss_test: 0.21738115 D acc_test: 50.0 G loss_test: 0.11165872 G pearson_test: 0.9787625\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 266 次训练 D loss_train: 0.022735698148608208 D acc_train: 50.0 G loss_train: 0.0935005396604538 G pearson_train: 0.9807145595550537\n",
      "第 266 次测试 D loss_test: 0.22085905 D acc_test: 50.0 G loss_test: 0.11166002 G pearson_test: 0.9787605\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 267 次训练 D loss_train: 0.022840537130832672 D acc_train: 50.0 G loss_train: 0.09354496002197266 G pearson_train: 0.9807097911834717\n",
      "第 267 次测试 D loss_test: 0.21907651 D acc_test: 50.0 G loss_test: 0.11170006 G pearson_test: 0.9787582\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 268 次训练 D loss_train: 0.0230608731508255 D acc_train: 50.0 G loss_train: 0.09349962323904037 G pearson_train: 0.9807087779045105\n",
      "第 268 次测试 D loss_test: 0.22525615 D acc_test: 50.0 G loss_test: 0.11168145 G pearson_test: 0.9787561\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 269 次训练 D loss_train: 0.023299839347600937 D acc_train: 50.0 G loss_train: 0.09359128028154373 G pearson_train: 0.9807019829750061\n",
      "第 269 次测试 D loss_test: 0.22043681 D acc_test: 50.0 G loss_test: 0.11174644 G pearson_test: 0.9787537\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 270 次训练 D loss_train: 0.023885833099484444 D acc_train: 50.0 G loss_train: 0.09350346028804779 G pearson_train: 0.9807036519050598\n",
      "第 270 次测试 D loss_test: 0.2327423 D acc_test: 50.0 G loss_test: 0.11171653 G pearson_test: 0.97875166\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 271 次训练 D loss_train: 0.024635814130306244 D acc_train: 50.0 G loss_train: 0.09367892146110535 G pearson_train: 0.9806930422782898\n",
      "第 271 次测试 D loss_test: 0.22228323 D acc_test: 50.0 G loss_test: 0.11182953 G pearson_test: 0.9787488\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 272 次训练 D loss_train: 0.02648177370429039 D acc_train: 50.0 G loss_train: 0.09352133423089981 G pearson_train: 0.9807008504867554\n",
      "第 272 次测试 D loss_test: 0.25137278 D acc_test: 50.0 G loss_test: 0.11177812 G pearson_test: 0.9787474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 273 次训练 D loss_train: 0.029909007251262665 D acc_train: 50.0 G loss_train: 0.09393759071826935 G pearson_train: 0.9806813597679138\n",
      "第 273 次测试 D loss_test: 0.23432775 D acc_test: 50.0 G loss_test: 0.11204541 G pearson_test: 0.9787433\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 274 次训练 D loss_train: 0.040240444242954254 D acc_train: 50.0 G loss_train: 0.09388401359319687 G pearson_train: 0.980711817741394\n",
      "第 274 次测试 D loss_test: 0.3685925 D acc_test: 50.0 G loss_test: 0.112171076 G pearson_test: 0.9787442\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 275 次训练 D loss_train: 0.08544567227363586 D acc_train: 50.0 G loss_train: 0.09782332926988602 G pearson_train: 0.9806420803070068\n",
      "第 275 次测试 D loss_test: 0.7981006 D acc_test: 52.17391304347826 G loss_test: 0.11557127 G pearson_test: 0.9787361\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 276 次训练 D loss_train: 0.6150779724121094 D acc_train: 52.930402755737305 G loss_train: 0.10631007701158524 G pearson_train: 0.9807916879653931\n",
      "第 276 次测试 D loss_test: 3.1193042 D acc_test: 52.17391304347826 G loss_test: 0.12248668 G pearson_test: 0.97874933\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 277 次训练 D loss_train: 3.1580471992492676 D acc_train: 53.113555908203125 G loss_train: 0.09464967250823975 G pearson_train: 0.9807181358337402\n",
      "第 277 次测试 D loss_test: 0.8512411 D acc_test: 53.2608695652174 G loss_test: 0.114208125 G pearson_test: 0.97871375\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 278 次训练 D loss_train: 0.7597451210021973 D acc_train: 58.974361419677734 G loss_train: 0.08925464004278183 G pearson_train: 0.9807885885238647\n",
      "第 278 次测试 D loss_test: 0.38157713 D acc_test: 59.78260869565217 G loss_test: 0.1077451 G pearson_test: 0.9787789\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 279 次训练 D loss_train: 0.22402942180633545 D acc_train: 58.05860757827759 G loss_train: 0.08920766413211823 G pearson_train: 0.9807824492454529\n",
      "第 279 次测试 D loss_test: 0.3299412 D acc_test: 56.52173913043478 G loss_test: 0.107174315 G pearson_test: 0.9787819\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 280 次训练 D loss_train: 0.1426997184753418 D acc_train: 54.212456941604614 G loss_train: 0.08906891942024231 G pearson_train: 0.9807916879653931\n",
      "第 280 次测试 D loss_test: 0.30951214 D acc_test: 55.434782608695656 G loss_test: 0.106816046 G pearson_test: 0.97879326\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 281 次训练 D loss_train: 0.11224545538425446 D acc_train: 52.014654874801636 G loss_train: 0.08928781002759933 G pearson_train: 0.9807904958724976\n",
      "第 281 次测试 D loss_test: 0.30128768 D acc_test: 53.2608695652174 G loss_test: 0.10697083 G pearson_test: 0.9787928\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 282 次训练 D loss_train: 0.0971786230802536 D acc_train: 52.19780206680298 G loss_train: 0.0894457995891571 G pearson_train: 0.9807908535003662\n",
      "第 282 次测试 D loss_test: 0.29385048 D acc_test: 53.2608695652174 G loss_test: 0.10713394 G pearson_test: 0.9787941\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 283 次训练 D loss_train: 0.0871039405465126 D acc_train: 51.64835453033447 G loss_train: 0.08961643278598785 G pearson_train: 0.9807902574539185\n",
      "第 283 次测试 D loss_test: 0.2881832 D acc_test: 52.17391304347826 G loss_test: 0.107333645 G pearson_test: 0.9787943\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 284 次训练 D loss_train: 0.0791580080986023 D acc_train: 50.91575384140015 G loss_train: 0.08978155255317688 G pearson_train: 0.9807893633842468\n",
      "第 284 次测试 D loss_test: 0.28152966 D acc_test: 52.17391304347826 G loss_test: 0.10750592 G pearson_test: 0.97879493\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 285 次训练 D loss_train: 0.07314935326576233 D acc_train: 50.91575384140015 G loss_train: 0.08992022275924683 G pearson_train: 0.9807888269424438\n",
      "第 285 次测试 D loss_test: 0.2763943 D acc_test: 51.63043478260869 G loss_test: 0.10770663 G pearson_test: 0.9787949\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 286 次训练 D loss_train: 0.06815390288829803 D acc_train: 50.732600688934326 G loss_train: 0.09002958238124847 G pearson_train: 0.9807886481285095\n",
      "第 286 次测试 D loss_test: 0.27057815 D acc_test: 51.08695652173913 G loss_test: 0.10785616 G pearson_test: 0.9787953\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 287 次训练 D loss_train: 0.06382126361131668 D acc_train: 50.54945349693298 G loss_train: 0.09014693647623062 G pearson_train: 0.9807881116867065\n",
      "第 287 次测试 D loss_test: 0.26646745 D acc_test: 51.08695652173913 G loss_test: 0.107971884 G pearson_test: 0.97879547\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 288 次训练 D loss_train: 0.05979328230023384 D acc_train: 50.54945349693298 G loss_train: 0.09025095403194427 G pearson_train: 0.9807875156402588\n",
      "第 288 次测试 D loss_test: 0.26149493 D acc_test: 51.08695652173913 G loss_test: 0.10809958 G pearson_test: 0.97879565\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 289 次训练 D loss_train: 0.056080691516399384 D acc_train: 50.54945349693298 G loss_train: 0.09036897122859955 G pearson_train: 0.9807862639427185\n",
      "第 289 次测试 D loss_test: 0.2577217 D acc_test: 51.08695652173913 G loss_test: 0.10820704 G pearson_test: 0.9787953\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 290 次训练 D loss_train: 0.052613791078329086 D acc_train: 50.36630034446716 G loss_train: 0.09050574898719788 G pearson_train: 0.9807844758033752\n",
      "第 290 次测试 D loss_test: 0.25391468 D acc_test: 51.08695652173913 G loss_test: 0.10832298 G pearson_test: 0.9787949\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 291 次训练 D loss_train: 0.0495002344250679 D acc_train: 50.36630034446716 G loss_train: 0.09062063694000244 G pearson_train: 0.980782687664032\n",
      "第 291 次测试 D loss_test: 0.25090227 D acc_test: 51.08695652173913 G loss_test: 0.1084618 G pearson_test: 0.9787938\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 292 次训练 D loss_train: 0.0472397543489933 D acc_train: 50.36630034446716 G loss_train: 0.09070558100938797 G pearson_train: 0.980780839920044\n",
      "第 292 次测试 D loss_test: 0.24856208 D acc_test: 51.08695652173913 G loss_test: 0.10856878 G pearson_test: 0.97879255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 293 次训练 D loss_train: 0.04535664618015289 D acc_train: 50.18315315246582 G loss_train: 0.09079156070947647 G pearson_train: 0.9807788133621216\n",
      "第 293 次测试 D loss_test: 0.2466038 D acc_test: 51.08695652173913 G loss_test: 0.10867939 G pearson_test: 0.9787914\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 294 次训练 D loss_train: 0.04371873289346695 D acc_train: 50.18315315246582 G loss_train: 0.09086710959672928 G pearson_train: 0.980776846408844\n",
      "第 294 次测试 D loss_test: 0.24481784 D acc_test: 51.08695652173913 G loss_test: 0.108773954 G pearson_test: 0.9787902\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 295 次训练 D loss_train: 0.04227573424577713 D acc_train: 50.18315315246582 G loss_train: 0.09094257652759552 G pearson_train: 0.9807746410369873\n",
      "第 295 次测试 D loss_test: 0.24338835 D acc_test: 51.08695652173913 G loss_test: 0.10886129 G pearson_test: 0.97878915\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 296 次训练 D loss_train: 0.04099947214126587 D acc_train: 50.18315315246582 G loss_train: 0.09101410210132599 G pearson_train: 0.9807723760604858\n",
      "第 296 次测试 D loss_test: 0.24206612 D acc_test: 50.54347826086957 G loss_test: 0.108949944 G pearson_test: 0.97878784\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 297 次训练 D loss_train: 0.039839666336774826 D acc_train: 50.18315315246582 G loss_train: 0.09109050780534744 G pearson_train: 0.98076993227005\n",
      "第 297 次测试 D loss_test: 0.24077591 D acc_test: 50.54347826086957 G loss_test: 0.10903947 G pearson_test: 0.9787867\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 298 次训练 D loss_train: 0.03881401568651199 D acc_train: 50.18315315246582 G loss_train: 0.0911434143781662 G pearson_train: 0.9807678461074829\n",
      "第 298 次测试 D loss_test: 0.23969686 D acc_test: 50.54347826086957 G loss_test: 0.10912254 G pearson_test: 0.9787854\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 299 次训练 D loss_train: 0.037894245237112045 D acc_train: 50.18315315246582 G loss_train: 0.09120257198810577 G pearson_train: 0.9807655811309814\n",
      "第 299 次测试 D loss_test: 0.23865941 D acc_test: 50.54347826086957 G loss_test: 0.109188214 G pearson_test: 0.9787843\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 300 次训练 D loss_train: 0.03704317659139633 D acc_train: 50.18315315246582 G loss_train: 0.09123064577579498 G pearson_train: 0.980763852596283\n",
      "第 300 次测试 D loss_test: 0.23793638 D acc_test: 50.54347826086957 G loss_test: 0.109248385 G pearson_test: 0.97878295\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 301 次训练 D loss_train: 0.03621571511030197 D acc_train: 50.18315315246582 G loss_train: 0.0912831574678421 G pearson_train: 0.980761706829071\n",
      "第 301 次测试 D loss_test: 0.23705493 D acc_test: 50.54347826086957 G loss_test: 0.10929147 G pearson_test: 0.97878206\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 302 次训练 D loss_train: 0.03542980179190636 D acc_train: 50.18315315246582 G loss_train: 0.09133932739496231 G pearson_train: 0.9807590246200562\n",
      "第 302 次测试 D loss_test: 0.23676294 D acc_test: 50.54347826086957 G loss_test: 0.10934025 G pearson_test: 0.9787805\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 303 次训练 D loss_train: 0.03456348180770874 D acc_train: 50.18315315246582 G loss_train: 0.0914059653878212 G pearson_train: 0.9807565212249756\n",
      "第 303 次测试 D loss_test: 0.23608592 D acc_test: 50.54347826086957 G loss_test: 0.109428026 G pearson_test: 0.9787788\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 304 次训练 D loss_train: 0.03373964875936508 D acc_train: 50.18315315246582 G loss_train: 0.09146937727928162 G pearson_train: 0.9807538390159607\n",
      "第 304 次测试 D loss_test: 0.23584351 D acc_test: 50.54347826086957 G loss_test: 0.10949021 G pearson_test: 0.978777\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 305 次训练 D loss_train: 0.03283829987049103 D acc_train: 50.18315315246582 G loss_train: 0.0915464237332344 G pearson_train: 0.9807510375976562\n",
      "第 305 次测试 D loss_test: 0.23515745 D acc_test: 50.54347826086957 G loss_test: 0.1095797 G pearson_test: 0.9787751\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 306 次训练 D loss_train: 0.031963713467121124 D acc_train: 50.18315315246582 G loss_train: 0.09161163866519928 G pearson_train: 0.9807483553886414\n",
      "第 306 次测试 D loss_test: 0.23487078 D acc_test: 50.0 G loss_test: 0.109650925 G pearson_test: 0.97877294\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 307 次训练 D loss_train: 0.03102850914001465 D acc_train: 50.18315315246582 G loss_train: 0.0916956290602684 G pearson_train: 0.9807453751564026\n",
      "第 307 次测试 D loss_test: 0.23393506 D acc_test: 50.0 G loss_test: 0.109748065 G pearson_test: 0.97877073\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 308 次训练 D loss_train: 0.03018108755350113 D acc_train: 50.18315315246582 G loss_train: 0.0917855054140091 G pearson_train: 0.9807420372962952\n",
      "第 308 次测试 D loss_test: 0.2332098 D acc_test: 50.0 G loss_test: 0.109831534 G pearson_test: 0.9787683\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 309 次训练 D loss_train: 0.029395902529358864 D acc_train: 50.18315315246582 G loss_train: 0.09188523143529892 G pearson_train: 0.9807382822036743\n",
      "第 309 次测试 D loss_test: 0.23171498 D acc_test: 50.0 G loss_test: 0.10995475 G pearson_test: 0.9787657\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 310 次训练 D loss_train: 0.028801700100302696 D acc_train: 50.18315315246582 G loss_train: 0.09197738021612167 G pearson_train: 0.980734646320343\n",
      "第 310 次测试 D loss_test: 0.23038527 D acc_test: 50.0 G loss_test: 0.11006571 G pearson_test: 0.9787631\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 311 次训练 D loss_train: 0.028218304738402367 D acc_train: 50.18315315246582 G loss_train: 0.09206223487854004 G pearson_train: 0.9807310700416565\n",
      "第 311 次测试 D loss_test: 0.22867255 D acc_test: 50.0 G loss_test: 0.1101902 G pearson_test: 0.9787603\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 312 次训练 D loss_train: 0.027702301740646362 D acc_train: 50.18315315246582 G loss_train: 0.09213831275701523 G pearson_train: 0.9807283282279968\n",
      "第 312 次测试 D loss_test: 0.22714277 D acc_test: 50.0 G loss_test: 0.11029107 G pearson_test: 0.9787576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 313 次训练 D loss_train: 0.027064809575676918 D acc_train: 50.18315315246582 G loss_train: 0.09223538637161255 G pearson_train: 0.980725109577179\n",
      "第 313 次测试 D loss_test: 0.22555679 D acc_test: 50.0 G loss_test: 0.110380635 G pearson_test: 0.9787546\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 314 次训练 D loss_train: 0.026451602578163147 D acc_train: 50.18315315246582 G loss_train: 0.09232401102781296 G pearson_train: 0.9807220697402954\n",
      "第 314 次测试 D loss_test: 0.2239362 D acc_test: 50.0 G loss_test: 0.11046988 G pearson_test: 0.97875124\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 315 次训练 D loss_train: 0.02581121399998665 D acc_train: 50.0 G loss_train: 0.09245017915964127 G pearson_train: 0.9807175993919373\n",
      "第 315 次测试 D loss_test: 0.22149552 D acc_test: 50.0 G loss_test: 0.110577516 G pearson_test: 0.9787477\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 316 次训练 D loss_train: 0.02523936703801155 D acc_train: 50.0 G loss_train: 0.09256217628717422 G pearson_train: 0.9807132482528687\n",
      "第 316 次测试 D loss_test: 0.21951127 D acc_test: 50.0 G loss_test: 0.11071751 G pearson_test: 0.9787436\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 317 次训练 D loss_train: 0.024639897048473358 D acc_train: 50.0 G loss_train: 0.09268800914287567 G pearson_train: 0.9807083010673523\n",
      "第 317 次测试 D loss_test: 0.21771938 D acc_test: 50.0 G loss_test: 0.11084411 G pearson_test: 0.97873956\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 318 次训练 D loss_train: 0.024187207221984863 D acc_train: 50.0 G loss_train: 0.09277305752038956 G pearson_train: 0.9807033538818359\n",
      "第 318 次测试 D loss_test: 0.21693993 D acc_test: 50.0 G loss_test: 0.110957205 G pearson_test: 0.9787352\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 319 次训练 D loss_train: 0.023821529000997543 D acc_train: 50.0 G loss_train: 0.09289427846670151 G pearson_train: 0.9806975722312927\n",
      "第 319 次测试 D loss_test: 0.21567576 D acc_test: 50.0 G loss_test: 0.11105901 G pearson_test: 0.9787315\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 320 次训练 D loss_train: 0.023541871458292007 D acc_train: 50.0 G loss_train: 0.09298156201839447 G pearson_train: 0.9806921482086182\n",
      "第 320 次测试 D loss_test: 0.21498078 D acc_test: 50.0 G loss_test: 0.111192964 G pearson_test: 0.978727\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 321 次训练 D loss_train: 0.02332368865609169 D acc_train: 50.0 G loss_train: 0.09305764734745026 G pearson_train: 0.9806870222091675\n",
      "第 321 次测试 D loss_test: 0.21448909 D acc_test: 50.0 G loss_test: 0.1112981 G pearson_test: 0.978723\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 322 次训练 D loss_train: 0.023164812475442886 D acc_train: 50.0 G loss_train: 0.0931299552321434 G pearson_train: 0.9806815981864929\n",
      "第 322 次测试 D loss_test: 0.2145597 D acc_test: 50.0 G loss_test: 0.11137212 G pearson_test: 0.9787191\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 323 次训练 D loss_train: 0.023032229393720627 D acc_train: 50.0 G loss_train: 0.09320235997438431 G pearson_train: 0.9806761145591736\n",
      "第 323 次测试 D loss_test: 0.21463391 D acc_test: 50.0 G loss_test: 0.11145293 G pearson_test: 0.9787153\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 324 次训练 D loss_train: 0.022952144965529442 D acc_train: 50.0 G loss_train: 0.09325682371854782 G pearson_train: 0.9806711077690125\n",
      "第 324 次测试 D loss_test: 0.21511225 D acc_test: 50.0 G loss_test: 0.11152801 G pearson_test: 0.97871137\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 325 次训练 D loss_train: 0.022908862680196762 D acc_train: 50.0 G loss_train: 0.09331325441598892 G pearson_train: 0.9806657433509827\n",
      "第 325 次测试 D loss_test: 0.21546336 D acc_test: 50.0 G loss_test: 0.11159541 G pearson_test: 0.9787077\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 326 次训练 D loss_train: 0.02289738692343235 D acc_train: 50.0 G loss_train: 0.09336034208536148 G pearson_train: 0.9806603789329529\n",
      "第 326 次测试 D loss_test: 0.21599415 D acc_test: 50.0 G loss_test: 0.11166206 G pearson_test: 0.9787036\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 327 次训练 D loss_train: 0.022890260443091393 D acc_train: 50.0 G loss_train: 0.09341608732938766 G pearson_train: 0.9806548953056335\n",
      "第 327 次测试 D loss_test: 0.21641503 D acc_test: 50.0 G loss_test: 0.11172498 G pearson_test: 0.9786999\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 328 次训练 D loss_train: 0.02289479970932007 D acc_train: 50.0 G loss_train: 0.09346373379230499 G pearson_train: 0.9806495904922485\n",
      "第 328 次测试 D loss_test: 0.21702334 D acc_test: 50.0 G loss_test: 0.111789584 G pearson_test: 0.978696\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 329 次训练 D loss_train: 0.02290104329586029 D acc_train: 50.0 G loss_train: 0.09351399540901184 G pearson_train: 0.9806442856788635\n",
      "第 329 次测试 D loss_test: 0.2175507 D acc_test: 50.0 G loss_test: 0.11185583 G pearson_test: 0.9786921\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 330 次训练 D loss_train: 0.022924775257706642 D acc_train: 50.0 G loss_train: 0.09355443716049194 G pearson_train: 0.9806390404701233\n",
      "第 330 次测试 D loss_test: 0.21829392 D acc_test: 50.0 G loss_test: 0.111916326 G pearson_test: 0.978688\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 331 次训练 D loss_train: 0.02294793911278248 D acc_train: 50.0 G loss_train: 0.09360281378030777 G pearson_train: 0.9806336164474487\n",
      "第 331 次测试 D loss_test: 0.2189174 D acc_test: 50.0 G loss_test: 0.11197219 G pearson_test: 0.97868425\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 332 次训练 D loss_train: 0.02297521010041237 D acc_train: 50.0 G loss_train: 0.09363836795091629 G pearson_train: 0.980628252029419\n",
      "第 332 次测试 D loss_test: 0.21986935 D acc_test: 50.0 G loss_test: 0.11202793 G pearson_test: 0.97868025\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 333 次训练 D loss_train: 0.023026473820209503 D acc_train: 50.0 G loss_train: 0.0936766266822815 G pearson_train: 0.9806230664253235\n",
      "第 333 次测试 D loss_test: 0.22078785 D acc_test: 50.0 G loss_test: 0.11207171 G pearson_test: 0.97867644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 334 次训练 D loss_train: 0.023087047040462494 D acc_train: 50.0 G loss_train: 0.09371499717235565 G pearson_train: 0.9806177616119385\n",
      "第 334 次测试 D loss_test: 0.22197935 D acc_test: 50.0 G loss_test: 0.112114616 G pearson_test: 0.9786724\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 335 次训练 D loss_train: 0.023162607103586197 D acc_train: 50.0 G loss_train: 0.09376107156276703 G pearson_train: 0.9806122779846191\n",
      "第 335 次测试 D loss_test: 0.22281922 D acc_test: 50.0 G loss_test: 0.1121689 G pearson_test: 0.97866845\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 336 次训练 D loss_train: 0.02326466515660286 D acc_train: 50.0 G loss_train: 0.09380146116018295 G pearson_train: 0.9806067943572998\n",
      "第 336 次测试 D loss_test: 0.22392912 D acc_test: 50.0 G loss_test: 0.11221927 G pearson_test: 0.97866416\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 337 次训练 D loss_train: 0.023351134732365608 D acc_train: 50.0 G loss_train: 0.09385689347982407 G pearson_train: 0.9806011915206909\n",
      "第 337 次测试 D loss_test: 0.2246671 D acc_test: 50.0 G loss_test: 0.112277776 G pearson_test: 0.9786601\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 338 次训练 D loss_train: 0.0234703216701746 D acc_train: 50.0 G loss_train: 0.09389519691467285 G pearson_train: 0.9805957078933716\n",
      "第 338 次测试 D loss_test: 0.22591376 D acc_test: 50.0 G loss_test: 0.11233411 G pearson_test: 0.97865564\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 339 次训练 D loss_train: 0.02354854717850685 D acc_train: 50.0 G loss_train: 0.09394285082817078 G pearson_train: 0.9805902242660522\n",
      "第 339 次测试 D loss_test: 0.22663337 D acc_test: 50.0 G loss_test: 0.11239487 G pearson_test: 0.9786516\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 340 次训练 D loss_train: 0.023663368076086044 D acc_train: 50.0 G loss_train: 0.09396415203809738 G pearson_train: 0.9805848002433777\n",
      "第 340 次测试 D loss_test: 0.22807947 D acc_test: 50.0 G loss_test: 0.112438664 G pearson_test: 0.97864705\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 341 次训练 D loss_train: 0.023722849786281586 D acc_train: 50.0 G loss_train: 0.09402856230735779 G pearson_train: 0.9805783629417419\n",
      "第 341 次测试 D loss_test: 0.22862424 D acc_test: 50.0 G loss_test: 0.11248571 G pearson_test: 0.97864366\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 342 次训练 D loss_train: 0.023856254294514656 D acc_train: 50.0 G loss_train: 0.09405360370874405 G pearson_train: 0.9805730581283569\n",
      "第 342 次测试 D loss_test: 0.23043582 D acc_test: 50.0 G loss_test: 0.11253176 G pearson_test: 0.97863907\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 343 次训练 D loss_train: 0.023902034386992455 D acc_train: 50.0 G loss_train: 0.09411212056875229 G pearson_train: 0.9805667400360107\n",
      "第 343 次测试 D loss_test: 0.23068213 D acc_test: 50.0 G loss_test: 0.112601765 G pearson_test: 0.97863543\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 344 次训练 D loss_train: 0.024058258160948753 D acc_train: 50.0 G loss_train: 0.09411854296922684 G pearson_train: 0.9805616140365601\n",
      "第 344 次测试 D loss_test: 0.232715 D acc_test: 50.0 G loss_test: 0.11263394 G pearson_test: 0.97863096\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 345 次训练 D loss_train: 0.02404693514108658 D acc_train: 50.0 G loss_train: 0.09417958557605743 G pearson_train: 0.9805549383163452\n",
      "第 345 次测试 D loss_test: 0.23279727 D acc_test: 50.0 G loss_test: 0.11269006 G pearson_test: 0.9786277\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 346 次训练 D loss_train: 0.024290326982736588 D acc_train: 50.0 G loss_train: 0.09417574107646942 G pearson_train: 0.9805500507354736\n",
      "第 346 次测试 D loss_test: 0.23527406 D acc_test: 50.0 G loss_test: 0.11270617 G pearson_test: 0.97862345\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 347 次训练 D loss_train: 0.02431357093155384 D acc_train: 50.0 G loss_train: 0.09423408657312393 G pearson_train: 0.9805434942245483\n",
      "第 347 次测试 D loss_test: 0.23474005 D acc_test: 50.0 G loss_test: 0.11276778 G pearson_test: 0.97862023\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 348 次训练 D loss_train: 0.024660691618919373 D acc_train: 50.0 G loss_train: 0.09421581029891968 G pearson_train: 0.9805392622947693\n",
      "第 348 次测试 D loss_test: 0.23786584 D acc_test: 50.0 G loss_test: 0.11276791 G pearson_test: 0.9786157\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 349 次训练 D loss_train: 0.02470937930047512 D acc_train: 50.0 G loss_train: 0.09429917484521866 G pearson_train: 0.9805311560630798\n",
      "第 349 次测试 D loss_test: 0.23703891 D acc_test: 50.0 G loss_test: 0.11283057 G pearson_test: 0.9786127\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 350 次训练 D loss_train: 0.02521606720983982 D acc_train: 50.0 G loss_train: 0.0942729115486145 G pearson_train: 0.9805276393890381\n",
      "第 350 次测试 D loss_test: 0.24090822 D acc_test: 50.0 G loss_test: 0.11284187 G pearson_test: 0.97860837\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 351 次训练 D loss_train: 0.02523651532828808 D acc_train: 50.0 G loss_train: 0.0943470448255539 G pearson_train: 0.9805192351341248\n",
      "第 351 次测试 D loss_test: 0.2394055 D acc_test: 50.0 G loss_test: 0.1129186 G pearson_test: 0.9786053\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 352 次训练 D loss_train: 0.025954492390155792 D acc_train: 50.0 G loss_train: 0.09430745989084244 G pearson_train: 0.9805171489715576\n",
      "第 352 次测试 D loss_test: 0.2443852 D acc_test: 50.0 G loss_test: 0.11289563 G pearson_test: 0.97860146\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 353 次训练 D loss_train: 0.025840606540441513 D acc_train: 50.0 G loss_train: 0.09440018981695175 G pearson_train: 0.9805071353912354\n",
      "第 353 次测试 D loss_test: 0.24195887 D acc_test: 50.0 G loss_test: 0.11297791 G pearson_test: 0.9785979\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 354 次训练 D loss_train: 0.026707256212830544 D acc_train: 50.0 G loss_train: 0.09434369206428528 G pearson_train: 0.9805066585540771\n",
      "第 354 次测试 D loss_test: 0.24783367 D acc_test: 50.0 G loss_test: 0.1129543 G pearson_test: 0.97859454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 355 次训练 D loss_train: 0.026511799544095993 D acc_train: 50.0 G loss_train: 0.09444200247526169 G pearson_train: 0.980495274066925\n",
      "第 355 次测试 D loss_test: 0.24446143 D acc_test: 50.0 G loss_test: 0.1130392 G pearson_test: 0.97859055\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 356 次训练 D loss_train: 0.027564922347664833 D acc_train: 50.0 G loss_train: 0.09437710791826248 G pearson_train: 0.9804966449737549\n",
      "第 356 次测试 D loss_test: 0.25123206 D acc_test: 50.0 G loss_test: 0.11299592 G pearson_test: 0.97858816\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 357 次训练 D loss_train: 0.02723691239953041 D acc_train: 50.0 G loss_train: 0.09448125958442688 G pearson_train: 0.9804835319519043\n",
      "第 357 次测试 D loss_test: 0.24680102 D acc_test: 50.0 G loss_test: 0.11310587 G pearson_test: 0.9785833\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 358 次训练 D loss_train: 0.028498180210590363 D acc_train: 50.0 G loss_train: 0.09440677613019943 G pearson_train: 0.9804873466491699\n",
      "第 358 次测试 D loss_test: 0.25481188 D acc_test: 50.0 G loss_test: 0.113029435 G pearson_test: 0.9785821\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 359 次训练 D loss_train: 0.02807132713496685 D acc_train: 50.0 G loss_train: 0.09453085064888 G pearson_train: 0.9804718494415283\n",
      "第 359 次测试 D loss_test: 0.24900353 D acc_test: 50.0 G loss_test: 0.11316952 G pearson_test: 0.9785762\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 360 次训练 D loss_train: 0.029498787596821785 D acc_train: 50.0 G loss_train: 0.09444643557071686 G pearson_train: 0.980478048324585\n",
      "第 360 次测试 D loss_test: 0.25840628 D acc_test: 50.0 G loss_test: 0.113071546 G pearson_test: 0.9785765\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 361 次训练 D loss_train: 0.029009854421019554 D acc_train: 50.0 G loss_train: 0.09458646923303604 G pearson_train: 0.9804602265357971\n",
      "第 361 次测试 D loss_test: 0.25122914 D acc_test: 50.0 G loss_test: 0.11324529 G pearson_test: 0.9785695\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 362 次训练 D loss_train: 0.03063957206904888 D acc_train: 50.0 G loss_train: 0.0944853127002716 G pearson_train: 0.9804698824882507\n",
      "第 362 次测试 D loss_test: 0.26227832 D acc_test: 50.0 G loss_test: 0.113115095 G pearson_test: 0.9785714\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 363 次训练 D loss_train: 0.030127933248877525 D acc_train: 50.0 G loss_train: 0.09464900195598602 G pearson_train: 0.9804486036300659\n",
      "第 363 次测试 D loss_test: 0.25332707 D acc_test: 50.0 G loss_test: 0.11332544 G pearson_test: 0.9785626\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 364 次训练 D loss_train: 0.03193725645542145 D acc_train: 50.0 G loss_train: 0.09452559053897858 G pearson_train: 0.9804624915122986\n",
      "第 364 次测试 D loss_test: 0.2663585 D acc_test: 50.0 G loss_test: 0.11316438 G pearson_test: 0.9785665\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 365 次训练 D loss_train: 0.03138689324259758 D acc_train: 50.0 G loss_train: 0.09470823407173157 G pearson_train: 0.9804376363754272\n",
      "第 365 次测试 D loss_test: 0.2555852 D acc_test: 50.0 G loss_test: 0.113398656 G pearson_test: 0.9785563\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 366 次训练 D loss_train: 0.03356827795505524 D acc_train: 50.0 G loss_train: 0.09457646310329437 G pearson_train: 0.980455756187439\n",
      "第 366 次测试 D loss_test: 0.27191785 D acc_test: 50.0 G loss_test: 0.113200985 G pearson_test: 0.9785621\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 367 次训练 D loss_train: 0.033551737666130066 D acc_train: 50.0 G loss_train: 0.09479357302188873 G pearson_train: 0.9804254770278931\n",
      "第 367 次测试 D loss_test: 0.25884944 D acc_test: 50.0 G loss_test: 0.11350156 G pearson_test: 0.9785496\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 368 次训练 D loss_train: 0.03607843443751335 D acc_train: 50.0 G loss_train: 0.09462512284517288 G pearson_train: 0.9804503917694092\n",
      "第 368 次测试 D loss_test: 0.27902108 D acc_test: 50.0 G loss_test: 0.11325434 G pearson_test: 0.97855866\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 369 次训练 D loss_train: 0.03664742410182953 D acc_train: 50.0 G loss_train: 0.09490280598402023 G pearson_train: 0.9804129004478455\n",
      "第 369 次测试 D loss_test: 0.2631314 D acc_test: 50.0 G loss_test: 0.11361087 G pearson_test: 0.9785435\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 370 次训练 D loss_train: 0.03923766687512398 D acc_train: 50.0 G loss_train: 0.09467601031064987 G pearson_train: 0.9804463386535645\n",
      "第 370 次测试 D loss_test: 0.28752294 D acc_test: 50.0 G loss_test: 0.11331361 G pearson_test: 0.9785563\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 371 次训练 D loss_train: 0.04009881243109703 D acc_train: 50.0 G loss_train: 0.09506034106016159 G pearson_train: 0.9804004430770874\n",
      "第 371 次测试 D loss_test: 0.26517594 D acc_test: 50.0 G loss_test: 0.11372277 G pearson_test: 0.97853816\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 372 次训练 D loss_train: 0.03980427607893944 D acc_train: 50.0 G loss_train: 0.09469085186719894 G pearson_train: 0.9804388284683228\n",
      "第 372 次测试 D loss_test: 0.28570604 D acc_test: 50.0 G loss_test: 0.11339718 G pearson_test: 0.9785536\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 373 次训练 D loss_train: 0.038425013422966 D acc_train: 50.0 G loss_train: 0.09500669687986374 G pearson_train: 0.9803940057754517\n",
      "第 373 次测试 D loss_test: 0.25899705 D acc_test: 50.0 G loss_test: 0.1136569 G pearson_test: 0.97853553\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 374 次训练 D loss_train: 0.03357948362827301 D acc_train: 50.0 G loss_train: 0.09479246288537979 G pearson_train: 0.9804214239120483\n",
      "第 374 次测试 D loss_test: 0.2578557 D acc_test: 50.0 G loss_test: 0.1134558 G pearson_test: 0.9785517\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 375 次训练 D loss_train: 0.02727767825126648 D acc_train: 50.0 G loss_train: 0.09496132284402847 G pearson_train: 0.9803986549377441\n",
      "第 375 次测试 D loss_test: 0.24753007 D acc_test: 50.0 G loss_test: 0.11360091 G pearson_test: 0.9785344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 376 次训练 D loss_train: 0.02606191113591194 D acc_train: 50.0 G loss_train: 0.09492664039134979 G pearson_train: 0.9804024696350098\n",
      "第 376 次测试 D loss_test: 0.25302875 D acc_test: 50.0 G loss_test: 0.11357053 G pearson_test: 0.97853667\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 377 次训练 D loss_train: 0.025324933230876923 D acc_train: 50.0 G loss_train: 0.09506183862686157 G pearson_train: 0.9803878664970398\n",
      "第 377 次测试 D loss_test: 0.24814087 D acc_test: 50.0 G loss_test: 0.11371378 G pearson_test: 0.9785301\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 378 次训练 D loss_train: 0.025277629494667053 D acc_train: 50.0 G loss_train: 0.09501269459724426 G pearson_train: 0.9803891181945801\n",
      "第 378 次测试 D loss_test: 0.2528996 D acc_test: 50.0 G loss_test: 0.11372543 G pearson_test: 0.9785285\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 379 次训练 D loss_train: 0.025029314681887627 D acc_train: 50.0 G loss_train: 0.09512291848659515 G pearson_train: 0.9803766012191772\n",
      "第 379 次测试 D loss_test: 0.24999855 D acc_test: 50.0 G loss_test: 0.11379647 G pearson_test: 0.9785248\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 380 次训练 D loss_train: 0.02515089511871338 D acc_train: 50.0 G loss_train: 0.09503975510597229 G pearson_train: 0.9803769588470459\n",
      "第 380 次测试 D loss_test: 0.25416958 D acc_test: 50.0 G loss_test: 0.113795154 G pearson_test: 0.9785231\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 381 次训练 D loss_train: 0.02505088597536087 D acc_train: 50.0 G loss_train: 0.09516263753175735 G pearson_train: 0.9803651571273804\n",
      "第 381 次测试 D loss_test: 0.2523334 D acc_test: 50.0 G loss_test: 0.11382773 G pearson_test: 0.9785217\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 382 次训练 D loss_train: 0.02533069998025894 D acc_train: 50.0 G loss_train: 0.09504935145378113 G pearson_train: 0.980365514755249\n",
      "第 382 次测试 D loss_test: 0.25665483 D acc_test: 50.0 G loss_test: 0.11381635 G pearson_test: 0.9785195\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 383 次训练 D loss_train: 0.025479061529040337 D acc_train: 50.0 G loss_train: 0.09519952535629272 G pearson_train: 0.9803542494773865\n",
      "第 383 次测试 D loss_test: 0.2543012 D acc_test: 50.0 G loss_test: 0.11385468 G pearson_test: 0.97851914\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 384 次训练 D loss_train: 0.025868406519293785 D acc_train: 50.0 G loss_train: 0.09506118297576904 G pearson_train: 0.980356752872467\n",
      "第 384 次测试 D loss_test: 0.25921744 D acc_test: 50.0 G loss_test: 0.11383294 G pearson_test: 0.9785159\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 385 次训练 D loss_train: 0.026173293590545654 D acc_train: 50.0 G loss_train: 0.09523787349462509 G pearson_train: 0.9803434014320374\n",
      "第 385 次测试 D loss_test: 0.25529188 D acc_test: 50.0 G loss_test: 0.11390117 G pearson_test: 0.978515\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 386 次训练 D loss_train: 0.026072628796100616 D acc_train: 50.0 G loss_train: 0.0950642004609108 G pearson_train: 0.9803479909896851\n",
      "第 386 次测试 D loss_test: 0.26117957 D acc_test: 50.0 G loss_test: 0.11385891 G pearson_test: 0.9785129\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 387 次训练 D loss_train: 0.026468828320503235 D acc_train: 50.0 G loss_train: 0.0952533558011055 G pearson_train: 0.980333149433136\n",
      "第 387 次测试 D loss_test: 0.2558503 D acc_test: 50.0 G loss_test: 0.113910794 G pearson_test: 0.97851217\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 388 次训练 D loss_train: 0.025896873325109482 D acc_train: 50.0 G loss_train: 0.0951005071401596 G pearson_train: 0.9803385734558105\n",
      "第 388 次测试 D loss_test: 0.2617494 D acc_test: 50.0 G loss_test: 0.11388629 G pearson_test: 0.9785109\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 389 次训练 D loss_train: 0.02596331015229225 D acc_train: 50.0 G loss_train: 0.09529057890176773 G pearson_train: 0.9803253412246704\n",
      "第 389 次测试 D loss_test: 0.2569493 D acc_test: 50.0 G loss_test: 0.11391939 G pearson_test: 0.9785091\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 390 次训练 D loss_train: 0.02601322904229164 D acc_train: 50.0 G loss_train: 0.09512477368116379 G pearson_train: 0.9803298115730286\n",
      "第 390 次测试 D loss_test: 0.26343423 D acc_test: 50.0 G loss_test: 0.11392425 G pearson_test: 0.97850573\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 391 次训练 D loss_train: 0.02653948962688446 D acc_train: 50.0 G loss_train: 0.09532299637794495 G pearson_train: 0.9803139567375183\n",
      "第 391 次测试 D loss_test: 0.2575973 D acc_test: 50.0 G loss_test: 0.11398138 G pearson_test: 0.9785053\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 392 次训练 D loss_train: 0.025759462267160416 D acc_train: 50.0 G loss_train: 0.09516925364732742 G pearson_train: 0.9803209900856018\n",
      "第 392 次测试 D loss_test: 0.2635494 D acc_test: 50.0 G loss_test: 0.113970436 G pearson_test: 0.97850454\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 393 次训练 D loss_train: 0.025669466704130173 D acc_train: 50.0 G loss_train: 0.09535549581050873 G pearson_train: 0.980307400226593\n",
      "第 393 次测试 D loss_test: 0.2585711 D acc_test: 50.0 G loss_test: 0.11397758 G pearson_test: 0.97850233\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 394 次训练 D loss_train: 0.02573324739933014 D acc_train: 50.0 G loss_train: 0.0951961874961853 G pearson_train: 0.9803118705749512\n",
      "第 394 次测试 D loss_test: 0.26495013 D acc_test: 50.0 G loss_test: 0.1140062 G pearson_test: 0.9784992\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 395 次训练 D loss_train: 0.026151582598686218 D acc_train: 50.0 G loss_train: 0.09538781642913818 G pearson_train: 0.9802956581115723\n",
      "第 395 次测试 D loss_test: 0.2595263 D acc_test: 50.0 G loss_test: 0.114042245 G pearson_test: 0.9784988\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 396 次训练 D loss_train: 0.025584472343325615 D acc_train: 50.0 G loss_train: 0.09523236751556396 G pearson_train: 0.9803035259246826\n",
      "第 396 次测试 D loss_test: 0.265857 D acc_test: 50.0 G loss_test: 0.11404643 G pearson_test: 0.9784984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 397 次训练 D loss_train: 0.025548629462718964 D acc_train: 50.0 G loss_train: 0.0954139456152916 G pearson_train: 0.980289101600647\n",
      "第 397 次测试 D loss_test: 0.26029465 D acc_test: 50.0 G loss_test: 0.11404309 G pearson_test: 0.9784963\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 398 次训练 D loss_train: 0.025515688583254814 D acc_train: 50.0 G loss_train: 0.09525488317012787 G pearson_train: 0.9802947044372559\n",
      "第 398 次测试 D loss_test: 0.26684654 D acc_test: 50.0 G loss_test: 0.11407483 G pearson_test: 0.9784939\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 399 次训练 D loss_train: 0.02584676258265972 D acc_train: 50.0 G loss_train: 0.09544156491756439 G pearson_train: 0.9802780151367188\n",
      "第 399 次测试 D loss_test: 0.26130766 D acc_test: 50.0 G loss_test: 0.114089996 G pearson_test: 0.9784938\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 400 次训练 D loss_train: 0.025525854900479317 D acc_train: 50.0 G loss_train: 0.09526268392801285 G pearson_train: 0.980287492275238\n",
      "第 400 次测试 D loss_test: 0.26847133 D acc_test: 50.0 G loss_test: 0.11410821 G pearson_test: 0.97849333\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 401 次训练 D loss_train: 0.025545410811901093 D acc_train: 50.0 G loss_train: 0.09546258300542831 G pearson_train: 0.9802706837654114\n",
      "第 401 次测试 D loss_test: 0.2611545 D acc_test: 50.0 G loss_test: 0.11407456 G pearson_test: 0.9784922\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 402 次训练 D loss_train: 0.025307293981313705 D acc_train: 50.0 G loss_train: 0.095279760658741 G pearson_train: 0.9802799820899963\n",
      "第 402 次测试 D loss_test: 0.26880768 D acc_test: 50.0 G loss_test: 0.114131905 G pearson_test: 0.97849065\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 403 次训练 D loss_train: 0.02549683302640915 D acc_train: 50.0 G loss_train: 0.09548312425613403 G pearson_train: 0.9802613258361816\n",
      "第 403 次测试 D loss_test: 0.26211083 D acc_test: 50.0 G loss_test: 0.11409116 G pearson_test: 0.97849005\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 404 次训练 D loss_train: 0.025340436026453972 D acc_train: 50.0 G loss_train: 0.0953059121966362 G pearson_train: 0.9802722334861755\n",
      "第 404 次测试 D loss_test: 0.27079076 D acc_test: 50.0 G loss_test: 0.11415143 G pearson_test: 0.9784894\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 405 次训练 D loss_train: 0.025429587811231613 D acc_train: 50.0 G loss_train: 0.09552032500505447 G pearson_train: 0.9802523851394653\n",
      "第 405 次测试 D loss_test: 0.26192296 D acc_test: 50.0 G loss_test: 0.11410834 G pearson_test: 0.9784882\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 406 次训练 D loss_train: 0.02516878768801689 D acc_train: 50.0 G loss_train: 0.09533502906560898 G pearson_train: 0.9802647233009338\n",
      "第 406 次测试 D loss_test: 0.27100587 D acc_test: 50.0 G loss_test: 0.11419508 G pearson_test: 0.9784873\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 407 次训练 D loss_train: 0.025251248851418495 D acc_train: 50.0 G loss_train: 0.09556460380554199 G pearson_train: 0.9802436828613281\n",
      "第 407 次测试 D loss_test: 0.26258075 D acc_test: 50.0 G loss_test: 0.11413018 G pearson_test: 0.97848624\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 408 次训练 D loss_train: 0.025251595303416252 D acc_train: 50.0 G loss_train: 0.09534342586994171 G pearson_train: 0.9802577495574951\n",
      "第 408 次测试 D loss_test: 0.27379102 D acc_test: 50.0 G loss_test: 0.11423522 G pearson_test: 0.97848463\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 409 次训练 D loss_train: 0.025551017373800278 D acc_train: 50.0 G loss_train: 0.09559925645589828 G pearson_train: 0.980233371257782\n",
      "第 409 次测试 D loss_test: 0.2623859 D acc_test: 50.0 G loss_test: 0.114140615 G pearson_test: 0.97848463\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 410 次训练 D loss_train: 0.0253544170409441 D acc_train: 50.0 G loss_train: 0.0953422263264656 G pearson_train: 0.9802523851394653\n",
      "第 410 次测试 D loss_test: 0.27484158 D acc_test: 50.0 G loss_test: 0.11427505 G pearson_test: 0.9784839\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 411 次训练 D loss_train: 0.025526665151119232 D acc_train: 50.0 G loss_train: 0.095646932721138 G pearson_train: 0.980224609375\n",
      "第 411 次测试 D loss_test: 0.2627143 D acc_test: 50.0 G loss_test: 0.11412832 G pearson_test: 0.97848296\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 412 次训练 D loss_train: 0.025732409209012985 D acc_train: 50.0 G loss_train: 0.09533078968524933 G pearson_train: 0.9802476763725281\n",
      "第 412 次测试 D loss_test: 0.2796852 D acc_test: 50.0 G loss_test: 0.11431952 G pearson_test: 0.978481\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 413 次训练 D loss_train: 0.026407595723867416 D acc_train: 50.0 G loss_train: 0.09571342170238495 G pearson_train: 0.9802117347717285\n",
      "第 413 次测试 D loss_test: 0.26301295 D acc_test: 50.0 G loss_test: 0.11412284 G pearson_test: 0.9784816\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 414 次训练 D loss_train: 0.02656557783484459 D acc_train: 50.0 G loss_train: 0.09532073140144348 G pearson_train: 0.980246901512146\n",
      "第 414 次测试 D loss_test: 0.28471708 D acc_test: 50.0 G loss_test: 0.1143975 G pearson_test: 0.9784808\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 415 次训练 D loss_train: 0.02728462964296341 D acc_train: 50.0 G loss_train: 0.09582681208848953 G pearson_train: 0.9802005290985107\n",
      "第 415 次测试 D loss_test: 0.2642722 D acc_test: 50.0 G loss_test: 0.11410563 G pearson_test: 0.97848016\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 416 次训练 D loss_train: 0.02863045409321785 D acc_train: 50.0 G loss_train: 0.09529012441635132 G pearson_train: 0.9802502393722534\n",
      "第 416 次测试 D loss_test: 0.30303884 D acc_test: 50.0 G loss_test: 0.11452024 G pearson_test: 0.9784778\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 417 次训练 D loss_train: 0.03237128257751465 D acc_train: 50.0 G loss_train: 0.09620284289121628 G pearson_train: 0.9801726341247559\n",
      "第 417 次测试 D loss_test: 0.27976495 D acc_test: 50.0 G loss_test: 0.11415594 G pearson_test: 0.9784796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 418 次训练 D loss_train: 0.04001370444893837 D acc_train: 50.0 G loss_train: 0.09548710286617279 G pearson_train: 0.9802806377410889\n",
      "第 418 次测试 D loss_test: 0.401078 D acc_test: 50.0 G loss_test: 0.11519988 G pearson_test: 0.9784786\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 419 次训练 D loss_train: 0.07525211572647095 D acc_train: 50.0 G loss_train: 0.1000852882862091 G pearson_train: 0.9800662398338318\n",
      "第 419 次测试 D loss_test: 0.726607 D acc_test: 50.0 G loss_test: 0.11617295 G pearson_test: 0.97848374\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 420 次训练 D loss_train: 0.39462849497795105 D acc_train: 50.0 G loss_train: 0.10149933397769928 G pearson_train: 0.980379045009613\n",
      "第 420 次测试 D loss_test: 2.212454 D acc_test: 50.54347826086957 G loss_test: 0.12348447 G pearson_test: 0.9784717\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 421 次训练 D loss_train: 1.7479941844940186 D acc_train: 51.28205418586731 G loss_train: 0.09885936975479126 G pearson_train: 0.9801063537597656\n",
      "第 421 次测试 D loss_test: 1.5821064 D acc_test: 53.2608695652174 G loss_test: 0.11567556 G pearson_test: 0.9785249\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 422 次训练 D loss_train: 1.3266817331314087 D acc_train: 60.256409645080566 G loss_train: 0.09076918661594391 G pearson_train: 0.9804250597953796\n",
      "第 422 次测试 D loss_test: 0.6944329 D acc_test: 51.08695652173913 G loss_test: 0.112836495 G pearson_test: 0.97853875\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 423 次训练 D loss_train: 0.33796781301498413 D acc_train: 53.113555908203125 G loss_train: 0.08971001952886581 G pearson_train: 0.9804078936576843\n",
      "第 423 次测试 D loss_test: 0.36801457 D acc_test: 50.0 G loss_test: 0.10701355 G pearson_test: 0.978575\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 424 次训练 D loss_train: 0.17537108063697815 D acc_train: 65.01831412315369 G loss_train: 0.09075047075748444 G pearson_train: 0.9804528951644897\n",
      "第 424 次测试 D loss_test: 0.33560532 D acc_test: 50.0 G loss_test: 0.10979156 G pearson_test: 0.97857195\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 425 次训练 D loss_train: 0.09076278656721115 D acc_train: 50.54945349693298 G loss_train: 0.09045705944299698 G pearson_train: 0.9804354310035706\n",
      "第 425 次测试 D loss_test: 0.2299738 D acc_test: 50.0 G loss_test: 0.10775507 G pearson_test: 0.978607\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 426 次训练 D loss_train: 0.04547079652547836 D acc_train: 52.3809552192688 G loss_train: 0.09105054289102554 G pearson_train: 0.9804378747940063\n",
      "第 426 次测试 D loss_test: 0.20870703 D acc_test: 50.0 G loss_test: 0.10874296 G pearson_test: 0.97860384\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 427 次训练 D loss_train: 0.037188492715358734 D acc_train: 51.64835453033447 G loss_train: 0.09133587777614594 G pearson_train: 0.9804377555847168\n",
      "第 427 次测试 D loss_test: 0.1976237 D acc_test: 50.0 G loss_test: 0.10899162 G pearson_test: 0.9786105\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 428 次训练 D loss_train: 0.03310657665133476 D acc_train: 51.46520137786865 G loss_train: 0.09160134941339493 G pearson_train: 0.9804365634918213\n",
      "第 428 次测试 D loss_test: 0.1884132 D acc_test: 50.0 G loss_test: 0.10920399 G pearson_test: 0.9786154\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 429 次训练 D loss_train: 0.03018268197774887 D acc_train: 51.46520137786865 G loss_train: 0.09186705201864243 G pearson_train: 0.9804336428642273\n",
      "第 429 次测试 D loss_test: 0.1810588 D acc_test: 50.0 G loss_test: 0.10944101 G pearson_test: 0.97861844\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 430 次训练 D loss_train: 0.027961313724517822 D acc_train: 51.46520137786865 G loss_train: 0.09206817299127579 G pearson_train: 0.9804317951202393\n",
      "第 430 次测试 D loss_test: 0.17516479 D acc_test: 50.0 G loss_test: 0.10971038 G pearson_test: 0.9786202\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 431 次训练 D loss_train: 0.02629871293902397 D acc_train: 51.28205418586731 G loss_train: 0.09226454794406891 G pearson_train: 0.9804287552833557\n",
      "第 431 次测试 D loss_test: 0.17041618 D acc_test: 50.0 G loss_test: 0.109899886 G pearson_test: 0.97862226\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 432 次训练 D loss_train: 0.025039758533239365 D acc_train: 51.28205418586731 G loss_train: 0.09244741499423981 G pearson_train: 0.9804253578186035\n",
      "第 432 次测试 D loss_test: 0.16655304 D acc_test: 50.0 G loss_test: 0.11009196 G pearson_test: 0.97862375\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 433 次训练 D loss_train: 0.024062547832727432 D acc_train: 51.09890103340149 G loss_train: 0.09260167181491852 G pearson_train: 0.9804221987724304\n",
      "第 433 次测试 D loss_test: 0.16345116 D acc_test: 50.0 G loss_test: 0.11028012 G pearson_test: 0.9786245\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 434 次训练 D loss_train: 0.023265548050403595 D acc_train: 51.09890103340149 G loss_train: 0.09274402260780334 G pearson_train: 0.9804191589355469\n",
      "第 434 次测试 D loss_test: 0.16108757 D acc_test: 50.0 G loss_test: 0.110427804 G pearson_test: 0.97862524\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 435 次训练 D loss_train: 0.022603534162044525 D acc_train: 50.91575384140015 G loss_train: 0.09287533909082413 G pearson_train: 0.9804161190986633\n",
      "第 435 次测试 D loss_test: 0.15928346 D acc_test: 50.0 G loss_test: 0.110572666 G pearson_test: 0.97862566\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 436 次训练 D loss_train: 0.022050626575946808 D acc_train: 50.91575384140015 G loss_train: 0.09301640838384628 G pearson_train: 0.9804121255874634\n",
      "第 436 次测试 D loss_test: 0.15774997 D acc_test: 50.0 G loss_test: 0.11071418 G pearson_test: 0.9786255\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 437 次训练 D loss_train: 0.02157648280262947 D acc_train: 50.91575384140015 G loss_train: 0.09313739836215973 G pearson_train: 0.9804085493087769\n",
      "第 437 次测试 D loss_test: 0.15638128 D acc_test: 50.0 G loss_test: 0.110878885 G pearson_test: 0.9786247\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 438 次训练 D loss_train: 0.021185968071222305 D acc_train: 50.91575384140015 G loss_train: 0.09323334693908691 G pearson_train: 0.9804055690765381\n",
      "第 438 次测试 D loss_test: 0.15522635 D acc_test: 50.0 G loss_test: 0.111019306 G pearson_test: 0.9786241\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 439 次训练 D loss_train: 0.020865442231297493 D acc_train: 50.91575384140015 G loss_train: 0.09335380792617798 G pearson_train: 0.9804012775421143\n",
      "第 439 次测试 D loss_test: 0.15439457 D acc_test: 50.0 G loss_test: 0.11112604 G pearson_test: 0.9786235\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 440 次训练 D loss_train: 0.020603567361831665 D acc_train: 50.91575384140015 G loss_train: 0.09345148503780365 G pearson_train: 0.9803974032402039\n",
      "第 440 次测试 D loss_test: 0.15381685 D acc_test: 50.0 G loss_test: 0.11126373 G pearson_test: 0.97862214\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 441 次训练 D loss_train: 0.020394928753376007 D acc_train: 50.91575384140015 G loss_train: 0.09352143108844757 G pearson_train: 0.9803940057754517\n",
      "第 441 次测试 D loss_test: 0.15349574 D acc_test: 50.0 G loss_test: 0.111378744 G pearson_test: 0.97862077\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 442 次训练 D loss_train: 0.020240604877471924 D acc_train: 50.91575384140015 G loss_train: 0.09360145777463913 G pearson_train: 0.9803897142410278\n",
      "第 442 次测试 D loss_test: 0.15359688 D acc_test: 50.0 G loss_test: 0.111462966 G pearson_test: 0.9786194\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 443 次训练 D loss_train: 0.020140890032052994 D acc_train: 50.91575384140015 G loss_train: 0.09367893636226654 G pearson_train: 0.9803853034973145\n",
      "第 443 次测试 D loss_test: 0.15389319 D acc_test: 50.0 G loss_test: 0.11155719 G pearson_test: 0.9786178\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 444 次训练 D loss_train: 0.020099205896258354 D acc_train: 50.91575384140015 G loss_train: 0.09374328702688217 G pearson_train: 0.9803808331489563\n",
      "第 444 次测试 D loss_test: 0.15434703 D acc_test: 50.0 G loss_test: 0.11164853 G pearson_test: 0.9786159\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 445 次训练 D loss_train: 0.02009923756122589 D acc_train: 50.91575384140015 G loss_train: 0.09382286667823792 G pearson_train: 0.9803758859634399\n",
      "第 445 次测试 D loss_test: 0.15507668 D acc_test: 50.0 G loss_test: 0.11172834 G pearson_test: 0.9786138\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 446 次训练 D loss_train: 0.020141031593084335 D acc_train: 50.91575384140015 G loss_train: 0.09388989210128784 G pearson_train: 0.9803709983825684\n",
      "第 446 次测试 D loss_test: 0.15591933 D acc_test: 50.0 G loss_test: 0.11182463 G pearson_test: 0.9786113\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 447 次训练 D loss_train: 0.020221397280693054 D acc_train: 50.91575384140015 G loss_train: 0.09396928548812866 G pearson_train: 0.980365514755249\n",
      "第 447 次测试 D loss_test: 0.15683833 D acc_test: 50.0 G loss_test: 0.11191016 G pearson_test: 0.9786087\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 448 次训练 D loss_train: 0.02033476158976555 D acc_train: 50.91575384140015 G loss_train: 0.09402431547641754 G pearson_train: 0.9803608655929565\n",
      "第 448 次测试 D loss_test: 0.15774202 D acc_test: 50.0 G loss_test: 0.11200371 G pearson_test: 0.9786056\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 449 次训练 D loss_train: 0.02048218622803688 D acc_train: 50.91575384140015 G loss_train: 0.09406755864620209 G pearson_train: 0.980355978012085\n",
      "第 449 次测试 D loss_test: 0.15872823 D acc_test: 50.0 G loss_test: 0.112075716 G pearson_test: 0.97860265\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 450 次训练 D loss_train: 0.020658811554312706 D acc_train: 50.91575384140015 G loss_train: 0.09412728995084763 G pearson_train: 0.9803502559661865\n",
      "第 450 次测试 D loss_test: 0.15982968 D acc_test: 50.0 G loss_test: 0.11213316 G pearson_test: 0.9785998\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 451 次训练 D loss_train: 0.020840086042881012 D acc_train: 50.91575384140015 G loss_train: 0.09417364001274109 G pearson_train: 0.9803450703620911\n",
      "第 451 次测试 D loss_test: 0.16092214 D acc_test: 50.0 G loss_test: 0.112213895 G pearson_test: 0.97859645\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 452 次训练 D loss_train: 0.021046262234449387 D acc_train: 50.91575384140015 G loss_train: 0.09424267709255219 G pearson_train: 0.9803386926651001\n",
      "第 452 次测试 D loss_test: 0.16205435 D acc_test: 50.0 G loss_test: 0.11227155 G pearson_test: 0.97859293\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 453 次训练 D loss_train: 0.021267812699079514 D acc_train: 50.732600688934326 G loss_train: 0.09431055188179016 G pearson_train: 0.9803324341773987\n",
      "第 453 次测试 D loss_test: 0.16319111 D acc_test: 50.0 G loss_test: 0.11236015 G pearson_test: 0.9785889\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 454 次训练 D loss_train: 0.021493200212717056 D acc_train: 50.36630034446716 G loss_train: 0.09436684101819992 G pearson_train: 0.9803261756896973\n",
      "第 454 次测试 D loss_test: 0.16434896 D acc_test: 50.0 G loss_test: 0.11245329 G pearson_test: 0.97858447\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 455 次训练 D loss_train: 0.021718347445130348 D acc_train: 50.36630034446716 G loss_train: 0.09442301094532013 G pearson_train: 0.9803199768066406\n",
      "第 455 次测试 D loss_test: 0.16553107 D acc_test: 50.0 G loss_test: 0.11252588 G pearson_test: 0.97858024\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 456 次训练 D loss_train: 0.021947884932160378 D acc_train: 50.36630034446716 G loss_train: 0.09446365386247635 G pearson_train: 0.9803140163421631\n",
      "第 456 次测试 D loss_test: 0.16673142 D acc_test: 50.0 G loss_test: 0.11259795 G pearson_test: 0.9785759\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 457 次训练 D loss_train: 0.0221723485738039 D acc_train: 50.36630034446716 G loss_train: 0.09452249854803085 G pearson_train: 0.9803076386451721\n",
      "第 457 次测试 D loss_test: 0.16787341 D acc_test: 50.0 G loss_test: 0.11265178 G pearson_test: 0.97857165\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 458 次训练 D loss_train: 0.022390000522136688 D acc_train: 50.18315315246582 G loss_train: 0.09458461403846741 G pearson_train: 0.9803009033203125\n",
      "第 458 次测试 D loss_test: 0.16905883 D acc_test: 50.0 G loss_test: 0.11272833 G pearson_test: 0.97856665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 459 次训练 D loss_train: 0.022620651870965958 D acc_train: 50.18315315246582 G loss_train: 0.09464089572429657 G pearson_train: 0.9802945852279663\n",
      "第 459 次测试 D loss_test: 0.17024115 D acc_test: 50.0 G loss_test: 0.11280767 G pearson_test: 0.97856164\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 460 次训练 D loss_train: 0.02285071462392807 D acc_train: 50.18315315246582 G loss_train: 0.09470511972904205 G pearson_train: 0.9802875518798828\n",
      "第 460 次测试 D loss_test: 0.17145742 D acc_test: 50.0 G loss_test: 0.112884894 G pearson_test: 0.9785564\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 461 次训练 D loss_train: 0.02307882159948349 D acc_train: 50.18315315246582 G loss_train: 0.09476830065250397 G pearson_train: 0.9802808165550232\n",
      "第 461 次测试 D loss_test: 0.17271715 D acc_test: 50.0 G loss_test: 0.11297015 G pearson_test: 0.978551\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 462 次训练 D loss_train: 0.02331528253853321 D acc_train: 50.18315315246582 G loss_train: 0.09481855481863022 G pearson_train: 0.9802742600440979\n",
      "第 462 次测试 D loss_test: 0.173857 D acc_test: 50.0 G loss_test: 0.11305033 G pearson_test: 0.97854537\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 463 次训练 D loss_train: 0.023563198745250702 D acc_train: 50.18315315246582 G loss_train: 0.09485428035259247 G pearson_train: 0.9802687168121338\n",
      "第 463 次测试 D loss_test: 0.17508715 D acc_test: 50.0 G loss_test: 0.11311712 G pearson_test: 0.9785399\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 464 次训练 D loss_train: 0.023812970146536827 D acc_train: 50.18315315246582 G loss_train: 0.09490089863538742 G pearson_train: 0.9802623987197876\n",
      "第 464 次测试 D loss_test: 0.17642316 D acc_test: 50.0 G loss_test: 0.11315675 G pearson_test: 0.97853464\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 465 次训练 D loss_train: 0.024044077843427658 D acc_train: 50.18315315246582 G loss_train: 0.09496070444583893 G pearson_train: 0.980255663394928\n",
      "第 465 次测试 D loss_test: 0.17769545 D acc_test: 50.0 G loss_test: 0.11321748 G pearson_test: 0.97852916\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 466 次训练 D loss_train: 0.024244260042905807 D acc_train: 50.18315315246582 G loss_train: 0.09500005841255188 G pearson_train: 0.9802496433258057\n",
      "第 466 次测试 D loss_test: 0.17891085 D acc_test: 50.0 G loss_test: 0.113290146 G pearson_test: 0.97852355\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 467 次训练 D loss_train: 0.024460460990667343 D acc_train: 50.18315315246582 G loss_train: 0.09503897279500961 G pearson_train: 0.9802438616752625\n",
      "第 467 次测试 D loss_test: 0.17993337 D acc_test: 50.0 G loss_test: 0.1133369 G pearson_test: 0.97851837\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 468 次训练 D loss_train: 0.02466176450252533 D acc_train: 50.18315315246582 G loss_train: 0.09507865458726883 G pearson_train: 0.9802379608154297\n",
      "第 468 次测试 D loss_test: 0.1807826 D acc_test: 50.0 G loss_test: 0.11338777 G pearson_test: 0.97851336\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 469 次训练 D loss_train: 0.02482936903834343 D acc_train: 50.18315315246582 G loss_train: 0.09511493891477585 G pearson_train: 0.980232298374176\n",
      "第 469 次测试 D loss_test: 0.1814685 D acc_test: 50.0 G loss_test: 0.11343803 G pearson_test: 0.9785083\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 470 次训练 D loss_train: 0.024964123964309692 D acc_train: 50.0 G loss_train: 0.09516704082489014 G pearson_train: 0.9802261590957642\n",
      "第 470 次测试 D loss_test: 0.18212235 D acc_test: 50.0 G loss_test: 0.1134781 G pearson_test: 0.9785038\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 471 次训练 D loss_train: 0.025048676878213882 D acc_train: 50.0 G loss_train: 0.09523347020149231 G pearson_train: 0.9802197217941284\n",
      "第 471 次测试 D loss_test: 0.18260965 D acc_test: 50.0 G loss_test: 0.11354201 G pearson_test: 0.9784988\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 472 次训练 D loss_train: 0.025117136538028717 D acc_train: 50.0 G loss_train: 0.09528946876525879 G pearson_train: 0.9802136421203613\n",
      "第 472 次测试 D loss_test: 0.18299711 D acc_test: 50.0 G loss_test: 0.11362377 G pearson_test: 0.97849363\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 473 次训练 D loss_train: 0.02517017349600792 D acc_train: 50.0 G loss_train: 0.09534719586372375 G pearson_train: 0.9802078008651733\n",
      "第 473 次测试 D loss_test: 0.18329164 D acc_test: 50.0 G loss_test: 0.11368293 G pearson_test: 0.97848916\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 474 次训练 D loss_train: 0.02518508955836296 D acc_train: 50.0 G loss_train: 0.09540321677923203 G pearson_train: 0.9802022576332092\n",
      "第 474 次测试 D loss_test: 0.18344519 D acc_test: 50.0 G loss_test: 0.11375315 G pearson_test: 0.9784843\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 475 次训练 D loss_train: 0.02516200952231884 D acc_train: 50.0 G loss_train: 0.09546966850757599 G pearson_train: 0.980196475982666\n",
      "第 475 次测试 D loss_test: 0.18350291 D acc_test: 50.0 G loss_test: 0.11381716 G pearson_test: 0.9784796\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 476 次训练 D loss_train: 0.025114446878433228 D acc_train: 50.0 G loss_train: 0.095539890229702 G pearson_train: 0.9801902770996094\n",
      "第 476 次测试 D loss_test: 0.18358359 D acc_test: 50.0 G loss_test: 0.11389463 G pearson_test: 0.9784747\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 477 次训练 D loss_train: 0.025047380477190018 D acc_train: 50.0 G loss_train: 0.09558812528848648 G pearson_train: 0.9801851511001587\n",
      "第 477 次测试 D loss_test: 0.18361394 D acc_test: 50.0 G loss_test: 0.113966115 G pearson_test: 0.9784699\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 478 次训练 D loss_train: 0.025005701929330826 D acc_train: 50.0 G loss_train: 0.09565667808055878 G pearson_train: 0.9801796078681946\n",
      "第 478 次测试 D loss_test: 0.18352361 D acc_test: 50.0 G loss_test: 0.11401281 G pearson_test: 0.97846556\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 479 次训练 D loss_train: 0.024947334080934525 D acc_train: 50.0 G loss_train: 0.09571391344070435 G pearson_train: 0.9801745414733887\n",
      "第 479 次测试 D loss_test: 0.18343413 D acc_test: 50.0 G loss_test: 0.11408442 G pearson_test: 0.9784608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 480 次训练 D loss_train: 0.024870391935110092 D acc_train: 50.0 G loss_train: 0.09578254073858261 G pearson_train: 0.9801689982414246\n",
      "第 480 次测试 D loss_test: 0.18314233 D acc_test: 50.0 G loss_test: 0.11414588 G pearson_test: 0.97845614\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 481 次训练 D loss_train: 0.02476642280817032 D acc_train: 50.0 G loss_train: 0.09584391117095947 G pearson_train: 0.9801640510559082\n",
      "第 481 次测试 D loss_test: 0.18288618 D acc_test: 50.0 G loss_test: 0.1142202 G pearson_test: 0.97845155\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 482 次训练 D loss_train: 0.02465544454753399 D acc_train: 50.0 G loss_train: 0.09590446203947067 G pearson_train: 0.9801589846611023\n",
      "第 482 次测试 D loss_test: 0.1824199 D acc_test: 50.0 G loss_test: 0.1142843 G pearson_test: 0.97844714\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 483 次训练 D loss_train: 0.024537932127714157 D acc_train: 50.0 G loss_train: 0.09597650915384293 G pearson_train: 0.9801540970802307\n",
      "第 483 次测试 D loss_test: 0.18198043 D acc_test: 50.0 G loss_test: 0.1143454 G pearson_test: 0.9784426\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 484 次训练 D loss_train: 0.024397606030106544 D acc_train: 50.0 G loss_train: 0.09604201465845108 G pearson_train: 0.9801491498947144\n",
      "第 484 次测试 D loss_test: 0.18137456 D acc_test: 50.0 G loss_test: 0.11442128 G pearson_test: 0.97843814\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 485 次训练 D loss_train: 0.024249613285064697 D acc_train: 50.0 G loss_train: 0.09611158072948456 G pearson_train: 0.9801440238952637\n",
      "第 485 次测试 D loss_test: 0.18070301 D acc_test: 50.0 G loss_test: 0.11448876 G pearson_test: 0.9784339\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 486 次训练 D loss_train: 0.024077555164694786 D acc_train: 50.0 G loss_train: 0.09618698060512543 G pearson_train: 0.9801392555236816\n",
      "第 486 次测试 D loss_test: 0.17994893 D acc_test: 50.0 G loss_test: 0.114557706 G pearson_test: 0.9784298\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 487 次训练 D loss_train: 0.02388424426317215 D acc_train: 50.0 G loss_train: 0.09626179188489914 G pearson_train: 0.9801346063613892\n",
      "第 487 次测试 D loss_test: 0.17914179 D acc_test: 50.0 G loss_test: 0.114634804 G pearson_test: 0.9784256\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 488 次训练 D loss_train: 0.023675840348005295 D acc_train: 50.0 G loss_train: 0.09634337574243546 G pearson_train: 0.9801300168037415\n",
      "第 488 次测试 D loss_test: 0.17827651 D acc_test: 50.0 G loss_test: 0.11470597 G pearson_test: 0.97842157\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 489 次训练 D loss_train: 0.023449940606951714 D acc_train: 50.0 G loss_train: 0.09641203284263611 G pearson_train: 0.9801254868507385\n",
      "第 489 次测试 D loss_test: 0.17727895 D acc_test: 50.0 G loss_test: 0.114788756 G pearson_test: 0.9784175\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 490 次训练 D loss_train: 0.023209355771541595 D acc_train: 50.0 G loss_train: 0.09649141877889633 G pearson_train: 0.9801211357116699\n",
      "第 490 次测试 D loss_test: 0.1762982 D acc_test: 50.0 G loss_test: 0.11485495 G pearson_test: 0.97841406\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 491 次训练 D loss_train: 0.02295496314764023 D acc_train: 50.0 G loss_train: 0.09657357633113861 G pearson_train: 0.980116605758667\n",
      "第 491 次测试 D loss_test: 0.17504422 D acc_test: 50.0 G loss_test: 0.114932254 G pearson_test: 0.97841024\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 492 次训练 D loss_train: 0.022701028734445572 D acc_train: 50.0 G loss_train: 0.09664341062307358 G pearson_train: 0.9801129102706909\n",
      "第 492 次测试 D loss_test: 0.17391515 D acc_test: 50.0 G loss_test: 0.115013234 G pearson_test: 0.97840655\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 493 次训练 D loss_train: 0.022447017952799797 D acc_train: 50.0 G loss_train: 0.09672555327415466 G pearson_train: 0.9801083207130432\n",
      "第 493 次测试 D loss_test: 0.17258114 D acc_test: 50.0 G loss_test: 0.11508013 G pearson_test: 0.97840315\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 494 次训练 D loss_train: 0.022186890244483948 D acc_train: 50.0 G loss_train: 0.09681198000907898 G pearson_train: 0.980104386806488\n",
      "第 494 次测试 D loss_test: 0.17145717 D acc_test: 50.0 G loss_test: 0.11516237 G pearson_test: 0.97839975\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 495 次训练 D loss_train: 0.021922187879681587 D acc_train: 50.0 G loss_train: 0.0969083309173584 G pearson_train: 0.9800992608070374\n",
      "第 495 次测试 D loss_test: 0.16989729 D acc_test: 50.0 G loss_test: 0.11524454 G pearson_test: 0.97839624\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 496 次训练 D loss_train: 0.021649684756994247 D acc_train: 50.0 G loss_train: 0.0969838947057724 G pearson_train: 0.9800963401794434\n",
      "第 496 次测试 D loss_test: 0.16884436 D acc_test: 50.0 G loss_test: 0.11534124 G pearson_test: 0.97839266\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 18ms/step\n",
      "第 497 次训练 D loss_train: 0.021394453942775726 D acc_train: 50.0 G loss_train: 0.09706497192382812 G pearson_train: 0.9800918698310852\n",
      "第 497 次测试 D loss_test: 0.16723618 D acc_test: 50.0 G loss_test: 0.11540548 G pearson_test: 0.97838944\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 498 次训练 D loss_train: 0.02113654464483261 D acc_train: 50.0 G loss_train: 0.09715453535318375 G pearson_train: 0.980088472366333\n",
      "第 498 次测试 D loss_test: 0.16606215 D acc_test: 50.0 G loss_test: 0.11548375 G pearson_test: 0.97838646\n",
      "9/9 [==============================] - 0s 12ms/step\n",
      "3/3 [==============================] - 0s 11ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 499 次训练 D loss_train: 0.020871896296739578 D acc_train: 50.0 G loss_train: 0.09725990146398544 G pearson_train: 0.9800834655761719\n",
      "第 499 次测试 D loss_test: 0.16439047 D acc_test: 50.0 G loss_test: 0.11556619 G pearson_test: 0.9783832\n",
      "9/9 [==============================] - 0s 11ms/step\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "第 500 次训练 D loss_train: 0.020601283758878708 D acc_train: 50.0 G loss_train: 0.0973576232790947 G pearson_train: 0.9800795912742615\n",
      "第 500 次测试 D loss_test: 0.16302978 D acc_test: 50.0 G loss_test: 0.11567578 G pearson_test: 0.9783797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 12ms/step\n",
      "相关系数 [0.97837657]\n"
     ]
    }
   ],
   "source": [
    "generator,discriminator,Vgg_19,predicty,testy,r,p=Auto_MSG_SE_Densenet_GAN(sm_HR,sm_LR,2,test_size=0.25,if_best_mode='no',modelpath=None,conv_core_num=512,model_deep=1,Vgg_deep=2,if_weight_initialize='no',weight_initialize_method='TruncatedNormal',weight_initialize_parameter1=0.00,weight_initialize_parameter2=0.05,loss_function='SSIM+Vgg',if_print_model='yes',optimizer='SGD',g_learning_rate=0.001,d_learning_rate=0.01,epochs=500,g_train_time=10,ifrandom_split='yes',ifmute='no',ifsave='no',savepath=None,device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b0882c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T07:36:11.968887Z",
     "start_time": "2024-04-12T07:36:11.630506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04851876770713716\n",
      "34.603477\n",
      "0.91660476\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import numpy as np\n",
    "mse=np.zeros((testy.shape[1],testy.shape[2]))\n",
    "for i in range(testy.shape[1]):\n",
    "    for j in range(testy.shape[2]):\n",
    "        mse[i,j]=sklearn.metrics.mean_squared_error(testy[:,i,j],predicty[:,i,j])\n",
    "psnr=tf.image.psnr(predicty,testy,max_val=(np.nanmax(sm_HR)-np.nanmin(sm_HR)))\n",
    "ssim=tf.image.ssim(predicty,testy,max_val=(np.nanmax(sm_HR)-np.nanmin(sm_HR)))\n",
    "print(np.nanmean(mse))\n",
    "print(np.nanmean(psnr))\n",
    "print(np.nanmean(ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85efb39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 40, 40, 1) (92, 40, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "print(testy.shape,predicty.shape)\n",
    "time_new=np.arange(0,testy.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b43005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Auto_paint_self\n",
    "Auto_paint_self.create_nc(time_new,lat[:-1],lon[:-1],None,np.nanmean(testy,axis=(3)),'time','latitude','longitude','testy','no',None,'self','yes','yes','D:/ESRGAN_Soil_moisture_SSIM+VGG_testys.nc')\n",
    "Auto_paint_self.create_nc(time_new,lat[:-1],lon[:-1],None,np.nanmean(predicty,axis=(3)),'time','latitude','longitude','predicty','no',None,'self','yes','yes','D:/ESRGAN_Soil_moisture_SSIM+VGG_predictys.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15d74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Auto_paint_self\n",
    "predicty,lon,lat,levels,latlow,lattop,lonleft,lonright,times=open_data_nc('one','D:/ESRGAN_Soil_moisture_SSIM+VGG_testys.nc','testy','self','time',0,91,'yes','longitude','yes','latitude',35.25,45.0,110.0,119.75,0.25,0.25,'no','no',None,None,changeresolution=1,timespace=1,ifchange_west_east='no',ifinterpolate='no')\n",
    "testy,lon,lat,levels,latlow,lattop,lonleft,lonright,times=open_data_nc('one','D:/ESRGAN_Soil_moisture_SSIM+VGG_predictys.nc','predicty','self','time',0,91,'yes','longitude','yes','latitude',35.25,45.0,110.0,119.75,0.25,0.25,'no','no',None,None,changeresolution=1,timespace=1,ifchange_west_east='no',ifinterpolate='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2487ded0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Auto_paint_self\n",
    "Auto_paint_self.Autoshaded_quiver(testy[0,:,:],None,None,None,lon,lat,None,None,latlow,lattop,lonleft,lonright,'shaded','xy',picturenum=1,row=1,column=1,dpi=600,shadedcolor=None,ifshp='no',shpname=None,ifsave='no',savename=None,valuemodel='+-',ifself_vmax_vmin='yes',selfvmax =2.0,selfvmin=-2.0,shaded_quiver_title='高分辨率数据',ifline='no',ifclabel='no',ifcolorbar='yes',ifhatch='no',hatchpoint=None,hatchvalue=None,quiverscale=1,xspace=2,yspace=2,zspace=10000,labelsize=20,section=10.0,ifmaskout='no',maskoutarea=None,iftangle='no',tangle=None,ifchina='no',chinamap=None,ifsouthseamap='no',southseamap=None,southsealoc=[0.8, 0.21, 0.1, 0.15],ifglobal='no',projection_mode='plate',ifgridline='no',ifgeo='no',geo=None)\n",
    "Auto_paint_self.Autoshaded_quiver(predicty[0,:,:],None,None,None,lon,lat,None,None,latlow,lattop,lonleft,lonright,'shaded','xy',picturenum=1,row=1,column=1,dpi=600,shadedcolor=None,ifshp='no',shpname=None,ifsave='no',savename=None,valuemodel='+-',ifself_vmax_vmin='yes',selfvmax =2.0,selfvmin=-2.0,shaded_quiver_title='降尺度数据',ifline='no',ifclabel='no',ifcolorbar='yes',ifhatch='no',hatchpoint=None,hatchvalue=None,quiverscale=1,xspace=2,yspace=2,zspace=10000,labelsize=20,section=10.0,ifmaskout='no',maskoutarea=None,iftangle='no',tangle=None,ifchina='no',chinamap=None,ifsouthseamap='no',southseamap=None,southsealoc=[0.8, 0.21, 0.1, 0.15],ifglobal='no',projection_mode='plate',ifgridline='no',ifgeo='no',geo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa3c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
